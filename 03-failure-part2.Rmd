# The subprime mortgage crisis unfolds in early 2007, part 2

Keywords: logistic regression, unsupervised learning, principal component analysis, k-means clustering, correspondence analysis, chi-squared test, contingency table, pivot, correlation table

## The fall of FICO

The problem with the conventional wisdom of long standing is that it loses sight of history. The prominence of FICO in home loan credit underwriting described in the [Pinto Testimony] had its origins in a different time (the early 1990s) and a different lending environment. Freddie Mac was in a good position to ensure that all other things *were* equal. It made only what came to be called "prime" loans, generally for no more than 80% of the value of the property, under more stringent limitations on the debt-to-income ratio of the borrower and many other criteria that it kept within a narrow range, and offered only a few varieties of loans.

In the subprime market that emerged in the late 90s, all of those factors changed. Criteria that were narrow became broad, documentation was relaxed and a widespread assumption was that continually rising home values would preclude any problems. It's not surprising that FICO lost its predictive power.

## Restructuring the data

Some of the testing of FICO as a useful metric involved subsetting the data. There were many more variables than the ones used, some of them categorical and some categorical coded as numeric. One potentially useful variable is location, because we know that real estate value are location sensitive. We have four location fields in the database, all derived from the postal zip code:

* The zip code itself, which is generally either much smaller or much larger than the real estate market, and also changes at the convenience of the postal service. 

* The metropolitan area derived from the U.S. Census ZIP code tabulation area, but covers a larger area than most real estate markets

* Longitude and latitude dervived from the ZCTA's, used for mapping

## Logistic regression using locational fields

As a compromise, I converted the 5-digit zip codes into 3-digit zip codes. In metropolitan areas, the 3-digit codes are the sizes comparable to how the multiple listing services divide the market. We'll see if there is any value in this proxy measure of real estate market, using logistic regression (linear regression is inappropriate because zip codes are not continuous).

```{r setup4A,echo = FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(DBI)
library(RMySQL)
library(ggfortify)
drv <- dbDriver("MySQL")
con <- dbConnect(drv, username="root", password="", dbname ="dlf", host="localhost")
res <- as.tibble(dbGetQuery(con, "SELECT down, round(zip/100,0) FROM y7"))
colnames(res) <- c("down", "zip3")
res.mod <- glm(down ~ zip3, data=res)
autoplot(res.mod)
``` 

We continue to bump up against the same problem in the Normal Q-Q plot.

Do metropolitan areas do a better job?

```{r, results="asis", echo = FALSE, warning=FALSE}
res <- as.tibble(dbGetQuery(con, "SELECT down, metro FROM y7"))
res.mod <- glm(down ~ metro, data=res)
rm(res)
autoplot(res.mod)
```

No.

[In preparation, remaining categorical variables for logistic regression]

## Principal component analysis

```{r psa1,  echo = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(DBI)
library(RMySQL)
#library(FacoMineR)
drv <- dbDriver("MySQL")
con <- dbConnect(drv, username="root", password="", dbname ="dlf", host="localhost")
res <- dbGetQuery(con, "SELECT fico, cltv, orate, obal, dti FROM loans")
cs <- as.tibble(res) # cs contains only the continuous variables
css <- sample_n(cs,300, replace = FALSE) 
# naming conventions following James et al. *An Introduction to Statistical Learning*

pr.out <- prcomp(css, scale = TRUE)
pr.var <- pr.out$sdev*2
pve <- pr.var/sum(pr.var)
# ------------------------------------------------------------------------
# Plots
par(mfrow=c(1,2))

screeplot(pr.out, type = "lines")

biplot(pr.out, scale = 0)
``` 

```{r cov1, echo = FALSE, warning=FALSE, message=FALSE}
orate_s <- cs$orate
obal_s <- cs$obal
```

The cumulative variance is  `r round(cumsum(pve),3)` for this sample of 300.

From the loadings chart on the right, we see that *obal* (the original amount of the loan) and *orate* (the original rate of interest) are negatively correlated, `r round(cor(orate_s,obal_s),3)`. The remaining continuous variables, *fico*, *dti* (debt-to-income ratio), *cltv* (the ratio of first and second loans to the value of the mortgaged property), have small correlations:

The variables *fico* and *cltv* have some correlation (`r round(cor(cs$fico,cs$cltv),3)`), while *fico* and *dti* (`r round(cor(cs$fico,cs$dti),3)`) and *dti* and *cltv* (`r round(cor(cs$dti,cs$cltv),3)`) have low correlations.

The variable *orate,* however, is clearly discrete (it takes only a handful of integer values), so we re-run without it and increase the size of the sample to 9,000.

```{r, results="asis", echo = FALSE, warning=FALSE}
res <- dbGetQuery(con, "SELECT fico, cltv, obal, dti FROM loans")
cs <- as.tibble(res) # cs contains only the continuous variables
css <- sample_n(cs,9000, replace = FALSE)
pr.out <- prcomp(css, scale = TRUE)
pr.var <- pr.out$sdev*2
pve <- pr.var/sum(pr.var)
par(mfrow=c(1,2))

screeplot(pr.out, type = "lines")

biplot(pr.out, scale = 0)

``` 

The variances are
```{r, results="asis", echo = FALSE, warning=FALSE}
pve
```


## K-means clustering

The following represents the unsupervised classification of the *fico, cltv, obal*, and *dti* of an approximately 10% sample from the **loans** dataset.

```{r, results="asis", echo = FALSE, warning=FALSE}
css.k <- kmeans(css, 3, iter.max = 20, nstart = 50)
plot(css, col=(css.k$cluster+1), main="K-Means Clustering Result with K=3, n = 9,000, nstart = 50, max.iter = 20", pch=20, cex=2)
```

Using the entire population, rather than a sample creates somewhat better defined clusters

```{r, results="asis", echo = FALSE, warning=FALSE}
## Not run
## cs.k <- kmeans(cs, 3, iter.max = 20, nstart = 50)
## use load("kmeans_loans.Rda") # brings in cs.k
## Not run
## plot(cs, col=(cs.k$cluster+1), main="K-Means Clustering Result with K=3, n = all_loans, nstart = 50, max.iter = 20", pch=20, cex=2)
## Use provided link (long running process)
```
![K-means clustering of all loans in loans database](http://media.richard-careaga.com/img/Kmeans_loans.png)
 
Consideration might be given to separately clustering high-balance loans. Other outliers, such as the zero FICO scores, can be safely ignored, as can be the anomalous loan with a 97% *dti* ratio.

### Housekeeping

Since PCA is a relatively expensive calculation for 100K records, I saved the result to and Rda file for reuse. Now, I'll add it to the create a new loans database as another categorical variable.

Because I don't run the MySQL server with rollback and to avoid a proliferation of names

    MariaDB [dlf]> CREATE TABLE y7k AS SELECT * from y7;
    Query OK, 96147 rows affected (1.06 sec)
    Records: 96147  Duplicates: 0  Warnings: 0

The strategy is to create df loans with all fields and cbind cs.k$clusters, renamed "value" to kcluster and write back to loans.

    dbWriteTable(con, "y7k", cs)

Here's the code (not run, to avoid churn)

    library(tidyverse)
    library(DBI)
    library(RMySQL)
    library(FactoMineR)

    drv <- dbDriver("MySQL")
    con <- dbConnect(drv, username="root", password="", dbname ="dlf", host="localhost")
    res <- dbGetQuery(con, "SELECT * from y7")
    cs <- as.tibble(res)
    load("data/kmeans_loans.Rda") # brings in cs.k
    #check for missing values
    nrow(cs) == length(cs.k$cluster)
    # convert to data frame
    csk <- as.tibble(cs.k$cluster)
    # rename the column
    csk <- csk %>% transmute(kcluster = value)
    # combine
    loans <- bind_cols(cs,csk)
    # remove duplicate rownames column
    loans <- loans[-1]
    # write back to SQL
    #dbWriteTable(con, "loans", y7k) 
    # test
    #dbGetQuery(con, "SELECT * from loans limit 25")

## Do the clusters predict loan performance?

Do the clusters derived from the principal components of the numeric variables, *obal*, *orate*, *dti* and *cltv* have an association with the *down* variable of consequtive payments missed?

If they do, we have the makings of a model; otherwise, we need to look at the other data available. 


[credit disclosure]: https://goo.gl/uhX1Pc
[LBMLT 2006-1]: https://www.sec.gov/Archives/edgar/data/1119605/000114420406002461/v033798_fwp.htm

[Pinto testimony]: https://democrats-oversight.house.gov/sites/democrats.oversight.house.gov/files/documents/Fannie%20Freddie%20Testimony%20of%20Edward%20Pinto%2012.9.08%20written%20submission%20Full.pdf
