--- 
title: "Data Science Portfolio"
author: "Richard Careaga"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
github-repo: github/technocrat/technocrat.github.io
description: "Landing site"
---
# Purpose  {#preface}

This site shows where Richard Careaga's data science skills stand in mid-2018. 

Its intent is to provide prospective employers with concrete evidence of my abilities to do the work of a data sciencist. The content is all my own work, and none of the cases are based on classroom assignments, except where indicated. The style of these pages is directed to that purpose. They are not academic papers, industry conference papers, nor reports to management. They are designed to show you my thought processes and methods.

## Updates

* 2018-06-23 added qqnorm tests to FICO example in Failure Analysis chapter, part 1
* 2018-06-26 stratified Failure Analysis 125K dataset to censor loan dropouts during 2006, classified remaining loans into performance categories based on monthly payment/nonpayment data and compared FICO distributions in the categories
* 2018-06-28 illustrated non-normality of FICO distributions with Shapiro-Wilks test; refactored database for multiple regression analysis including categorical data

## Literate programming: the tight integration of code and text

An analysis can have different audiences, and one of those may be peers, who may want to look under the hood to see exactly how the data were processed to produce the results given. This book is in that style and the RMarkdown files used to produce this portfolio are all available: 

    git clone https://github.com/technocrat/technocrat.github.io

## Background

In 2007, I put much effort into acquiring the nuts-and-bolts of data science, dusting off old statistial learning, and throwing them into the front line of the initial skirmishes of the Great Recession. This was not something in my job description at Washington Mutual Bank, the largest ever to fail in the U.S. Fortunately, I was senior enough to decide on my own how best to spend my time. 

I spent a month split between data acquisition and learning three new software tools -- MySQL, R and Python. That dataset is one of several cases studies in this document that I use to show what you can expect if you take me on as a data scientist.

The cases range from spreadsheet sized (a few hundred records) to small (a thousand), middling (125,000) and largest (500,000). As the cases get larger, the number of fields grow, as well, along with the data clean-up.

## The Cases

Each of the cases is designed to illustrate one or more specific skill by presenting an example and explaining what motivated it, what it does, the tools used, and what its output accomplished.

* 2015 police involved homicides: Descriptive statistics, observational data hypothesis testing
* The cinancial cash flow model: OOP Python, model derivation from narrative
* Failure analysis of subprime Loans: MySQL,R, exploratory data analysis, high-dimensional data, covariance, clustering, regression, principal component analyis, machine learning
* The Enron email corpus: Python, NLTK, social network analysis, de-duplication, stopwords, boilerplate stripping
* Daycare costs and unexamined Assumptions: Data skepticism
* Examples of utility programming in Python, Haskell, Lua and Flex/Bison

## Current Credentials

I've completed the following online courses, to consolidate and expand my previous training and experience in data science, which sprung, ultimately, from undergraduate and graduate degrees in geology and geophysics. My plan is to use these cases to apply the many new techniques that I have learned to date, and expect to learn as I complete the remaining courses in the series.

### Graduate school level

![MITxPro Data Science and Big Data Analytics: Making Data-Driven Decisions](https://s3-us-west-2.amazonaws.com/tuva/MITxPRO+DSx+Certificate+%7C+MIT+xPro.png)

!["HarvardX PH125.1x Certificate"](https://s3-us-west-2.amazonaws.com/tuva/HarvardX+PH125.1x+Certificate+%7C+edX.png)
![""HarvardX PH125.2x Certificat"](https://s3-us-west-2.amazonaws.com/tuva/HarvardX+PH125.2x+Certificate+%7C+edX.png)
!["HarvardX PH125.3x Certificate"](https://s3-us-west-2.amazonaws.com/tuva/HarvardX+PH125.3x+Certificate+%7C+edX.png)
!["HarvardX PH125.4x Certificate"](https://s3-us-west-2.amazonaws.com/tuva/HarvardX+PH125.4x+Certificate+%7C+edX.png)
![Harvard PH125.5x, Productivity, certificate issuance pending](https://s3-us-west-2.amazonaws.com/tuva/HarvardX%3B+PH125.5x.png)
 
![HarvardX PH559x Certificate](https://s3-us-west-2.amazonaws.com/tuva/HarvardX+PH559x+Certificate+%7C+edX.png)
### Undergraduate level

!["BerkeleyX Data8.1x Certificate"](https://s3-us-west-2.amazonaws.com/tuva/BerkeleyX+Data8.1x+Certificate+%7C+edX.png)

## Prior analytic and programming experience

My prior analytic education and experience was in geology/geophysics (M.S.) and law (J.D.) I have been on *nix as my own sysadm since 1984, including Venix (a v7 derivative), Irix (SGI), Ubuntu and other Linux versions, and Mac OSx. My orientation is strongly CLI, rather than GUI, but I have used Excel and Word since their early release. I have non-PC experience with the IBM S/34 and implementation of payroll, general ledger and budgeting. During the mid-90s, I installed, configured and operated http, mail, news, proxy, certificate and LDAP servers. I am familiar with many of the important bash tools (which I use on a daily basis) and with C and Perl (which I can still read, but seldom use).

> When we got our numbers off of greenbar, no one cared how pretty it looked. 

<!--chapter:end:index.Rmd-->

# 2015 Police Involved Homicides {#counted}

Keywords: Descriptive statistics, R, Census API, geocoding, thematic mapping

## Introduction

Police involved civilian homicides in the United States is the subject of a series published by *The Guardian,* [The Counted], which compiles media reports of homicides resulting from police encounters in the United States. You can explore the data interactively at their [interactive] page. The data version used in this chapter is dated 2016-06-30.


```{r setup1, include=TRUE, echo=FALSE, results= 'asis', message = FALSE, warning=FALSE}
##########################################################
# required libraries
library(bitops)
library(broom)
library(Cairo)
library(classInt)
library(dplyr) # select conflict, use dplyr::select()
library(ggmap)
library(ggplot2)
library(kableExtra)
library(knitr)
library(lunar)
library(mapproj)
library(maps)
library(maptools)
library(MASS)
library(pander)
library(printr)
library(RColorBrewer)
library(RCurl)
library(rgdal)
library(rgeos)
library(scales)
library(sp)
library(tidyverse)
library(Unicode)
library(xtable)
##########################################################
# global options
options(stringsAsFactors = FALSE)
options(xtable.booktabs = TRUE)
##########################################################
# convenience functions
bindh <- function(x,y) trunc(x/y)*y
cdefactor <- function(x) {as.character(levels(x))[x]}
ndefactor <- function(x) {as.numeric(levels(x))[x]}
pct <- function(x,y) round((x/y*100), digits = 2)
##########################################################
# functions to make multicolumn xtables
# format a 51-state table into 4 columns
squarestate <- function (.df) {
    blankrow <- as.data.frame("")        
    pt1 = slice(.df,1:13)
    pt2 = slice(.df,14:26)
    pt3 = slice(.df,27:39)
    pt4 = slice(.df,40:52)
    pt4 = bind_rows(pt4,blankrow)
    result <- bind_cols(pt1,pt2,pt3,pt4)
    return(result)
}
# format a 17-cohort table into 3 columns
squareage <- function (.df) {
    blankrow <- as.data.frame("")        
    pt1 = slice(.df,1:6)
    pt2 = slice(.df,7:12)
    pt3 = slice(.df,13:17)
    pt3 = bind_rows(pt3,blankrow)
    result <- bind_cols(pt1,pt2,pt3)
    result$age <- as.character(result$age)
    result[5,5] <- "Unknown"
    result[6,5] <- ""
    result[6,6] <- ""
    result[7] <- NULL
    return(result)
}
# format days of month into 4 columns
squaredays <- function (.df) {
    blankrow <- as.data.frame("")        
    pt1 <- slice(.df,1:8)
    pt2 <- slice(.df,9:16)
    pt3 <- slice(.df,17:24)
    pt4 <- slice(.df,25:31)
    pt4 = bind_rows(pt4, blankrow)
    result <- bind_cols(pt1,pt2,pt3,pt4)
    result[9] <- NULL
    return(result)
}

# divide data into intervals
intervals = function(.df, ...)
  {
  argList = match.call(expand.dots = FALSE)$... 
    for (i in 1:length(argList)) 
      {
        colName <- argList[[i]]
        series_colName = eval(substitute(colName), envir = .df, enclos = parent.frame())
        min <- min(series_colName)
        max <- max(series_colName)
        diff <- max - min
        std <- sd(series_colName)
        equal.interval <- seq(min, max, by = diff/6)
        quantile.interval <- quantile(series_colName, probs = seq(0, 1, by = 1/6))
        std.interval <- c(seq(min, max, by = std), max)
        natural.interval <- classIntervals(series_colName, n = 6, style = 'jenks')$brks
        .df$equal <- cut(series_colName, breaks = equal.interval, include.lowest = TRUE)
        names(.df)[names(.df) == "equal"] <- paste(colName,".","equal", sep = '')
        .df$quantile <- cut(series_colName, breaks = quantile.interval, include.lowest = TRUE)
        names(.df)[names(.df) == "quantile"] <- paste(colName,".","quantile", sep = '')
        .df$std <- cut(series_colName, breaks = std.interval, include.lowest =  TRUE)
        names(.df)[names(.df) == "std"] <- paste(colName,".","std", sep = '')
        .df$natural <- cut(series_colName, breaks = natural.interval, include.lowest = TRUE)
        names(.df)[names(.df) == "natural"] <-     paste(colName,".","natural", sep = '')
  }
  return(.df)
}

##########################################################
# Data access for relatively static sources
thecounted <- read.csv("data/2015.csv", header = TRUE, stringsAsFactors = FALSE)
counted <- tbl_df(thecounted)
fields <- tbl_df(colnames(counted))
fields1 <- slice(fields,1:7)
fields2 <- slice(fields,8:14)
fields <- bind_cols(fields1, fields2)
# convenience constants
n <- nrow(counted) # numeral, e.g., 1145
N <- as.character(prettyNum(n, big.mark = ',')) # e.g., 1,145
# add long/lat to address points
addr <- as.data.frame(paste(counted$streetaddress,",",counted$city,",",counted$state))
colnames(addr) <- "location"
addr <- tbl_df(addr)
##########################################################
# acquire geo/popuation data
# following only need to be done when dataset changes
#api_key <- "[USER SPECIFIC]" # google
#coord <- geocode(addr$location, output = "latlon")
#save(coord, file = "data/coord.Rda")
# following only needs to be done when new cartographic boundary file published
#dsn <- "data"
#layer <- "data/cb_2015_us_state_5m"
#cb5 <- readOGR(dsn = dsn, layer = layer)
#save(cb5, file = "data/cb5.Rda")
# following only needs to be done when new census estiates published
#pop_source <- "https://www.census.gov/popest/data/national/totals/2015/files/NST-EST2015-alldata.csv"
#population <- read.csv(textConnection(getURL(pop_source)))[-(1:5),]
#save(population, file = "data/population.Rda")
# add lat/long to dataframe
load("data/coord.Rda")
counted <- bind_cols(counted, coord)
colnames(counted)[11] <- "id"
##########################################################
load("data/cb5.Rda")
us <- cb5
# relocate AK & HI and outset RI, DE and DC
us_aea <- spTransform(us, CRS("+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs"))
alaska <- us_aea[us_aea$STATEFP == "02",]
alaska <- elide(alaska, rotate = -50)
alaska <- elide(alaska, scale = max(apply(bbox(alaska), 1, diff)) / 2.3)
alaska <- elide(alaska, shift = c(-2100000, -2500000))
proj4string(alaska) <- proj4string(us_aea)
hawaii <- us_aea[us_aea$STATEFP == "15",]
hawaii <- elide(hawaii, rotate = -35)
hawaii <- elide(hawaii, shift = c(5400000, -1400000))
proj4string(hawaii) <- proj4string(us_aea)
us_aea <- us_aea[!us_aea$STATEFP %in% c("02", "15"),]
us_aea <- rbind(us_aea, alaska, hawaii)
rhode_island <- us_aea[us_aea$STATEFP == "44",]
rhode_island <- elide(rhode_island, shift = c(125000,-125000))
proj4string(rhode_island) <- proj4string(us_aea)
delaware <- us_aea[us_aea$STATEFP == "10",]
delaware <- elide(delaware, shift = c(200000,0))
proj4string(delaware) <- proj4string(us_aea)
dc <- us_aea[us_aea$STATEFP == "11",]
dc <- elide(dc, shift = c(250000,-125000))
proj4string(dc) <- proj4string(us_aea)
us_aea <- us_aea[!us_aea$STATEFP %in% c("44", "10", "11", "60", "66", "69", "72", "78"),] #exclude overseas territories
us_aea <- rbind(us_aea, delaware, rhode_island, dc)
##########################################################
# obtain locations to put state names
centroids <- data.frame(us_aea$STUSPS, coordinates(us_aea))
names(centroids) <- c("id", "clong", "clat")
us50 <- fortify(us_aea, region = "STUSPS")
poly = coord_map("polyconic")
rownames(centroids) = centroids$id
# adjust centroids for better positioning of state names
centroids['LA',]$clong = 729000
centroids['FL',]$clong = 1800000
centroids['DE',]$clong = 2400000
centroids['RI',]$clong = 2500000
centroids['MD',]$clong = 1950000
centroids['MD',]$clat = -360000
centroids['MA',]$clat = 100000
centroids['NJ',]$clong = 2130000
centroids['NJ',]$clat = -255000
# load population data
load("data/population.Rda")
# load convenience data frame cross indexing state, id, fips
load("data/converter.Rda")
converter$state <- cdefactor(converter$state)
converter$id <- cdefactor(converter$id)
converter <- tbl_df(converter)
# create dataframe of states with zero-values in second column
vec <- vector(,51)
vec[1:51] <- 0
state <- as.data.frame(converter$id)
colnames(state) <- "id"
state <- tbl_df(state)
blank <- bind_cols(state,tbl_df(vec))
##########################################################
# variables from counted
# national data
age <- counted$age
age <- as.integer(age)
age <- bindh(age,5) # 5-year cohorts)
age <- as.data.frame(age)
age <- tbl_df(age)
age <- count(age,age)
age$n <- pct(age$n,n)
armed <- count(counted, armed)        # civilian weapon
cod <- count(counted, classification) # cause of death
cod$n <- pct(cod$n,n)
gender <- count(counted,gender)
gender$n <- pct(gender$n,n)
race <- count(counted,raceethnicity)
race$n <- pct(race$n,n)
state <- count(counted,id)
state$n <- pct(state$n,n)
weapon <- count(counted,armed)
weapon$n <- pct(weapon$n,n)
codvsarmed <- pct(table(counted$classification,counted$armed),n)
codvsarmed1 <- codvsarmed[,c(1:4)] # first half wide table
codvsarmed2 <- codvsarmed[,c(5:8)] # second half
deaths <- count(counted,id)
deaths <- tbl_df(deaths)
colnames(deaths) <- c("id", "deaths")
##########################################################
# Date manipulations
month <- as.data.frame(counted$month)
colnames(month) <- c("mon")
day <- counted$day
day <- as.data.frame(as.integer(day))
colnames(day) <- c("day")
monthday <- bind_cols(month,day)
dateinput <- paste(monthday$mon,monthday$day)
monthday <- transmute(monthday, date = paste(mon,day))
datefile <- (cat(system("bin/datify | bin/datify2 | bin/stripblanks > data/dates.csv", input = dateinput, intern = TRUE)))
dated <- read.csv("data/dates.csv", header = FALSE, stringsAsFactors = FALSE)
colnames(dated) = c("date")
dated <- tbl_df(dated)
dated$date <- as.Date(dated$date)
dated$dow <- weekdays(dated$date, abbreviate = FALSE)
dated$dow <- factor(dated$dow, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
dated$month <- counted$month
dowcount <- count(dated, dow) # day of week
dowcount$n <- pct(dowcount$n,n)
mindow = min(dowcount$n)
maxdow = max(dowcount$n)
dom <- as.data.frame(count(counted, day)) # day of month
dom$n <- pct(dom$n,n)
domspread <- squaredays(dom)
dated$moon <- lunar.phase(dated$date, name = TRUE)
lunar <- count(dated, moon)
lunar$n <- pct(lunar$n,n)
datecount <- count(dated,date)
datecountdays <- nrow(datecount)
datecount$date <- as.character(datecount$date)
datecounthigh <- filter(datecount, n == max(n))
monthcount <- count(month,mon)
monthcount$mon <- factor(monthcount$mon, levels = c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"))
monthcount$n <- pct(monthcount$n,n)
monthcount <- monthcount[order(monthcount$mon),]
monthorder <- monthcount
monthorder$n <- as.character(monthorder$n)
monthorder$mon <- factor(monthorder$mon, levels = c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"))
monthorder$mon <- as.character(monthorder$mon)
monthorder$n   <- as.character(monthorder$n)
##########################################################
uspop <- sum(population$POPESTIMATE2015)
uspopper5k <- uspop/100000
homi <- uspopper5k*5.1 # approimate homicidesh
comphomi <- pct(n,homi)
##########################################################
# Breakout for women
femme <- filter(counted, gender == "Female")
bfemme <- filter(femme, raceethnicity == "Black")
wfemme <- filter(femme, raceethnicity == "White")
nf <- nrow(femme)
nfb <- nrow(bfemme)
nfw <- nrow(wfemme)
fcount <- count(femme, id)
fcombo <- full_join(fcount,blank, by = "id")
fcombo[is.na(fcombo)] <- 0
fveh <- filter(counted, gender == "Female", armed == "No", classification == "Struck by vehicle")
fcodvsarmed <- pct(table(femme$classification,femme$armed),nf)
fcodvsarmed1 <- fcodvsarmed[,c(1:3)] # split wide table
fcodvsarmed2 <- fcodvsarmed[,c(4:6)]
fbcodvsarmed <- pct(table(bfemme$classification,bfemme$armed),nfb)
fbcodvsarmed1 <- fbcodvsarmed[,c(1:4)]
#fbcodvsarmed2 <- fbcodvsarmed[,c(3:4)]
fwcodvsarmed <- pct(table(wfemme$classification,wfemme$armed),nfw)
fwcodvstaser <- 0
fwcodvsarmed1 <- fwcodvsarmed[,c(1:4)]
#fwcodvsarmed2 <- fwcodvsarmed[,c(4:4)]
btrial <- count(femme, raceethnicity == "Black")[2,2]
wtrial <- count(femme, raceethnicity == "White")[2,2]
trial <- unlist(c(btrial,wtrial))
wgundeath <- filter(femme, classification == "Gunshot")
bw <- count(wgundeath, raceethnicity == "Black")[2,2]
ww <- count(wgundeath, raceethnicity == "White")[2,2]
outcome <- unlist(c(bw,ww))
##########################################################
# breakout for men
homme <- filter(counted, gender == "Male")
bhomme <- filter(homme, raceethnicity == "Black")
hhomme <- filter(homme, raceethnicity == "Hispanic/Latino")
whomme <- filter(homme, raceethnicity == "White")
nh <- nrow(homme)
nhb <- nrow(bhomme)
nhh <- nrow(hhomme)
nhw <- nrow(whomme)
hcodvsarmed <- pct(table(homme$classification,homme$armed),nh)
hcodvsarmed1 <- hcodvsarmed[,c(1:4)]
hcodvsarmed2 <- hcodvsarmed[,c(5:8)]
hwcodvsarmed <- pct(table(whomme$classification,whomme$armed),nhw)
hwcodvsarmed1 <- hwcodvsarmed[,c(1:4)]
hwcodvsarmed2 <- hwcodvsarmed[,c(5:7)]
hbcodvsarmed <- pct(table(bhomme$classification,bhomme$armed),nhb)
hbcodvsarmed1 <- hbcodvsarmed[,c(1:4)]
hbcodvsarmed2 <- hbcodvsarmed[,c(5:8)]
hhcodvsarmed <- pct(table(hhomme$classification,hhomme$armed),nhh)
hhcodvsarmed1 <- hhcodvsarmed[,c(1:4)]
hhcodvsarmed2 <- hhcodvsarmed[,c(5:8)]
##########################################################
# xtables are for LaTeX output; kable has been used throughout the text
#ageprint <- print.xtable(xtable(squareage(age), caption = "Deaths by age group"))
#codprint <- print.xtable(xtable(cod, "Deaths by cause"))
#datehighprint <- print.xtable(xtable(datecounthigh, caption = "Dates with highest number of deaths"))
#domprint <- print.xtable(xtable(domspread, caption = "Deaths by days of month"))
#dowprint <- print.xtable(xtable(dowcount, caption = "Deaths by day of week"))
#fbcodvsarmedprint1 <- print.xtable(xtable(fbcodvsarmed1, "Deaths of black women by whether and how officers and civilians were armed, part A"))
#fbcodvsarmedprint2 <- print.xtable(xtable(fbcodvsarmed2, "Deaths of black women by whether and how officers and civilians were armed, part B"))
#fcodvsarmedprint1 <- print.xtable(xtable(fcodvsarmed1, "Deaths of women by whether and how officers and civilians were armed, part A"))
#fcodvsarmedprint2 <- print.xtable(xtable(fcodvsarmed2, "Deaths of women by whether and how officers and civilians were armed, part B"))
#fieldsprint <- print.xtable(xtable(fields, "Fields in the data provided by The Guardian The Counted Project"))
#fwcodvsarmedprint1 <- print.xtable(xtable(fwcodvsarmed1, "Deaths of white women by whether and how officers and civilians were armed, part A"))
#fwcodvsarmedprint2 <- print.xtable(xtable(fwcodvsarmed, "Deaths of white women by whether and how officers and civilians were armed, part B"))
#genderprint <- print.xtable(xtable(gender, caption = "Deaths by gender"))
#genracprint <- print.xtable(xtable(count(femme,raceethnicity), caption = "Deaths of women by race/ethnicity"))
#hbcodvsarmedprint1 <- print.xtable(xtable(hbcodvsarmed1, "Deaths of black men by whether and how officers and civilians were armed, part A"))
#hbcodvsarmedprint2 <- print.xtable(xtable(hbcodvsarmed2, "Deaths of black men by whether and how officers and civilians were armed, part B"))
#hcodvsarmedprint1 <- print.xtable(xtable(hcodvsarmed1, "Deaths of men by whether and how officers and civilians were armed, part A"))
#hcodvsarmedprint2 <- print.xtable(xtable(hcodvsarmed2, "Deaths of men by whether and how officers and civilians were armed, part B"))
#hhcodvsarmedprint1 <- print.xtable(xtable(hhcodvsarmed1, "Deaths of hispanic men by whether and how officers and civilians were armed, part A"))
#hhcodvsarmedprint2 <- print.xtable(xtable(hhcodvsarmed2, "Deaths of hispanic men by whether and how officers and civilians were armed, part B"))
#hwcodvsarmedprint1 <- print.xtable(xtable(hwcodvsarmed1, "Deaths of white men by whether and how officers and civilians were armed, part A"))
#hwcodvsarmedprint2 <- print.xtable(xtable(hwcodvsarmed2, "Deaths of white men by whether and how officers and civilians were armed, part B"))
#howarmed <- print.xtable(xtable(codvsarmed1, caption = "Deaths by whether and how officers and civilians were armed, part A."))
#howarmed2 <- print.xtable(xtable(codvsarmed2, caption = "Deaths by whether and how officers and civilians were armed, part B."))
#lunprint <- print.xtable(xtable(lunar, caption = "Deaths by phase of moon"))
#monthdeathprint <- print.xtable(xtable(monthcount, caption = "Deaths by month"))
#raceprint <- print.xtable(xtable(race, caption = "Deaths by race/ethnicity"))
#stateprint <- print.xtable(xtable(squarestate(state), caption = "Deaths by state", label = {"tabhold"}), auto = TRUE)
#weaponprint <- print.xtable(xtable(weapon, caption = "Deaths by whether and how civilian was armed"))
##########################################################
# thematic maps
# ggplot setup2
plain_theme = theme(axis.text = element_blank()) + 
    theme(panel.background = element_blank(), 
        panel.grid = element_blank(), 
        axis.ticks = element_blank())
no_ylab <- ylab("") 
no_xlab <- xlab("")
# death count
# raw numbers for deaths
natural.interval <- classIntervals(deaths$deaths, n = 6, style = 'jenks', lowest = TRUE)$brks
natint <- natural.interval
deaths$natural <- cut(deaths$deaths, breaks = natint, include.lowest = TRUE)
map <- merge(us50, deaths, by = "id")
b <- ggplot(data = map) + geom_map(map = map, aes(x = long, y = lat, map_id = id, group = group), fill = "white", color = "black", size = 0.3) + plain_theme + no_ylab + no_xlab
fire = "YlOrRd"
drought = "YlOrBr"
l = geom_text(data = centroids, aes(clong, clat, label = id), color = "black", size = 2)
makewhite = centroids[c('CA','TX'),] # has to be set by inspection
w = geom_text(data = makewhite, aes(clong, clat, label = id), color = "white", size = 2)
# raw numbers for deaths
c <- b + geom_polygon(data = map, aes(x = long, y = lat, group = group, fill = natural), color = "dark grey", size = 0.3)
d <- c + l + w + ggtitle("Number of Police Involved Civilian Homicides in the U.S. in 2015") + scale_fill_brewer(palette = drought, name = "Number\nof deaths")
# population comparison
pop52 <- dplyr::select(population, POPESTIMATE2015, NAME)
pop52 <- tbl_df(pop52)
#remove PR
pop51 <- filter(pop52, NAME != "Puerto Rico")
pop51$id <- converter$id
pop51 <- tbl_df(pop51)
colnames(pop51) <- c("EST2015", "NAME", "id")
natural.interval <- classIntervals(pop51$EST2015, n = 6, style = 'jenks', lowest = TRUE)$brks
natint <- natural.interval
pop51$natural <- cut(pop51$EST2015, breaks = natint, include.lowest = TRUE)
map1 <- merge(us50, pop51, by = "id")
e <- b + geom_polygon(data = map1, aes(x = long, y = lat, group = group, fill = natural), color = "dark grey", size = 0.3)
q <- e + scale_fill_brewer(palette = drought) + l + w
natural.interval <- classIntervals(deaths$deaths, n = 6, style = 'jenks', lowest = TRUE)$brks
natint <- natural.interval
deaths$natural <- cut(deaths$deaths, breaks = natint, include.lowest = TRUE)
e <- b + geom_polygon(data = map, aes(x = long, y = lat, group = group, fill = natural), color = "dark grey", size = 0.3)
q <- e + scale_fill_brewer(palette = drought) + l + w
pop51a <- merge(pop51, deaths, by = "id")
pop51a <- mutate(pop51a, ratio = round(pop51a$deaths/(pop51a$EST2015/100000),2))
natural.interval <- classIntervals(pop51a$ratio, n = 6, style = 'jenks', lowest = TRUE)$brks
natint <- natural.interval
pop51a$natural <- cut(pop51a$ratio, breaks = natint, include.lowest = TRUE)
makewhite = centroids[c('CA'),]
w = geom_text(data = makewhite, aes(clong, clat, label = id), color = "white", size = 2)
makewhite = centroids[c('WY','NM', 'OK'),]
w = geom_text(data = makewhite, aes(clong, clat, label = id), color = "white", size = 2)
map3 <- merge(us50, pop51a, by = "id")
f <- b + geom_polygon(data = map3, aes(x = long, y = lat, group = group, fill = natural), color = "dark grey", size = 0.3)
r <- f + scale_fill_brewer(palette = drought) + l + w
d <- b + geom_polygon(data = map, aes(x = long, y = lat, group = group, fill = natural), color = "dark grey", size = 0.3)
p <- d + scale_fill_brewer(palette = drought) + l + w
##########################################################
# End set-up routines
```

## The Data

[The Counted] information on 2015 police involved civilian homicides report that the deaths in 2015 represent `r N` of the current population of the United States, approximately 320,000,000, a vanishingly small percentage. As a percentage of all homicides in 2014, the deaths represent approximately `r comphomi`%. *See* [Health, United States, 2015 - Individual Charts and Tables: Spreadsheet, PDF, and PowerPoint files, Table 17].

[The Counted] data layout in in CSV (comma separated value form) is:

```{r, results="asis", echo = FALSE}
fields_ <- fields
colnames(fields_) <- c(NULL,NULL)
kable(fields_, format.args = list(big.mark = ","), caption = "Fields in the data provided by The Guardian The Counted Project")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

Classification" is the cause of death and "armed" is whether or how the civilian was armed.

## Tabular summary of the data

### Gender

Approximately 95 percent of deaths were men. Gender of one death was reported as "non-conforming," possibly representing the delays involved in revising reporting systems to account for transgendered citizens.


```{r, results="asis", echo = FALSE}
gender_ <- gender
colnames(gender_) <- c(NULL,NULL)
kable(gender_, format.args = list(big.mark = ","), caption = "Percentages deaths by gender")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

### Race/Ethnicity

White deaths are under-represented compared to the national population. White, non-Hispanic population in 2014 was 62.2%. [Census Projections]


```{r, results="asis", echo = FALSE}
race_ = race
colnames(race_) <- c(NULL,NULL)
kable(race_, format.args = list(big.mark = ","), caption = "Percentage deaths by race/ethnicity")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```


### Age

Deaths were of all ages. The youngest death was `r min(age$age, na.rm=TRUE)` and the oldest, `r max(age$age, na.rm=TRUE)`. The median age of death (half older and half younger) was `r median(age$age, na.rm=TRUE)`, and the mean (average) age of death was approximately `r trunc(mean(age$age, na.rm=TRUE))`.


```{r, results="asis", echo = FALSE}
age_ <- age
colnames(age_) <- c(NULL,NULL)
kable(age_, format.args = list(big.mark = ","), caption = "Percentage deaths by age group")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

### Cause of Death

Gunshots are the leading cause of deaths in police involved civilian homicides, representing `r max(cod$n, na.rm=TRUE)`% of all deaths in [The Counted] dataset.

### Whether and how civilians were armed

Civilian firearms were the most common category involved, but unarmed civilians were the next most common, representing `r max(weapon$n, na.rm=TRUE)`%  and `r weapon$n[4]`% of all deaths, respectively.

```{r, results="asis", echo = FALSE}
weapon_ <- weapon
colnames(weapon_) <- c(NULL,NULL)
kable(weapon_, format.args = list(big.mark = ","), caption = "Percentage deaths by whether and how civilian was armed")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

### Location

More deaths occurred in California than in any other state, representing `r max(state$n, na.rm=TRUE)`% of all deaths, which is disproportionately higher than its national share of population, `r round(37254503/308758105*100,2)`%. [American Fact Finder]

```{r, results="asis", echo = FALSE}
state_ <- state
colnames(state_) <- c(NULL,NULL)
kable(state_, format.args = list(big.mark = ","), caption = "Percentage deaths by state")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

### Months and Days of Death

Nationally, deaths were spread approximately evenly among months, days of the week, days of the month and phases of the moon.  *There was at least one death on `r n_distinct(dated)` days of the year,* `r round(n_distinct(dated)/365*100,2)`% of all days. The median number of deaths per day was `r median(count(dated, date)$n)`, and the mean number of deaths was slightly larger, `r round(mean(count(dated,date)$n),2)`. On `r nrow(datecounthigh)` days, there were `r mean(datecounthigh$n)` deaths. 

```{r, results="asis", echo = FALSE}
datecounthigh_ <- datecounthigh
colnames(datecounthigh_) <- c(NULL,NULL)
kable(datecounthigh_, format.args = list(big.mark = ","), caption = "Dates with highest number of deaths")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

`r filter(monthcount, n == min(n))$mon` had the fewest deaths, `r min(count(month,mon)$n)`, `r round(min(count(month,mon)$n)/n*100,2)`%,  and `r filter(monthcount, n == max(n))$mon` had the most deaths, `r max(count(month,mon)$n)`, `r round(max(count(month,mon)$n)/n*100,2)`%.

```{r, results="asis", echo = FALSE}
monthcount_ <- monthcount
colnames(monthcount_) <- c(NULL,NULL)
kable(monthcount_, format.args = list(big.mark = ","), caption = "Percentage deaths by month")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

The day of the month with the fewest deaths was day `r filter(dom, n == min(dom$n))$day`, and the day of the month with the most deaths was day  `r filter(dom, n == max(dom$n))$day`.

```{r, results="asis", echo = FALSE}
domspread_ <- domspread
colnames(domspread_) <- c(NULL,NULL)
kable(domspread_, format.args = list(big.mark = ","), caption = "Percentage deaths by day of month")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

The day of the week with the fewest deaths was `r filter(dowcount, n == min(dowcount$n))$dow`, `r mindow`%, and the day of the month with the most deaths was `r filter(dowcount, n == max(dowcount$n))$dow`, `r maxdow`%.

```{r, results="asis", echo = FALSE}
dowcount_ <- dowcount
colnames(dowcount_) <- c(NULL,NULL)
kable(dowcount_, format.args = list(big.mark = ","), caption = "Percentage deaths by day of week")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

Folk wisdom attributes increased crime and other aberations to the full moon; *see [Lunacy and the Full Moon]*. However, deaths are approximately evenly distributed over the phases of the moon.

```{r, results="asis", echo = FALSE}
lunar_ <- lunar
colnames(lunar_) <- c(NULL,NULL)
kable(lunar_, format.args = list(big.mark = ","), caption = "Percentage deaths by lunar phase")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

### Cause of Death and Civilian Armed Status

```{r, results="asis", echo = FALSE}
kable(codvsarmed1, format.args = list(big.mark = ","), caption = "Cause of death and civilian armed status, part A")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

```{r, results="asis", echo = FALSE}
kable(codvsarmed2, format.args = list(big.mark = ","), caption = "Cause of death and civilian armed status, part B")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```
Of the `r N` deaths, `r nf` were women, approximately `r  pct(nf,n)`%. All but `r nf - nfw - nfb` of those deaths were white or black.

```{r, results="asis", echo = FALSE}
kable(fcodvsarmed1, format.args = list(big.mark = ","), caption = "Cause of death of women and civilian armed status, part A")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

```{r, results="asis", echo = FALSE}
kable(fcodvsarmed2, format.args = list(big.mark = ","), caption = "Cause of death of women and civilian armed status, part B")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

Of the `r nf` deaths of women, `r nfw`, approximately `r  pct(nfw,nf)`% were white. Tables show percentages of deaths among white women.

```{r, results="asis", echo = FALSE}
kable(fwcodvsarmed1, format.args = list(big.mark = ","), caption = "Cause of death of white women and civilian armed status")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```


Of the `r nf` deaths of women, `r nfb`, approximately `r  pct(nfb,nf)`% were black. Tables show percentages of deaths among black women.

```{r, results="asis", echo = FALSE}
kable(fbcodvsarmed1, format.args = list(big.mark = ","), caption = "Cause of death of black women and civilian armed status")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

Of the `r N` deaths, `r as.character(prettyNum(nh, big.mark = ','))` were men, approximately `r  pct(nh,n)`%. 

```{r, results="asis", echo = FALSE}
kable(hcodvsarmed1, format.args = list(big.mark = ","), caption = "Cause of death of men and civilian armed status, part A")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

```{r, results="asis", echo = FALSE}
kable(hcodvsarmed2, format.args = list(big.mark = ","), caption = "Cause of death of men and civilian armed status, part B")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

Of the `r as.character(prettyNum(nh, big.mark = ','))` deaths of men, `r nhw`, approximately `r pct(nhw,nh)`%, were white. Tables show percentages of deaths among white men.

```{r, results="asis", echo = FALSE}
kable(hwcodvsarmed1, format.args = list(big.mark = ","), caption = "Cause of death of white men and civilian armed status, part A")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

```{r, results="asis", echo = FALSE}
kable(hwcodvsarmed2, format.args = list(big.mark = ","), caption = "Cause of death of white men and civilian armed status, part B")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

Of the `r as.character(prettyNum(nh, big.mark = ','))` deaths of men, `r nhb`, approximately `r  pct(nhb,nh)`%, were black. Tables show percentages of deaths among black men.

```{r, results="asis", echo = FALSE}
kable(hbcodvsarmed1, format.args = list(big.mark = ","), caption = "Cause of death of black men and civilian armed status, part A")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

```{r, results="asis", echo = FALSE}
kable(hbcodvsarmed2, format.args = list(big.mark = ","), caption = "Cause of death of black men and civilian armed status, part B")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

Of the `r as.character(prettyNum(nh, big.mark = ','))` deaths of men, `r nhh`, approximately `r  pct(nhh,nh)`%, were hispanic. Tables show percentages of deaths among hispanic men.

```{r, results="asis", echo = FALSE}
kable(hhcodvsarmed1, format.args = list(big.mark = ","), caption = "Cause of death of hispanic men and civilian armed status, part A")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

```{r, results="asis", echo = FALSE}
kable(hhcodvsarmed2, format.args = list(big.mark = ","), caption = "Cause of death of black men and civilian armed status, part B")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

## Geographic Analysis of the Data

One way to look at the data geograhically is to map the number of deaths by state. We tabulated percentages of deaths occurring by state. We can also show the *numbers*.

```{r, results="asis", echo = FALSE}
kable(count(counted,id), format.args = list(big.mark = ","), caption = "Number deaths by state")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 

```

### Many Datasets are Distributed Similarly to Population

Counts of some features often parallel counts of population. As a result, a map that shows the raw counts may be difficult to distinguish from a map of population. For example

```{r results="asis", echo = FALSE}
p + theme(legend.position = "bottom") + ggtitle("Police Involved Civilian Homicides, 2015")
```


```{r results="asis", echo = FALSE}
q + theme(legend.position = "bottom") + ggtitle("Estimated Population, 2015")
```

Comparing the two maps above, police involved civilian homicides are approximately proportional to state population. Some states, such as New York, have fewer police involved civilian homicides than would be expected based only on population, while other states, such as Oklahoma, have more. However, the ratios of police involved civilian homicides per hundred thousand population is a much different picture, showing strong regional differences.

```{r results="asis", echo = FALSE}
r + theme(legend.position = "bottom") + ggtitle("Police Involved Civilian Homicides, 2015\n, Number per 100,000 Population")
```

In general, states west of the Mississippi River have higher rates of police involved civilian homicides. North Dakota's low rate is an exception in the West, while Louisiana and West Virigia have higher rates than the other states in the East.

## What conclusions can we draw from the short, national summary?

Before attempting an answer, it would be well to remember that the United States is composed of 50 states and the District of Columbia, many of which has hundreds or thousands of separate law enforement agencies that operate in communities that range in diversity from little at all to very diverse in terms of national origin, ethnicity, duration of residence and that may have relatively high or relatively low unemployment and relatively different perspectives on law enforcement. 

### Detail underlying deaths of women

In short, these data are best to pose questions, rather than to provide answers. For example `r pct(nrow(fveh),nf)`% of deaths of women involved situations in which the woman was unarmed and struck by a vehicle.

* [Hue Dang]
* [Nuwnah Laroche]
* [Barbara Ramey]
* [Kimberly Bedford]
* [Kylie Lindsey]
* [Isabella Chinchilla]
* [Bendetta 'Lynn' Miller]

In reviewing the related news reports, these deaths were accidental in nature (striking pedestrian in intersection, striking pedestrians walking on highway, for example). In the case of [Kimberly Bedford], the officer was convicted of a serious traffic offense for hitting a pedestrian while speeding without lights or siren. Thus, although the relative proportion, `r pct(nrow(fveh),nf)`%, of unarmed women struck by vehicles is notable, these deaths are quite unlike deaths inflicted by gunshot on unarmed civilians. Without the addition of facts in addition to those from from news accounts, the `r pct(nrow(fveh),nf)`% figure, by itself, is misleading.

For another example, `r sum(codvsarmed["Taser",])`% of all deaths were caused by Tasers. Among women, however, only `r sum(fcodvsarmed["Taser",])`% were due to Tasers. Yet, among white women `r sum(fwcodvstaser)`% of deaths were due to Tasers while `r sum(fbcodvsarmed["Taser",])`% of deaths of black women were Taser inflicted. That was the death of [Natasha McKenna], who was a mentally ill, 5'4" tall, 180-pound woman in handcuffs and leg shackles. Six deputies were attempting to transfer her from a jail cell and administered 4 Taser shocks that proved fatal. [The coroner]) ruled the death accidental. [A comprehensive report] details the circumstances involved in the administration of the Taser shocks and the difficulties in physically controlling the prisoner that led to the decisions to administer each of them.

Among all women, a single case of death in custody was reported, an hispanic woman who died from lack of timely medical attention after complaining of feeling unwell. The outside contractor providing medical services decided to delay services until the regularly scheduled evening rounds. Lobato died shortly after the start of rounds before being seen.

The deaths of two women, one, [Natasha McKenna,] and the other, [Jennifer Lobato], are similar in that they were both related to medical treatment in custody (one was being attempted to be transferred to a medical facility; the other died waiting medical attention).

Percentages of deaths of white and black women killed by police gunshot while in vehicles are comparable.

* [Alice Brown]
* [Mya Hall]
* [Karen Janks]
* [Christie Cathers]

All of the cases of deaths of women in vehicles by police gunshot involved situations in which the drivers were endangering the lives of civilians or officers.

In the one case of a woman, [Candace Blakley], classified as "unknown" as to whether she was armed, the cause of death was the unintentional discharge of a rifle by her husband, an officer, off duty at home. The officer was charged with involuntary manslaughter.

There was one case of a woman, [Shelly Lynn Haendiges], who died by police gunshot when she pointed a pellet gun at him in the course of an armed robbery. The [prosecutor] did not charge the officer; one of the factors appeared to be that the pellet gun closely resembled an actual semi-automatic pistol. The [family] supported the officer's action, and attributed the death to their family member's mental illness.

All other causes of deaths of women involve gunshots in situations in which the women were either armed with firearms or knives or were unarmed. Percentages of white women armed with firearms were approximately twice those of black women; percentages of black women armed with knives were approximately three times those of white women. The percentages of unarmed white and black women killed by gunshot are comparable.

In deaths involving civilians armed with knives:

* [Kristiana Coignard]
* [Tiffany Terry]
* [Janisha Fonville]
* [Jessica Uribe]
* [Monique Deckard]
* [Meagan Hockaday]
* [Nikki Burtsfield]
* [Redel Jones]
* [Norma Guzman]
* [Phyllis Jepsen]
* [Siolosega Velega-Nuufolau]

each involved situations in which the civilian threatened the officer with a knife; many of these civilians were afflicted with mental illnesses or under the influence of various substances.

The largest category of deaths of women involved death by police gunshot was in situations in which the woman was armed with a firearm.

* [Yuvette Henderson]
* [Betty Sexton]
* [Crystal Miley]
* [Stephanie Hill]
* [Kaylene Stone]
* [Deanne Choate]
* [Alexia Christian]
* [Cassandra Bolin]
* [Wendy Chappell]
* [Tamala Satre]
* [Tina Money]
* [Margaret Wagner]
* [Michelle Burg]
* [Linda Lush]
* [Marquesha McMillan]
* [Laura Lemieux]
* [Somer Speer]
* [Tashfeen Malik]
* [Sheilah Huck]
* [Shirley Weis]
* [Brenda Kimberling]
* [Erica Lauro]

The circumstances surrounding these deaths range from exchange of gunfire with a mass shooter, to armed robberies to civilians suffering mental disorders, under the influence of alcohol or other drugs and other situations involving civilians armed with firearms whose actions threatened police or bystanders.

Among women, this leaves police involved homicides by gunshot in which the civilian was unarmed. Three of the women were white and two were black.

* [Autumn Steele]
* [Christina Prestianni]
* [Tamara Seidle]
* [India Kager]
* [Bettie Jones]

Of these five deaths, two involved off-duty officers killing their girlfriend and wife, respectively; one death occurred when the driver was accidentially shot during a gun fight between her passenger and police; and one death occurred when a woman was accidentally shot as police shot at a young man holding a baseball bat in circumstances when opening the door to the victim's residence. The remaining death was also accidental. It involved a domestic dispute in the couple's front yard. The officer was talking to the husband. The wife,[Autumn Steele], and a large dog ran out of the house. The dog knocked over the officer. Neither owner restrained the dog. The officer shot to protect himself from the dog and hit the wife by accident. 

In the only case involving a "non-conforming" civilian [Jesse or Jessie Herandez], police killed the transgender driver of a car moving toward an officer in an attempt to avoid an investigation. The department, as a result, changed its policy to declassify vehicles as deadly weapons justifying the response of deadly force in most cases.

Most of the police involved civilian homicides involving deaths of women were of white or black women. Based on her Samoan given and surname, one woman, [Siolosega Velega-Nuufolau], should be classified as "Asian/Pacific Islander" and another, [Tashfeen Malik], should be classified as "Arab-American," as a Pakistani-born person.)

The review of the relatively few deaths of women illustrate the hazards inherent in observational data consisting only of a single outcome under a variety of circumstances at different times and places involving different types of law enforcement agencies and situations. 

Because this is an **observational** study, it will not be possible to state the *causes* of police involved homicides, only look for associated statistical factors.

### Cautionary Example

For a relatively small dataset, the `r as.character(prettyNum(nf, big.mark = ','))` deaths of women represent a large variety of circumstances. It's a classic problem in analysis, which is trying to put too many pigeons into too few pigeon holes.

## Next steps

As I update this work, I will be looking at two broad areas.

### Finer geographic detail

The state level is not the right geographic unit for comparative analysis. Fortunately, [The Counted] provided sufficient location information to geocode the data. This will enable me to analyze data by the demographic characteristics at the Congressional District level. The advantage is that those units are roughly equal in population (ranging between around 800,000 and 1,000,000).

### Testing hypotheses

At the Congressional District level, are police involved homicides randomly distributed among the population? Which demographic variables or combination of variables are associated with the deaths. Are statistics on arrest data (police/civilian encounters with non-lethal outcomes) available and do they provide a basis to construct a predictive model?

[A comprehensive report]: http://www.fairfaxcounty.gov/news/2015/report_of_investigation_of_in-custody_death.pdf
[Alexia Christian]: http://www.clatl.com/news/article/13082907/what-happened-in-the-death-of-alexia-christian
[Alice Brown]: http://www.sfgate.com/news/article/Driver-killed-by-S-F-police-identified-as-6142564.php
[American Fact Finder]: http://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?src=bkmk
[Autumn Steele]: http://www.desmoinesregister.com/story/news/crime-and-courts/2015/03/24/burlington-police-fatal-shooting-body-cameras/70373048/
[Barbara Ramey]: http://www.whsv.com/home/headlines/State-Trooper-Involved-in-Luray-Crash-304924801.html
[Bendetta 'Lynn' Miller]: http://www.pennlive.com/midstate/index.ssf/2015/10/outrage_over_innocent_woman_ki.html
[Bettie Jones]: http://abcnews.go.com/US/teen-shot-chicago-cops-called-911-times/story?id=36524006
[Betty Sexton]: http://www.wbtv.com/story/28129851/officer-involved-shooting-in-gastonia-sends-woman-to-hospital
[Brenda Kimberling]: http://www.reviewjournal.com/news/las-vegas/police-describe-details-recent-officer-involved-shootings
[Candace Blakley]: http://www.northaugustastar.com/article/20150615/STAR01/150619665/
[Cassandra Bolin]: http://kxan.com/2015/05/25/woman-shot-dead-after-five-hour-standoff-with-apd-swat-officers/
[Census Projections]: https://www.census.gov/content/dam/Census/library/publications/2015/demo/p25-1143.pdf
[Christie Cathers]: http://www.wvalways.com/story/29256189/update-grand-jury-decides-fatal-monongalia-county-shooting-by-deputy-was-justifiable
[Christina Prestianni]: http://www.nj.com/essex/index.ssf/2015/01/corrections_officer_shot_girlfriend_in_apparent_mu.html
[Crystal Miley]: http://www.al.com/news/birmingham/index.ssf/2015/03/gun-wielding_alabama_woman_sho.html
[Deanne Choate]: http://www.kansascity.com/news/local/crime/article17157194.html
[Erica Lauro]: http://www.reviewjournal.com/news/las-vegas/police-describe-details-recent-officer-involved-shootings
[family]: http://kokomoherald.com/Content/News/All-News/Article/Haendiges-family-releases-statement-/1/98/24260
[Health, United States, 2015 - Individual Charts and Tables: Spreadsheet, PDF, and PowerPoint files, Table 17]: (http://www.cdc.gov/nchs/data/hus/2015/017.pdf
[Hue Dang]: http://www.northjersey.com/news/pedestrian-fatally-struck-by-car-driven-by-employee-of-bergen-county-prosecutor-s-office-1.1286154
[India Kager]: http://wtkr.com/2015/09/06/two-killed-after-shots-fired-at-police-from-vehicle-infant-inside-unhurt/
[interactive]: http://www.theguardian.com/us-news/ng-interactive/2015/jun/01/the-counted-police-killings-us-database
[Isabella Chinchilla]: http://www.myajc.com/news/news/local/ex-trooper-avoids-charges-in-crash-families-of-tee/nqSY2/?icmp=AJC_internallink_021816_AJCtoMyAJC_trooper
[Janisha Fonville]: http://wfae.org/post/cmpd-officer-fatally-shot-woman-who-lunged-police-knife
[Jennifer Lobato]: http://www.columbinecourier.com/content/autopsy-jail-inmate-died-excessive-vomiting
[Jesse or Jessie Herandez]: http://denver.cbslocal.com/2015/06/09/moving-cars-no-longer-considered-deadly-weapons-for-dpd/
[Jessica Uribe]: http://www.kvoa.com/story/28230565/woman-killed-after-officer-involved-shooting-on-westside
[Karen Janks]: http://www.pressdemocrat.com/news/3841197-181/wild-chase-of-wrong-way-highway?gallery=3870520&artslide=0
[Kaylene Stone]: http://www.abc15.com/news/region-west-valley/peoria/glendale-peoria-pd-investigating-shooting
[Kimberly Bedford]: http://fox17online.com/2016/02/19/former-benton-twp-officer-sentenced-for-killing-woman-in-crash/
[Kristiana Coignard]: https://www.news-journal.com/news/2015/jun/27/longview-police-officers-cleared-in-teens-deadly-s/
[Kylie Lindsey]: http://www.myajc.com/news/news/local/ex-trooper-avoids-charges-in-crash-families-of-tee/nqSY2/?icmp=AJC_internallink_021816_AJCtoMyAJC_trooper
[Laura Lemieux]: http://www.postandcourier.com/article/20160109/PC16/160119982/in-berkeley-county-mom-x2019-s-death-poses-questions-about-how-mentally-ill-are-handled-before-police-shootings
[Linda Lush]: http://www.reviewjournal.com/news/las-vegas/armed-woman-killed-metro-left-suicidal-notes-had-been-drinking-bourbon
[Lunacy and the Full Moon]: http://www.scientificamerican.com/article/lunacy-and-the-full-moon/
[Margaret Wagner]: http://www.pe.com/articles/wagner-783208-home-vasquez.html
[Marquesha McMillan]: https://www.washingtonpost.com/local/public-safety/man-charged-in-burglary-that-ended-in-shootout-with-dc-police/2015/10/27/4d3608b2-7cd6-11e5-b575-d8dcfedb4ea1_story.html
[Meagan Hockaday]: http://www.keyt.com/news/woman-killed-in-oxnard-officer-involved-shooting/32066036
[Michelle Burg]: http://archive.naplesnews.com/news/crime/woman-dies-after-chase-gunfire-exchange-with-collier-deputies-ep-1320771152-340322001.html
[Monique Deckard]: http://orangecountyda.org/civicax/filebank/blobdload.aspx?BlobID=23399
[Mya Hall]: http://www.chicagotribune.com/news/nationworld/chi-nsa-shooting-20150401-story.html
[Natasha McKenna]: https://www.washingtonpost.com/news/local/wp/2015/04/13/the-death-of-natasha-mckenna-in-the-fairfax-jail-the-rest-of-the-story/
[Nikki Burtsfield]: http://www.gillettenewsrecord.com/news/local/article_af6fef0a-af06-5923-b8bc-6f8cdb691337.html
[Norma Guzman]: http://www.nbclosangeles.com/news/local/Video-Sheds-Light-on-Police-Shooting-of-Mentally-Ill-Woman-with-Knife-374850811.html
[Nuwnah Laroche]: http://newyork.cbslocal.com/2015/05/06/nj-turnpike-pedestrians-killed-police-car/
[Phyllis Jepsen]: (http://www.mentalhealthportland.org/what-happened-to-phyllis-jepsen/
[prosecutor]: http://www.wthr.com/article/no-charges-for-kokomo-police-officer-who-fatally-shot-female-robbery-suspect-at-gas-station
[Redel Jones]: http://www.latimes.com/local/lanow/la-me-ln-lapd-shooting-redel-jones-20160712-snap-story.html
[Sheilah Huck]: http://www.stltoday.com/news/local/crime-and-courts/woman-fatally-shot-by-st-louis-county-police-was-bizarre/article_8d80dbf7-0abd-5605-8c29-14f7ee2a4195.html
[Shelly Lynn Haendiges]: http://www.kokomotribune.com/news/updated-family-of-girl-killed-in-armed-robbery-retains-wrongful/article_b04bfa58-1429-11e5-9c05-3f07d22d3ea4.html
[Shirley Weis]: http://www.star-telegram.com/news/local/community/arlington/article49578020.html
[Siolosega Velega-Nuufolau]: (http://www.modbee.com/news/local/crime/article52053610.html
[Somer Speer]: http://www.news-leader.com/story/news/local/christian-county/2016/05/09/woman-killed-police-ozark-firefight-suicidal-homicidal-records-show/83656806/
[Stephanie Hill]: http://www.sfgate.com/bayarea/article/Penngrove-slaying-suspect-ID-d-after-cops-kill-6113061.php
[Tamala Satre]: http://www.sacbee.com/news/local/crime/article28452859.html
[Tamara Seidle]: http://www.nj.com/monmouth/index.ssf/2016/03/suspended_neptune_cop_has_status_hearing_in_ex-wif.html
[Tashfeen Malik]: http://www.latimes.com/nation/la-na-malik-visa-application-20151222-story.html
[The coroner]: https://www.washingtonpost.com/local/crime/death-of-woman-shocked-by-stun-gun-in-fairfax-jail-is-ruled-an-accident/2015/04/28/7bc85f36-edfc-11e4-a55f-38924fca94f9_story.html
[The Counted]: https://interactive.guim.co.uk/2015/the-counted/thecounted-data.zip
[Tiffany Terry]: http://www.wowt.com/home/headlines/Shooting-Victim-Taken-to-Hospital--289991681.html
[Tina Money]: http://www.ksbw.com/article/two-people-shot-and-killed-in-sand-city-identified/1057674
[Wendy Chappell]: http://abc3340.com/archive/coroner-maylene-woman-killed-in-officer-involved-shooting-in-clanton
[Yuvette Henderson]: http://www.mercurynews.com/crime-courts/ci_27461977/oakland-woman-killed-by-emeryville-police-tried-carjack

<!--chapter:end:01-police.Rmd-->

# The Financial Cash Flow Model: Python on Wall Street {#dectab}

*Keywords: fintec, data wrangling, Python*

## Case Description

After the financial crisis beginning in 2008, the Securities and Exchange Commission issued a proposed [rulemaking]("https://www.sec.gov/rules/proposed/2010/33-9117.pdf") in 2010 that asked whether it should require whether it should require

> The asset-level information ... according to proposed standards and in a tagged data format using eXtensible Markup Language (XML)... [and the] filing of a computer program of the contractual cash flow provisions expressed as downloadable source code in Python

in offerings of residential mortgage backed securities and other asset types. In the trade, the asset-level information is called *the tape.*

In my [comment letter]("https://www.sec.gov/comments/s7-08-10/s70810-41.pdf"), I supported both requirements and provided a demonstration of how they would work based on an [actual transaction]("https://www.sec.gov/Archives/edgar/data/1176320/000114420410022414/v182145_424b5.htm").

## XML Conversion

The [asset data]("https://www.sec.gov/Archives/edgar/data/1490028/000114420410022348/v182018_fwp.htm") for this transaction was filed in HTML format in particularly ugly form in multiple tables.


        </font></td>
                  </tr>
                  <tr bgcolor="white">
                    <td align="right" valign="top" width="3%">
                      <div style="DISPLAY: block; MARGIN-LEFT: 0pt; TEXT-INDENT: 0pt; MARGIN-RIGHT: 0pt" align="right"><font style="DISPLAY: inline; FONT-SIZE: 8pt; COLOR: #000000; FONT-FAMILY: times new roman"><font style="DISPLAY: inline; COLOR: #000000">6</font></font></div>
                    </td>
                    <td align="right" valign="top" width="7%">
                      <div style="DISPLAY: block; MARGIN-LEFT: 0pt; TEXT-INDENT: 0pt; MARGIN-RIGHT: 0pt" align="right"><font style="DISPLAY: inline; FONT-SIZE: 8pt; COLOR: #000000; FONT-FAMILY: times new roman"><font style="DISPLAY: inline; COLOR: #000000">1000115</font></font></div>
                    </td>
                    <td align="right" valign="top" width="7%">
                      <div style="DISPLAY: block; MARGIN-LEFT: 0pt; TEXT-INDENT: 0pt; MARGIN-RIGHT: 0pt" align="right"><font style="DISPLAY: inline; FONT-SIZE: 8pt; COLOR: #000000; FONT-FAMILY: times new roman"><font style="DISPLAY: inline; COLOR: #000000">0.0025</font></font></div>
                    </td>
                    <td valign="top" width="5%"><font style="DISPLAY: inline; FONT-SIZE: 8pt; FONT-FAMILY: times new roman">&#160;
        </font></td>
                    <td valign="top" width="5%"><font style="DISPLAY: inline; FONT-SIZE: 8pt; FONT-FAMILY: times new roman">&#160;
        </font></td>
                    <td align="right" valign="top" width="5%">
                      <div style="DISPLAY: block; MARGIN-LEFT: 0pt; TEXT-INDENT: 0pt; MARGIN-RIGHT: 0pt" align="right"><font style="DISPLAY: inline; FONT-SIZE: 8pt; COLOR: #000000; FONT-FAMILY: times new roman"><font style="DISPLAY: inline; COLOR: #000000">1000115</font></font></div>
                    </td>
                    <td valign="top" width="5%"><font style="DISPLAY: inline; FONT-SIZE: 8pt; FONT-FAMILY: times new roman">&#160;
        </font></td>
                    <td align="right" valign="top" width="5%">
                      <div style="DISPLAY: block; MARGIN-LEFT: 0pt; TEXT-INDENT: 0pt; MARGIN-RIGHT: 0pt" align="right"><font style="DISPLAY: inline; FONT-SIZE: 8pt; COLOR: #000000; FONT-FAMILY: times new roman"><font style="DISPLAY: inline; COLOR: #000000">5264358737</font></font></div>
                    </td>
                    <td align="right" valign="top" width="5%">
                      <div style="DISPLAY: block; MARGIN-LEFT: 0pt; TEXT-INDENT: 0pt; MARGIN-RIGHT: 0pt" align="right"><font style="DISPLAY: inline; FONT-SIZE: 8pt; COLOR: #000000; FONT-FAMILY: times new roman"><font style="DISPLAY: inline; COLOR: #000000">2</font></font></div>
                    </td>
                    <td align="right" valign="top" width="5%">
                      <div style="DISPLAY: block; MARGIN-LEFT: 0pt; TEXT-INDENT: 0pt; MARGIN-RIGHT: 0pt" align="right"><font style="DISPLAY: inline; FONT-SIZE: 8pt; COLOR: #000000; FONT-FAMILY: times new roman"><font style="DISPLAY: inline; COLOR: #000000">1</font></font></div>

Printed out, this is about 12 pages, depending on your printer. If you assigned a conscientious junior lawyer to perform a count, he or she would report back that there are approximately 2,843 lines, 34,524 words and 195,652 characters visible. (Junior lawyers, by and large have a limited understanding of the word *approximate.*) If, however, you asked your IT person the same question, you would learn that there are exactly 137,811 lines, 56,881 words and 2,689,760 characters. Why did the lawyer only pick up a little more than 7% of the bytes?

They are both right from their perspectives of what the eye can see and what the computer has to process. The difference is that vast proportions of the file containing the data is devoted to making it appear as if it were printed. That's fine if what you plan to do is read. If you want to perform data crunching, however, say to run your own model on the tape, you have to get rid of a lot of crud before you can proceed.

Here's the typical payload of one of the HTML blocks:

    9
    1000115
    0.0025
    1000115
    1332854261
    2
    1
    0
    9
 
 We would prefer, of course, a comma delimited file
 
    2,1000115,0.0025,NULL,1000115,,NULL,6875009669,2,1,0,9,,NULL,NULL,NULL,1,4,0.00,NULL,NULL,,242000,NULL,2009-05-26,623700,0.04500,240,,360,2009-07-01,NULL,120,NULL,,552900.71,552900.71,0.04500,2073.38,2010-04-01,,NULL,39,45,0.02250,NULL,0.00125,,60,0.05000,0.02250,12,0.02000,0.02000,0.09500,0.0225,0,NULL,,60,12,NULL,NULL,,0,NULL,NULL,0,NULL,NULL,0,,0,NULL,7.70,4.70,5.00,NULL,,NULL,NULL,NULL,722,778,NULL,,NULL,NULL,NULL,,NULL,770000000000,NULL,12500,0.00,9422.3,0.00,12500,21922.3,,1,5,NULL,3,NULL,4,,NULL,184669.53,NULL,0.23310,4,NULL,WILMETTE,IL,60091,1,1,,NULL,1500000,NULL,NULL,,NULL,NULL,NULL,0.57710,0.41580,0.00,0,0,NULL,,NULL,0.23310,NULL,NULL,,NULL,NULL,NULL,NULL,Full,Doc,less,than,12,months,,1,125.54,8.00,11.00,225939,2039-06-01, 3,1000115,0.0025,NULL,1000115

XML is potentially a large improvement over HTML. It does one thing very well, which is to separate content from decoration. All of the decisions about font, size, color,alignment, etc., can be isolated to a separate file, called a stylesheet. Here is an XML file of the tape with a minimalist style rendering.

There is, however, a rub. To allow the ability to decorate the content, the designers of XML require, in effect, a new header for every row of data. So, the first of the 255 rows in the XML version looks like:
<pre>
&lt;record&gt;
    &lt;field name="id"&gt;1&lt;/field&gt;
    &lt;field name="servicer"&gt;1000115&lt;/field&gt;
    &lt;field name="sfpct"&gt;0.002500&lt;/field&gt;
    &lt;field name="sfamt"&gt;0.00&lt;/field&gt;
    &lt;field name="adv"&gt;0&lt;/field&gt;
    &lt;field name="orig"&gt;1000115&lt;/field&gt;
    &lt;field name="lg"&gt;NULL&lt;/field&gt;
    &lt;field name="lnum"&gt;2147483647&lt;/field&gt;
    &lt;field name="amtype"&gt;2&lt;/field&gt;
    &lt;field name="lienpos"&gt;1&lt;/field&gt;
    &lt;field name="heloc"&gt;0&lt;/field&gt;
    &lt;field name="purpose"&gt;9&lt;/field&gt;
    &lt;field name="cashoutamt"&gt;0.00&lt;/field&gt;
    &lt;field name="points"&gt;0.000&lt;/field&gt;
    &lt;field name="chcl"&gt;0&lt;/field&gt;
    &lt;field name="relo"&gt;0&lt;/field&gt;
    &lt;field name="broker"&gt;0&lt;/field&gt;
    &lt;field name="channel"&gt;1&lt;/field&gt;
    &lt;field name="escrecord"&gt;0&lt;/field&gt;
    &lt;field name="balsenior"&gt;0.00&lt;/field&gt;
    &lt;field name="ltypesr"&gt;0&lt;/field&gt;
    &lt;field name="hybridper"&gt;0&lt;/field&gt;
    &lt;field name="negamlmtsr"&gt;0.0000&lt;/field&gt;
    &lt;field name="jrbal"&gt;0.00&lt;/field&gt;
    &lt;field name="odatesenior"&gt;0000-00-00&lt;/field&gt;
    &lt;field name="odate"&gt;2009-06-23&lt;/field&gt;
    &lt;field name="obal"&gt;446000.00&lt;/field&gt;
    &lt;field name="oint"&gt;0.0475&lt;/field&gt;
    &lt;field name="oterm"&gt;240&lt;/field&gt;
    &lt;field name="ottm"&gt;360&lt;/field&gt;
    &lt;field name="fpd"&gt;2009-08-01&lt;/field&gt;
    &lt;field name="inttype"&gt;0&lt;/field&gt;
    &lt;field name="intonlyterm"&gt;120&lt;/field&gt;
    &lt;field name="bdownper"&gt;0&lt;/field&gt;
    &lt;field name="helocper"&gt;0&lt;/field&gt;
    &lt;field name="cbal"&gt;446000.00&lt;/field&gt;
    &lt;field name="sbal"&gt;446000.00&lt;/field&gt;
    &lt;field name="cintpct"&gt;0.0475&lt;/field&gt;
    &lt;field name="cintamt"&gt;1765.42&lt;/field&gt;
    &lt;field name="ptd"&gt;2010-03-01&lt;/field&gt;
    &lt;field name="cstatus"&gt;0&lt;/field&gt;
    &lt;field name="indextype"&gt;39&lt;/field&gt;
    &lt;field name="lookdays"&gt;45&lt;/field&gt;
    &lt;field name="gmargin"&gt;0.0225&lt;/field&gt;
    &lt;field name="rounded"&gt;0&lt;/field&gt;
    &lt;field name="roundfac"&gt;0.0012&lt;/field&gt;
    &lt;field name="ofixper"&gt;60&lt;/field&gt;
    &lt;field name="ocapup"&gt;0.0500&lt;/field&gt;
    &lt;field name="ocapdn"&gt;0.0250&lt;/field&gt;
    &lt;field name="resetper"&gt;12&lt;/field&gt;
    &lt;field name="capup"&gt;0.0200&lt;/field&gt;
    &lt;field name="capdn"&gt;0.0200&lt;/field&gt;
    &lt;field name="ceiling"&gt;0.0975&lt;/field&gt;
    &lt;field name="floor"&gt;0.0225&lt;/field&gt;
    &lt;field name="negammax"&gt;0.0000&lt;/field&gt;
    &lt;field name="orecast"&gt;0&lt;/field&gt;
    &lt;field name="recast"&gt;0&lt;/field&gt;
    &lt;field name="ofixedpay"&gt;60&lt;/field&gt;
    &lt;field name="spayreset"&gt;12&lt;/field&gt;
    &lt;field name="opercap"&gt;0.0000&lt;/field&gt;
    &lt;field name="percap"&gt;0.0000&lt;/field&gt;
    &lt;field name="opayreset"&gt;0&lt;/field&gt;
    &lt;field name="payreset"&gt;0&lt;/field&gt;
    &lt;field name="optionarm"&gt;0&lt;/field&gt;
    &lt;field name="optionrecast"&gt;0&lt;/field&gt;
    &lt;field name="ominpay"&gt;0.00&lt;/field&gt;
    &lt;field name="minpay"&gt;0.00&lt;/field&gt;
    &lt;field name="prepaycalc"&gt;0&lt;/field&gt;
    &lt;field name="prepaytype"&gt;0&lt;/field&gt;
    &lt;field name="prepayterm"&gt;0&lt;/field&gt;
    &lt;field name="prepayhard"&gt;0&lt;/field&gt;
    &lt;field name="pid"&gt;0&lt;/field&gt;
    &lt;field name="propnum"&gt;0&lt;/field&gt;
    &lt;field name="borrecorders"&gt;0&lt;/field&gt;
    &lt;field name="selfemp"&gt;0&lt;/field&gt;
    &lt;field name="comonpay"&gt;0.00&lt;/field&gt;
    &lt;field name="pempl"&gt;36.00&lt;/field&gt;
    &lt;field name="sempl"&gt;0.00&lt;/field&gt;
    &lt;field name="yearshome"&gt;8.00&lt;/field&gt;
    &lt;field name="ficomodel"&gt;0&lt;/field&gt;
    &lt;field name="ficodate"&gt;0000-00-00&lt;/field&gt;
    &lt;field name="pequifax"&gt;0&lt;/field&gt;
    &lt;field name="pexperian"&gt;0&lt;/field&gt;
    &lt;field name="ptransu"&gt;0&lt;/field&gt;
    &lt;field name="sequifax"&gt;0&lt;/field&gt;
    &lt;field name="sexperian"&gt;0&lt;/field&gt;
    &lt;field name="stranstransu"&gt;0&lt;/field&gt;
    &lt;field name="pofico"&gt;802&lt;/field&gt;
    &lt;field name="prfico"&gt;806&lt;/field&gt;
    &lt;field name="srfico"&gt;0&lt;/field&gt;
    &lt;field name="cficometh"&gt;0&lt;/field&gt;
    &lt;field name="pvant"&gt;0&lt;/field&gt;
    &lt;field name="svant"&gt;0&lt;/field&gt;
    &lt;field name="cvantmeth"&gt;0&lt;/field&gt;
    &lt;field name="vantdate"&gt;0000-00-00&lt;/field&gt;
    &lt;field name="longtrade"&gt;NULL&lt;/field&gt;
    &lt;field name="maxtrade"&gt;0.00&lt;/field&gt;
    &lt;field name="numtrade"&gt;0&lt;/field&gt;
    &lt;field name="tradeuse"&gt;0.00&lt;/field&gt;
    &lt;field name="payhist"&gt;770000000000&lt;/field&gt;
    &lt;field name="monbk"&gt;0&lt;/field&gt;
    &lt;field name="monfc"&gt;0&lt;/field&gt;
    &lt;field name="pwage"&gt;8750.00&lt;/field&gt;
    &lt;field name="swage"&gt;0.00&lt;/field&gt;
    &lt;field name="pothinc"&gt;24883.57&lt;/field&gt;
    &lt;field name="sothinc"&gt;0.00&lt;/field&gt;
    &lt;field name="allwage"&gt;8750.00&lt;/field&gt;
    &lt;field name="alltot"&gt;33633.57&lt;/field&gt;
    &lt;field name="t_4506"&gt;1&lt;/field&gt;
    &lt;field name="pincver"&gt;5&lt;/field&gt;
    &lt;field name="sincver"&gt;0&lt;/field&gt;
    &lt;field name="pempver"&gt;3&lt;/field&gt;
    &lt;field name="sempver"&gt;0&lt;/field&gt;
    &lt;field name="pastver"&gt;4&lt;/field&gt;
    &lt;field name="sastver"&gt;0&lt;/field&gt;
    &lt;field name="liquid"&gt;250000.00&lt;/field&gt;
    &lt;field name="mondebt"&gt;0.00&lt;/field&gt;
    &lt;field name="odti"&gt;0.11&lt;/field&gt;
    &lt;field name="fullindex"&gt;4&lt;/field&gt;
    &lt;field name="ownfundsdown"&gt;0.00&lt;/field&gt;
    &lt;field name="city"&gt;CLARKSTON&lt;/field&gt;
    &lt;field name="state"&gt;MI&lt;/field&gt;
    &lt;field name="zip"&gt;48348&lt;/field&gt;
    &lt;field name="ptype"&gt;1&lt;/field&gt;
    &lt;field name="occ"&gt;1&lt;/field&gt;
    &lt;field name="price"&gt;0.00&lt;/field&gt;
    &lt;field name="oappr"&gt;575000.00&lt;/field&gt;
    &lt;field name="ovaltype"&gt;0&lt;/field&gt;
    &lt;field name="ovaldate"&gt;0000-00-00&lt;/field&gt;
    &lt;field name="oavm"&gt;0&lt;/field&gt;
    &lt;field name="oavmscore"&gt;0.0000&lt;/field&gt;
    &lt;field name="rpval"&gt;0.00&lt;/field&gt;
    &lt;field name="rpvaltype"&gt;0&lt;/field&gt;
    &lt;field name="rpvaldate"&gt;0000-00-00&lt;/field&gt;
    &lt;field name="ravm"&gt;0&lt;/field&gt;
    &lt;field name="ravmscore"&gt;0.0000&lt;/field&gt;
    &lt;field name="ocltv"&gt;0.78&lt;/field&gt;
    &lt;field name="oltv"&gt;0.78&lt;/field&gt;
    &lt;field name="opledge"&gt;0.00&lt;/field&gt;
    &lt;field name="micomp"&gt;0&lt;/field&gt;
    &lt;field name="mipct"&gt;0.00&lt;/field&gt;
    &lt;field name="poolcomp"&gt;0&lt;/field&gt;
    &lt;field name="stoploss"&gt;0.0000&lt;/field&gt;
    &lt;field name="micert"&gt;NULL&lt;/field&gt;
    &lt;field name="rdtifront"&gt;0.11&lt;/field&gt;
    &lt;field name="rdtibback"&gt;0.00&lt;/field&gt;
    &lt;field name="modpaydate"&gt;0000-00-00&lt;/field&gt;
    &lt;field name="totcap"&gt;0.00&lt;/field&gt;
    &lt;field name="totdef"&gt;0.00&lt;/field&gt;
    &lt;field name="premodint"&gt;0.00&lt;/field&gt;
    &lt;field name="premodpi"&gt;0.00&lt;/field&gt;
    &lt;field name="premodoicap"&gt;0.00&lt;/field&gt;
    &lt;field name="premodsubicap"&gt;0.00&lt;/field&gt;
    &lt;field name="premodnxtdate"&gt;0000-00-00&lt;/field&gt;
    &lt;field name="premodioterm"&gt;0&lt;/field&gt;
    &lt;field name="fbal"&gt;0.00&lt;/field&gt;
    &lt;field name="fint"&gt;0.00&lt;/field&gt;
    &lt;field name="doccode"&gt;Citiquik process&lt;/field&gt;
    &lt;field name="rwtinc"&gt;less than 12 months&lt;/field&gt;
    &lt;field name="rwtast"&gt;1&lt;/field&gt;
    &lt;field name="cashatclose"&gt;1048.73&lt;/field&gt;
    &lt;field name="pyrind"&gt;36.00&lt;/field&gt;
    &lt;field name="syrind"&gt;0.00&lt;/field&gt;
    &lt;field name="jrdrawn"&gt;0.00&lt;/field&gt;
    &lt;field name="maturity"&gt;2039-07-01&lt;/field&gt;
  &lt;/record&gt;
</pre>
and every following row, except for the few bytes devoted to data, looks the same, bulking the tape up almost to the size of the HTML version.

## From XML to Plain Text
The good news is that a few lines of Python is sufficient to make the HTML-XML conversion. The next step is to set up a template:

<?xml version="1.0"?>
    <xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform">

    <xsl:template match="/">
    <HTML>
    <BODY>
        <xsl:apply-templates/>
    </BODY>
    </HTML>
    </xsl:template>

    <xsl:template match="/*">
    <TABLE BORDER="0">
    <TR>
            <xsl:for-each select="*[position() = 1]/*">
              <TD>
                  <xsl:value-of select="local-name()"/>
              </TD>
            </xsl:for-each>
    </TR>
          <xsl:apply-templates/>
    </TABLE>
    </xsl:template>

    <xsl:template match="/*/*">
    <TR>
        <xsl:apply-templates/>
    </TR>
    </xsl:template>

    <xsl:template match="/*/*/*">
    <TD>
        <xsl:value-of select="."/>
    </TD>
    </xsl:template>

    </xsl:stylesheet>
    
to apply to the conversion, using

    import amara # package for parsing xml
    doc = amara.parse("xmlsample.xhtml") #URL
    sequoia = doc.xml_children[1] # skip 0th item, just a header
    records = sequoia.xml_children
    exemplar = records[1] # skip newline
    fields = exemplar.xml_children
    elements = fields[1] #skip newline
    rec_id = elements.xml_children
    loan_id = int(rec_id[0].xml_value.encode('us-ascii'))
    loan_id
    1

To recap progress to date, we can pull XML data directly from a web page, parse it into a list of loan level records, and identify and exclude by copy the blank and constant fields. We ended up with a list that has 255 sublists, one for each loan. What can we do with the list?

Since we want to preserve loan identity (we may care *which* FICO goes with which zipcode), we can't just use one dictionary to hold everything. Instead, we will give each record its own dictionary, d1, d2, ... d255.

Next, we lazily generate the statements needed to create them by a little statement to print out the short commands, then cutting and pasting back to actually run them. If we had more than a couple of hundred records, we'd need to find a more elegant way of doing this, but this is a down-and-dirty way that's easy to follow.

from collections import defaultdict

    for record in range(256):
        print ("%s = defaultdict(list)") % ('d'+str(record))
    d1 = defaultdict(list)
    d2 = defaultdict(list)
    #...
    d255 = defaultdict(list)

This gives up 255 blank dictionary objects which we will assemble in a list:

    websters = [d1, d2, ..., d255]

Then it is a simple matter to pair up empty dictionaries with the revised list of records:

    z = zip(websters,LR)
    for entries in z:
        for pairs in entries[1]:
            entries[0][pairs[0]].append(pairs[1])

and we now have a set of populated dictionaries with which we can do useful work.

## Proof of Concept, Summary FICO Statistics
    fico = flatten([entry['prfico'] for entry in websters])
    mean(fico)
    771.6313725490196
    min(fico)
    701
    max(fico)
    815
    median(fico)
    777.0
    std(fico)
    23.592234433539552
    #Poor man's distribution graph of the unweighted scores
    print stemplot(data0)
    70 | 1 1 7 8 9 9
    71 | 6 7 7 8 9
    72 | 1 4 6 8
    73 | 0 2 2 2 4 4 4 5 6 7 8 9 9
    74 | 0 0 1 1 2 2 2 3 4 4 4 6 6 6 7 9
    75 | 0 1 2 3 3 3 3 3 4 5 5 5 5 6 6 6 7 7 7 8 8 8 9
    76 | 1 1 1 1 2 2 2 3 3 3 3 3 3 4 4 5 5 6 6 7 7 8 8 8 8 8 9 9 9
    77 | 0 0 0 0 0 1 1 1 1 1 1 1 1 1 2 2 2 2 3 3 4 4 4 5 5 5 5 5 6 6 6 7 7 7 7 7 7 7 8 8 8 8 8 8 9 9 9 9
    78 | 0 0 0 0 0 0 0 1 1 1 1 1 1 2 2 2 2 2 3 3 3 4 4 4 4 5 5 5 5 5 5 5 6 6 6 6 7 7 7 7 8 8 8 8 8 8 8 8 9 9 9 9 9
    79 | 0 0 0 0 1 1 1 1 1 1 1 3 3 3 3 3 4 4 5 5 5 5 5 5 6 6 6 7 7 7 8 8 9 9 9
    80 | 0 1 1 1 1 2 2 2 2 3 3 3 4 5 6 6 7 8 9 9
    81 | 0 3 5

This is a stem and leaf plot, which is quite useful. For the top line, read 701, 701, 707, 708, 799, 799, for example. It's useful as a quick check on barbell distributions of credit scores.

## The Cash Flow Model

The purpose of the next program is to replicate the results of pages S-84 and S-85 in the [actual transaction]("https://www.sec.gov/Archives/edgar/data/1176320/000114420410022414/v182145_424b5.htm"). The trade name for these tables is *decrement table* and they show for each class of security when it will be retired, given certain assumptions.

    """
    demonstration.py
    Created on 2010-07-07
    Python 2.6
    """
    # Obtain various standard helper functions and classes
    from __future__ import division        # needs to be first line
    import sys
    import os
    import plac
    import urllib2
    from collections import defaultdict
    from datetime import date
    from datetime import datetime
    from dateutil.relativedelta import *
    from lxml import etree
    from StringIO import StringIO

    help_message = '''
    demonstration: calculate a decrement table for Sequoia 2010-H1 at a constant
    prepayment rate assumption modified so that each loan that prepays does so
    in full, rather than a curtailment.
    Usage: python ./demonstration.py cpr where cpr is a decimal fraction between
    0.01 and 1.00, inclusive
    '''

    '''Constants, from Sequoia Mortgage Trust 2010-H1 (http://goo.gl/I9Wi)'''
    dealname = 'Sequoia 2010-H1'
    bond = 'Class A-1'
    replinefile = 'dectable.csv'
    margin = 2.25       # identical for each loan
    index = 0.9410      # assumed constant per 'modelling assumptions'
    expfee = 0.2585     # servicing and trustee fees
    reset = margin + index - expfee # interest rate calcuation on adjustment
                                    # dates
    pbal = 237838333.0  # initial aggregate principal balance of the loans
    obal = 222378000.0  # initial aggregate principal balance of the Class A-1 
    srpct = obal/pbal   # initial Senior Principal Percentage
    cod = date(2010,5,1)# cut-off date
    close_month = cod - relativedelta(months=1)
    anniversary_month = (cod - relativedelta(months=1)).strftime('%B')
    '''stepdown dates'''
    stepdown = dict(
    stepone = [date(2017,5,1), 1.0],
    steptwo = [date(2018,5,1), 0.7],
    stepthree = [date(2019,5,1), 0.6],
    stepfour = [date(2020,5,1), 0.4],
    stepfive = [date(2021,5,1), 0.2]
    )
    tttdate = date(2013,5,1) # two times test date
    num_replines = 16
    num_loans = 255
    speeds = [0, 0.1, 0.2, 0.3, 0.4, 0.5]

    url='xmlsample.xhtml' #XML file of loans

    #
    def generateItems(seq):
        for item in seq:
            yield item

    def md(lexicon,key, contents):
        """Generic append key, contents to lexicon"""
        lexicon.setdefault(key,[]).append(contents)


    class Solver(object):
        '''takes a function, named arg value (opt.) and returns a Solver object
           http://code.activestate.com/recipes/303396/'''
        def __init__(self,f,**args):
            self._f=f
            self._args={}
            # see important note on order of operations in __setattr__ below.
            for arg in f.func_code.co_varnames[0:f.func_code.co_argcount]:
                self._args[arg]=None
            self._setargs(**args)
        def __repr__(self):
            argstring=','.join(['%s=%s' % (arg,str(value)) for (arg,value) in
                                 self._args.items()])
            if argstring:
                return 'Solver(%s,%s)' % (self._f.func_code.co_name, argstring)
            else:
                return 'Solver(%s)' % self._f.func_code.co_name
        def __getattr__(self,name):
            '''used to extract function argument values'''
            self._args[name]
            return self._solve_for(name)
        def __setattr__(self,name,value):
            '''sets function argument values'''
            # Note - once self._args is created, no new attributes can
            # be added to self.__dict__.  This is a good thing as it throws
            # an exception if you try to assign to an arg which is inappropriate
            # for the function in the solver.
            if self.__dict__.has_key('_args'):
                if name in self._args:
                    self._args[name]=value
                else:
                    raise KeyError, name
            else:
                object.__setattr__(self,name,value)
        def _setargs(self,**args):
            '''sets values of function arguments'''
            for arg in args:
                self._args[arg]  # raise exception if arg not in _args
                setattr(self,arg,args[arg])
        def _solve_for(self,arg):
            '''Newton's method solver'''
            TOL=0.0000001      # tolerance
            ITERLIMIT=1000        # iteration limit
            CLOSE_RUNS=10   # after getting close, do more passes
            args=self._args
            if self._args[arg]:
                x0=self._args[arg]
            else:
                x0=1
            if x0==0:
                x1=1
            else:
                x1=x0*1.1
            def f(x):
                '''function to solve'''
                args[arg]=x
                return self._f(**args)
            fx0=f(x0)
            n=0
            while 1:                    # Newton's method loop here
                fx1 = f(x1)
                if fx1==0 or x1==x0:  # managed to nail it exactly
                    break
                if abs(fx1-fx0)<TOL:    # very close
                    close_flag=True
                    if CLOSE_RUNS==0:       # been close several times
                        break
                    else:
                        CLOSE_RUNS-=1       # try some more
                else:
                    close_flag=False
                if n>ITERLIMIT:
                    print "Failed to converge; exceeded iteration limit"
                    break
                slope=(fx1-fx0)/(x1-x0)
                if slope==0:
                    if close_flag:  # we're close but have zero slope, finish
                        break
                    else:
                        print 'Zero slope and not close enough to solution'
                        break
                x2=x0-fx0/slope           # New 'x1'
                fx0 = fx1
                x0=x1
                x1=x2
                n+=1
            self._args[arg]=x1
            return x1

    def tvm(pv,fv,pmt,n,i):
        '''equation for time value of money'''
        i=i/100
        tmp=(1+i)**n
        return pv*tmp+pmt/i*(tmp-1)-fv
    ## end of http://code.activestate.com/recipes/303396/ }}}

    class Payoff():
        '''prepares a decrement table given constant prepayment speed'''
        def __init__(self, L, C):
            self.L = L
            self.C = C
            self.bbal = float(L[0])     #beginning balance
            self.rbal = self.bbal       #remaining balance
            self.i = float(L[1])        #interest rate in form 4.5
            self.rtm = int(L[2])        #remaining months to maturity 
            self.mtr = int(L[3])+1      #months to roll date new i in effect
            self.mta = int(L[4])        #months remaining of interest only
            self.cod = C[0]             #cut-off date
            self.tttdate = C[1]         #twotimes test date
            self.srpct = C[2]           #initial senior percentage
            self.osrpct = C[2]          #original senior percentage 
            self.reset = C[3]           #interest rate at reset
            self.stepdown = C[4]        #stepdown dates
            self.pbal = C[5]            #original aggregate principal balance
            self.obal = C[6]            #original aggregate class balance
            self.obsupct = 1 - C[2]     #original subordinate percentage
            s = Solver(tvm,pv=self.bbal, fv=0, i = self.i/12, n = self.rtm)
            self.pmt = s.pmt            #monthly payment
            self.teaser = self.mtr      #counter for initial fixed rate period
            self.io = self.mta          #counter for remaining interest only
            self.n = self.rtm+1         #to take into account range()
            self.current = self.cod + relativedelta(months=+1)
            self.smm = 0.0              #single monthly mortality
        def __nonzero__(self):
            return True
        def __bool__(self):
            return False
        def payone(self):
            def is_twice():             #Twotimes test
                if self.subprct >= 2*self.osubpct:
                    return 1
                else:
                    return 0
            def is_shrinking():
                if self.srpct > self.osrpct:
                     return 1
                else:
                    return 0
            def payoff():
                import random               #import standard randomization module
                space = int(1//self.smm)    #calculate sample space
                outcomes = [1]              #create list with one positive outcome
                for n in range(space-1):    #for the remainder of the sample space
                    outcomes.append(0)      #populate with negative outcome
                payoff = random.choice(outcomes)#randomly choose an outcome
                return payoff                   #report result to calling function
            def senior_prepay_percentage():
                if self.current < self.tttdate and is_twice:
                    self.srpppct = self.srpct + 0.5*(1-self.srpct)
                elif self.current >= self.tttdate and is_twice:
                    self.srpppct = self.srpct
                elif self.current < self.stepdown['stepone'][0]:
                    if is_shrinking():
                        self.srpppct = 1.0
                    elif is_twice():
                        self.srpppct = self.stepdown['stepone'][1]
                    else:
                        self.srpppct = self.srpppct
                elif self.current < self.stepdown['steptwo'][0]:
                    if is_shrinking():
                        self.srpppct = 1.0
                    elif is_twice():
                        self.srpppct = self.stepdown['steptwo'][1]
                    else:
                        self.srpppct = self.srpppct
                elif self.current < self.stepdown['stepthree'][0]:
                    if is_shrinking():
                        self.srpppct = 1.0
                    elif is_twice():
                        self.srpppct = self.stepdown['stepthree'][1]
                    else:
                        self.srpppct = self.srpppct
                elif self.current < self.stepdown['stepfour'][0]:
                    if is_shrinking():
                        self.srpppct = 1.0
                    elif is_twice():
                        self.srpppct = self.stepdown['stepfour'][1]
                    else:
                        self.srpppct = self.srpppct
                elif self.current < self.stepdown['stepfive'][0]:
                    if is_shrinking():
                        self.srpppct = 1.0
                    elif is_twice():
                        self.srpppct = self.stepdown['stepfive'][1]
                    else:
                        self.srpppct = self.srpppct
                elif self.current >= self.stepdown['stepfive'][0]:
                    self.srpppct = self.srpct
                else:
                    self.srpppct = self.srpct
                next_month = self.current + relativedelta(months=+1)
                self.current = next_month
            senior_prepay_percentage()  #calculate senior prepayment                                
                                        #percentage
            self.teaser -= 1            #reduce remaining teaser period
            self.io -= 1                #reduce remaining interest only period
            self.bbal = self.rbal       #beginning balance to last period's ending
            ipay = self.rbal*self.i/1200    #interest payment portion
            if payoff():
                self.smm = 1.0
            if self.mta > 0:            #if during interest only period
                self.paydown = 0        #no scheduled principal
                self.prepay = self.smm*(self.bbal-self.paydown)
            else:
                self.paydown = -self.pmt-ipay # reverse negative paid out conv
                self.prepay = self.smm*(self.bbal-self.paydown)
            if self.rtm > 0:            #decrement remaining term to maturity
                self.rtm -= 1
            if self.mtr == 0:           #begin 12-month reset period 11 .. 0
                self.mtr = 11
            elif self.mtr > 0:          #decrement months to reset
                self.mtr -= 1
            if self.mta > 0:            #decrement months to end of i/o period
                self.mta -= 1
            if self.bbal == 0:          #see if final payment has been made
                self.paydown = 0
                self.prepay = 0
            elif self.bbal >= self.paydown + self.prepay:  #not last payment?
                self.rbal -= self.paydown + self.prepay
            elif self.bbal < self.paydown: # scheduled payment enough to final out
                self.paydown = self.bbal
                self.prepay = 0
                self.rbal = 0
            elif self.bbal < self.prepay: # prepayment enough to final out
                self.paydown = self.bbal
                if self.bbal > 0:       # if any still left, allocate to prepay
                    self.prepay = self.bbal
                    self.rbal = 0
            else:
                self.rbal = 0
            if self.teaser == 1:        #last month of fixed rate period
                self.i = self.reset     #change interest rate for following month
                s = Solver(tvm,pv=self.rbal, fv=0, i = self.i/12, \
                n = self.rtm+1)         #calculate new amortizing payment
                self.pmt = s.pmt        #set new payment
            if self.io == 1:            #last month of i/o period
                s = Solver(tvm,pv=self.rbal, fv=0, i = self.i/12, \
                n = self.rtm)           #calculate amortizing payment
                self.pmt = s.pmt        #set new payment
            yield self.srpct*self.paydown + self.srpppct*self.prepay

    #create an empty dictionary for each loan record
    d1 = defaultdict(list)
    d2 = defaultdict(list)
    d3 = defaultdict(list)
    d4 = defaultdict(list)
    d5 = defaultdict(list)
    d6 = defaultdict(list)
    d7 = defaultdict(list)
    d8 = defaultdict(list)
    d9 = defaultdict(list)
    d10 = defaultdict(list)
    d11 = defaultdict(list)
    d12 = defaultdict(list)
    d13 = defaultdict(list)
    d14 = defaultdict(list)
    d15 = defaultdict(list)
    d16 = defaultdict(list)
    d17 = defaultdict(list)
    d18 = defaultdict(list)
    d19 = defaultdict(list)
    d20 = defaultdict(list)
    d21 = defaultdict(list)
    d22 = defaultdict(list)
    d23 = defaultdict(list)
    d24 = defaultdict(list)
    d25 = defaultdict(list)
    d26 = defaultdict(list)
    d27 = defaultdict(list)
    d28 = defaultdict(list)
    d29 = defaultdict(list)
    d30 = defaultdict(list)
    d31 = defaultdict(list)
    d32 = defaultdict(list)
    d33 = defaultdict(list)
    d34 = defaultdict(list)
    d35 = defaultdict(list)
    d36 = defaultdict(list)
    d37 = defaultdict(list)
    d38 = defaultdict(list)
    d39 = defaultdict(list)
    d40 = defaultdict(list)
    d41 = defaultdict(list)
    d42 = defaultdict(list)
    d43 = defaultdict(list)
    d44 = defaultdict(list)
    d45 = defaultdict(list)
    d46 = defaultdict(list)
    d47 = defaultdict(list)
    d48 = defaultdict(list)
    d49 = defaultdict(list)
    d50 = defaultdict(list)
    d51 = defaultdict(list)
    d52 = defaultdict(list)
    d53 = defaultdict(list)
    d54 = defaultdict(list)
    d55 = defaultdict(list)
    d56 = defaultdict(list)
    d57 = defaultdict(list)
    d58 = defaultdict(list)
    d59 = defaultdict(list)
    d60 = defaultdict(list)
    d61 = defaultdict(list)
    d62 = defaultdict(list)
    d63 = defaultdict(list)
    d64 = defaultdict(list)
    d65 = defaultdict(list)
    d66 = defaultdict(list)
    d67 = defaultdict(list)
    d68 = defaultdict(list)
    d69 = defaultdict(list)
    d70 = defaultdict(list)
    d71 = defaultdict(list)
    d72 = defaultdict(list)
    d73 = defaultdict(list)
    d74 = defaultdict(list)
    d75 = defaultdict(list)
    d76 = defaultdict(list)
    d77 = defaultdict(list)
    d78 = defaultdict(list)
    d79 = defaultdict(list)
    d80 = defaultdict(list)
    d81 = defaultdict(list)
    d82 = defaultdict(list)
    d83 = defaultdict(list)
    d84 = defaultdict(list)
    d85 = defaultdict(list)
    d86 = defaultdict(list)
    d87 = defaultdict(list)
    d88 = defaultdict(list)
    d89 = defaultdict(list)
    d90 = defaultdict(list)
    d91 = defaultdict(list)
    d92 = defaultdict(list)
    d93 = defaultdict(list)
    d94 = defaultdict(list)
    d95 = defaultdict(list)
    d96 = defaultdict(list)
    d97 = defaultdict(list)
    d98 = defaultdict(list)
    d99 = defaultdict(list)
    d100 = defaultdict(list)
    d101 = defaultdict(list)
    d102 = defaultdict(list)
    d103 = defaultdict(list)
    d104 = defaultdict(list)
    d105 = defaultdict(list)
    d106 = defaultdict(list)
    d107 = defaultdict(list)
    d108 = defaultdict(list)
    d109 = defaultdict(list)
    d110 = defaultdict(list)
    d111 = defaultdict(list)
    d112 = defaultdict(list)
    d113 = defaultdict(list)
    d114 = defaultdict(list)
    d115 = defaultdict(list)
    d116 = defaultdict(list)
    d117 = defaultdict(list)
    d118 = defaultdict(list)
    d119 = defaultdict(list)
    d120 = defaultdict(list)
    d121 = defaultdict(list)
    d122 = defaultdict(list)
    d123 = defaultdict(list)
    d124 = defaultdict(list)
    d125 = defaultdict(list)
    d126 = defaultdict(list)
    d127 = defaultdict(list)
    d128 = defaultdict(list)
    d129 = defaultdict(list)
    d130 = defaultdict(list)
    d131 = defaultdict(list)
    d132 = defaultdict(list)
    d133 = defaultdict(list)
    d134 = defaultdict(list)
    d135 = defaultdict(list)
    d136 = defaultdict(list)
    d137 = defaultdict(list)
    d138 = defaultdict(list)
    d139 = defaultdict(list)
    d140 = defaultdict(list)
    d141 = defaultdict(list)
    d142 = defaultdict(list)
    d143 = defaultdict(list)
    d144 = defaultdict(list)
    d145 = defaultdict(list)
    d146 = defaultdict(list)
    d147 = defaultdict(list)
    d148 = defaultdict(list)
    d149 = defaultdict(list)
    d150 = defaultdict(list)
    d151 = defaultdict(list)
    d152 = defaultdict(list)
    d153 = defaultdict(list)
    d154 = defaultdict(list)
    d155 = defaultdict(list)
    d156 = defaultdict(list)
    d157 = defaultdict(list)
    d158 = defaultdict(list)
    d159 = defaultdict(list)
    d160 = defaultdict(list)
    d161 = defaultdict(list)
    d162 = defaultdict(list)
    d163 = defaultdict(list)
    d164 = defaultdict(list)
    d165 = defaultdict(list)
    d166 = defaultdict(list)
    d167 = defaultdict(list)
    d168 = defaultdict(list)
    d169 = defaultdict(list)
    d170 = defaultdict(list)
    d171 = defaultdict(list)
    d172 = defaultdict(list)
    d173 = defaultdict(list)
    d174 = defaultdict(list)
    d175 = defaultdict(list)
    d176 = defaultdict(list)
    d177 = defaultdict(list)
    d178 = defaultdict(list)
    d179 = defaultdict(list)
    d180 = defaultdict(list)
    d181 = defaultdict(list)
    d182 = defaultdict(list)
    d183 = defaultdict(list)
    d184 = defaultdict(list)
    d185 = defaultdict(list)
    d186 = defaultdict(list)
    d187 = defaultdict(list)
    d188 = defaultdict(list)
    d189 = defaultdict(list)
    d190 = defaultdict(list)
    d191 = defaultdict(list)
    d192 = defaultdict(list)
    d193 = defaultdict(list)
    d194 = defaultdict(list)
    d195 = defaultdict(list)
    d196 = defaultdict(list)
    d197 = defaultdict(list)
    d198 = defaultdict(list)
    d199 = defaultdict(list)
    d200 = defaultdict(list)
    d201 = defaultdict(list)
    d202 = defaultdict(list)
    d203 = defaultdict(list)
    d204 = defaultdict(list)
    d205 = defaultdict(list)
    d206 = defaultdict(list)
    d207 = defaultdict(list)
    d208 = defaultdict(list)
    d209 = defaultdict(list)
    d210 = defaultdict(list)
    d211 = defaultdict(list)
    d212 = defaultdict(list)
    d213 = defaultdict(list)
    d214 = defaultdict(list)
    d215 = defaultdict(list)
    d216 = defaultdict(list)
    d217 = defaultdict(list)
    d218 = defaultdict(list)
    d219 = defaultdict(list)
    d220 = defaultdict(list)
    d221 = defaultdict(list)
    d222 = defaultdict(list)
    d223 = defaultdict(list)
    d224 = defaultdict(list)
    d225 = defaultdict(list)
    d226 = defaultdict(list)
    d227 = defaultdict(list)
    d228 = defaultdict(list)
    d229 = defaultdict(list)
    d230 = defaultdict(list)
    d231 = defaultdict(list)
    d232 = defaultdict(list)
    d233 = defaultdict(list)
    d234 = defaultdict(list)
    d235 = defaultdict(list)
    d236 = defaultdict(list)
    d237 = defaultdict(list)
    d238 = defaultdict(list)
    d239 = defaultdict(list)
    d240 = defaultdict(list)
    d241 = defaultdict(list)
    d242 = defaultdict(list)
    d243 = defaultdict(list)
    d244 = defaultdict(list)
    d245 = defaultdict(list)
    d246 = defaultdict(list)
    d247 = defaultdict(list)
    d248 = defaultdict(list)
    d249 = defaultdict(list)
    d250 = defaultdict(list)
    d251 = defaultdict(list)
    d252 = defaultdict(list)
    d253 = defaultdict(list)
    d254 = defaultdict(list)
    d255 = defaultdict(list)
    websters = [d1, d2, d3, d4, d5, d6, d7, d8, d9, d10, d11, d12, d13, d14, d15, d16, d17, d18, d19, d20, d21, d22, d23, d24, d25, d26, d27, d28, d29, d30, d31, d32, d33, d34, d35, d36, d37, d38, d39, d40, d41, d42, d43, d44, d45, d46, d47, d48, d49, d50, d51, d52, d53, d54, d55, d56, d57, d58, d59, d60, d61, d62, d63, d64, d65, d66, d67, d68, d69, d70, d71, d72, d73, d74, d75, d76, d77, d78, d79, d80, d81, d82, d83, d84, d85, d86, d87, d88, d89, d90, d91, d92, d93, d94, d95, d96, d97, d98, d99, d100, d101, d102, d103, d104, d105, d106, d107, d108, d109, d110, d111, d112, d113, d114, d115, d116, d117, d118, d119, d120, d121, d122, d123, d124, d125, d126, d127, d128, d129, d130, d131, d132, d133, d134, d135, d136, d137, d138, d139, d140, d141, d142, d143, d144, d145, d146, d147, d148, d149, d150, d151, d152, d153, d154, d155, d156, d157, d158, d159, d160, d161, d162, d163, d164, d165, d166, d167, d168, d169, d170, d171, d172, d173, d174, d175, d176, d177, d178, d179, d180, d181, d182, d183, d184, d185, d186, d187, d188, d189, d190, d191, d192, d193, d194, d195, d196, d197, d198, d199, d200, d201, d202, d203, d204, d205, d206, d207, d208, d209, d210, d211, d212, d213, d214, d215, d216, d217, d218, d219, d220, d221, d222, d223, d224, d225, d226, d227, d228, d229, d230, d231, d232, d233, d234, d235, d236, d237, d238, d239, d240, d241, d242, d243, d244, d245, d246, d247, d248, d249, d250, d251, d252, d253, d254, d255]

    content = urllib2.urlopen(url).read()
    root = etree.fromstring(content)
    records = list(root)
    lexicon = generateItems(websters)
    for record in records:
        lex = lexicon.next()
        for field in record:
           md(lex, field.attrib['name'], field.text)

    tape = []
    for loan in websters:
        record = []
        record.append(float(loan['obal'][0]))
        record.append(float(loan['cintpct'][0]))
        tmat = loan['maturity'][0]
        mat = datetime.strptime(tmat, '%Y-%m-%d').date()
        to_mat = relativedelta(mat,cod)
        mtm = to_mat.months + to_mat.years*12
        record.append(mtm)
        fpd = datetime.strptime(loan['fpd'][0], '%Y-%m-%d').date()
        to_roll = relativedelta(fpd + relativedelta(months=60), cod)
        mtr = to_roll.months + to_roll.years*12
        record.append(mtr)
        intonlyterm = int(loan['intonlyterm'][0])
        to_amort = relativedelta(fpd + relativedelta(months=intonlyterm), cod)
        mta = to_amort.months + to_amort.years*12
        record.append(mta)
        tape.append(record)

    def run_loan_payoff(cpr):
        '''cpr = 0.1 Constant Prepayment Rate in decimal fraction'''
        C = [cod, tttdate, srpct, reset, stepdown, pbal, obal]
        cbal = obal
        anniversary = cod.year+1
        E = {}
        for record in tape:
            md(E,'tape', Payoff(record,C))
        twelfth = 1.0/12.0
        smm = 1.0 - (1.0-cpr)**twelfth  #  single monthly mortality
        column = []                     # empty list to collect principal payments
        for year in range(2011,2041):
            annual = []                 # temporary list
            for month in range(12):
                for entry in E['tape']:
                    payment = []        # temporary list
                    entry.srpct = srpct # set object senior percentage
                    entry.subpct = 1 - srpct
                    entry.smm = smm     # set smm for object
                    try:                # while still data
                        payment.append(entry.payone().next())
                    except StopIteration:
                        pass
                    annual.append(sum(payment))    # aggregate for month
                    cbal -= sum(payment)           # knock down senior 
                sprct = cbal/obal                  # recalculate senior percentage
            column.append(annual)                  # collect months
        column[:] = [sum(item) for item in column] # aggregate for year
        cbal=obal
        ''' output decrement table for given CPR speed '''
        print "%s %s at CPR of %d%%" % (dealname, bond, cpr*100)
        for year in column:
            cbal -= year
            percentout = round(cbal/obal*100,2)
            if percentout >= 1:
                print("%s %d:\t\t%0.0f") % (anniversary_month,  anniversary,\
                percentout)
            elif percentout <= 0:
                print("%s %d:\t\t0") % (anniversary_month, anniversary)
            else:
                percentout < 1
                print("%s %d:\t\t*") % (anniversary_month, anniversary)
            anniversary += 1


    def main(cpr_arg):
        print help_message
        cpr = float(cpr_arg) # command line argument is a string
        run_loan_payoff(cpr) # call the function to produce the table

    if __name__ == "__main__":
            plac.call(main)

The output of a run of this program looks like this:

    Sequoia 2010-H1 Class A-1 at CPR of 10%
    April 2011:     78
    April 2012:     60
    April 2013:     47
    April 2014:     35
    April 2015:     26
    April 2016:     20
    April 2017:     16
    April 2018:     11
    April 2019:     8
    April 2020:     5
    April 2021:     3
    April 2022:     *
    April 2023:     0
    April 2024:     0
    April 2025:     0
    April 2026:     0
    April 2027:     0
    April 2028:     0
    April 2029:     0
    April 2030:     0
    April 2031:     0
    April 2032:     0
    April 2033:     0
    April 2034:     0
    April 2035:     0
    April 2036:     0
    April 2037:     0
    April 2038:     0
    April 2039:     0
    April 2040:     0
    >>> 

The results are likely to be different each time. This is because whether any given loan pays off in any given month is based on a probability for a given prepayment speed. Thus, at a CPR of 0.1, a loan has an approximately 1 in 113 chance of paying of in any particular month. The determination, each month, of whether a payoff occurs is based on a random selection from the possibilities of payoff (1 chance) or no payoff (112 chances). The function assumes no defaults, similarly to the traditional decrement table. However, a default function based on some combination of loan characteristics could be used to arrive at a probability of default in any given month similarly.

I tested the results of the model against the issuer's table, and I found good agreement with results taken to the second decimal place.

```{r echo = FALSE}
knitr::include_graphics(rep("images/CPR02dec.png"))
```

This is the most complex Python program that I've written. Again, I don't consider it production code, but did find it a useful prototype.

No one else submitted an example program, and the agency ended up dropping the proposal. That's a shame, because in the round trip from the jargon of the trader's desk to the lawyer's chambers and then to the proprietary model, some meaning is lost in translation. On an industry conference call during the comment period, one proprietary modeler admitted that he didn't know how to interpret the legal term *notwithstanding.* That is just as understandable as a lawyer being innocent of a NAND gate, but potentially more harmful.

<!--chapter:end:02-cashflow.Rmd-->

# The subprime mortgage crisis unfolds in early 2007, part 1

Keywords: large database, MySQL, Python, data wrangling, exploratory data analysis, data mining, qqnorm, qqplot, Shapiro-Wilk

## Case Description

Washington Mutual Bank originated subprime loans as one of its home loan mortgage types. The term *subprime* has always been hard to pin down precisely. Generally, though, they represented higher-interest rate loans to less creditworthy borrowers. In competition with other lenders, the risks types, such as the borrower's debt-to-income ratio, the combined loan-to-value ratio (considering both the first and any second lien loan), the level of income and asset verification, and the purpose of the loan began to become *layered.* That is, the cumulative risk increased. And too much reliance, perhaps, was being placed on ever-increasing home values.

In early 2007, a marketing specialist from Washington Mutual Bank's Wall Street subsidiary, called me to say that investors were complaining about the peformance of our subprime securities issued in 2006. I was the principal lawyer in-house dealing with mortgage backed securities, and he wanted to know what kinds of written materials he could provide.

I had some questions that had no ready answers:

* How did our pools of mortgage loans differ?
* What do we mean by performance?

We could begin, I suggested, by looking at the tape for the January 2006 deal, [LBMLT 2006-1], to see how the origination characteristics differed from competing deals. We could also pull the monthly report that showed last payment date, delinquency status and other data. He said that he'd try to get that together, but it wasn't really his job.

This is the case that Donald Rumsfelt (*known knowns, known unknowns and unknown unknowns*) missed: **unknown knowns,** information we had that was scattered throughout the organization.

It became apparent that I wasn't going to get any help dealing with my rising sense of unease that the company faced potential liabilities as a result of these transactions. I was senior enough to set my own priorities so I decided to assemble the data to see what I could make of it.

## Data Wrangling

There were 14 transactions issued in 2006, all of which were accompanied by tapes similar to the [LBMLT 2006-1] deal with origination data. I also looked at default data, from over 100 spreadsheets, many with manual changes, that had to be beaten into submission.

Fortunately the origination tapes, while wrapped in HTML, were tagged with

    <pre>payload</pre> 
    
pairs, making their extraction easy. One of the typical tasks was converting dates into ISO form -- from 3/6/2006, say, to 2006-03-06, for which I turned to Python.

    """
    function to convert dates into date objects

    """
    from datetime import datetime
    import re

    def canonize_date(slashdate):
        """convert date strings from 3/1/2009 to 2009-03-01"""
        dateString = re.compile(r'(\d{1,2})/(\d{1,2})/(\d{4})') # match 3/1/2009
        dash = '-'
        parts = dateString.search(slashdate).groups()
        composed = parts[-1] + dash + parts[-3] + dash + parts[-2]
        return composed

    def make_date(entry):
        """
        Convert string in form 2011-10-25 to a date object
        """
        return datetime.date(datetime.strptime(entry, "%Y-%m-%d"))
        
## Database conversion

There were too many records (nearly 125K) to work in memory, which lead me to create my first MySQL database.

    +---------+---------------+------+-----+---------+-------+
    | Field   | Type          | Null | Key | Default | Extra |
    +---------+---------------+------+-----+---------+-------+
    | ctapeno | decimal(10,0) | YES  | MUL | NULL    |       |
    | fpd     | date          | YES  |     | NULL    |       |
    | obal    | decimal(10,0) | YES  |     | NULL    |       |
    | ltype   | varchar(15)   | YES  |     | NULL    |       |
    | cservno | decimal(10,0) | YES  |     | NULL    |       |
    | st      | varchar(4)    | YES  |     | NULL    |       |
    | odate   | date          | YES  |     | NULL    |       |
    | margin  | decimal(10,0) | YES  |     | NULL    |       |
    | oltv    | decimal(10,0) | YES  |     | NULL    |       |
    | lien    | tinyint(4)    | YES  |     | NULL    |       |
    | second  | decimal(10,0) | YES  |     | NULL    |       |
    | pflag   | varchar(1)    | YES  |     | NULL    |       |
    | pterm   | tinyint(4)    | YES  |     | NULL    |       |
    | pmon    | tinyint(4)    | YES  |     | NULL    |       |
    | grade   | varchar(5)    | YES  |     | NULL    |       |
    | fico    | decimal(10,0) | YES  |     | NULL    |       |
    | cbal    | varchar(1)    | YES  |     | NULL    |       |
    | sbal    | decimal(10,0) | YES  |     | NULL    |       |
    | deal    | varchar(15)   | YES  |     | NULL    |       |
    | orate   | decimal(10,0) | YES  |     | NULL    |       |
    | ptype   | varchar(10)   | YES  |     | NULL    |       |
    | otype   | varchar(20)   | YES  |     | NULL    |       |
    | dtype   | varchar(15)   | YES  |     | NULL    |       |
    | purpose | varchar(25)   | YES  |     | NULL    |       |
    | lg      | tinyint(4)    | YES  |     | NULL    |       |
    | rterm   | smallint(6)   | YES  |     | NULL    |       |
    | pmi     | varchar(1)    | YES  |     | NULL    |       |
    | cdate   | date          | YES  |     | NULL    |       |
    | mdate   | date          | YES  |     | NULL    |       |
    | pexp    | date          | YES  |     | NULL    |       |
    | pptype  | varchar(4)    | YES  |     | NULL    |       |
    | icode   | varchar(4)    | YES  |     | NULL    |       |
    | duedate | date          | YES  |     | NULL    |       |
    | scap    | tinyint(4)    | YES  |     | NULL    |       |
    | dti     | decimal(10,0) | YES  |     | NULL    |       |
    | oterm   | smallint(6)   | YES  |     | NULL    |       |
    | oservno | decimal(10,0) | YES  |     | NULL    |       |
    | cltv    | decimal(10,0) | YES  |     | NULL    |       |
    | rdate   | date          | YES  |     | NULL    |       |
    | aterm   | smallint(6)   | YES  |     | NULL    |       |
    | zip     | int(11)       | YES  |     | NULL    |       |
    | city    | varchar(25)   | YES  |     | NULL    |       |
    | status  | varchar(10)   | YES  |     | NULL    |       |
    +---------+---------------+------+-----+---------+-------+
    43 rows in set (0.00 sec)

The data dictionary:

    +---------------------------+------------+-------------------------------------+
    | tapefield                 | tablefield | desciption                          |
    +---------------------------+------------+-------------------------------------+
    | LOAN_NUMBER               | ctapeno    | serial number of loan on closing ta |
    | FPDATE                    | fpd        | first schedule payment date after o |
    | ORIGBAL                   | obal       | original note balance               |
    | LOANTYPE                  | ltype      | loan type                           |
    | SERVICING_LOAN_NUMBER     | servno     | customer account number             |
    | STATE                     | st         | state where mortgaged property is l |
    | FUNDDATE                  | odate      | date on which the mortage loan was  |
    | MARGIN                    | margin     | "for ARMs, the percentage added to  |
    | OLTV                      | oltv       | original loan to value ratio        |
    | LIEN                      | lien       | priority                            |
    | SUBLIEN                   | second     | "balance of sublien, if record is a |
    | PREPAY_IND                | pflag      | whether loan has prepayment penalty |
    | PREPAY_TERM               | pterm      | term of any prepayment penalty      |
    | PREPAY_MNTH               | pmon       | number of months of prepayment pena |
    | GRADE                     | grade      | credit grade of loan                |
    | CALCFICO                  | cfico      | whether fico was calculated         |
    | CALCBLN                   | cbal       | whether cut off date balance was ca |
    | SCHDBAL                   | sbal       | schedule balance sold to deal       |
    | SECURITY                  | deal       | securitization identifier           |
    | ORIGRATE                  | orate      | note rate for initial payment       |
    | PROPTYPEDESC              | ptype      | property type                       |
    | OCCUPANCYDESC             | otype      | occupancy type                      |
    | DOCTYPEDESC               | dtype      | documentation type                  |
    | PURPOSEDESC               | purpose    | loan purpose                        |
    | POOLGROUP                 | lg         | loan group                          |
    | ORIG_CALCRTERM            | rterm      | calculated remaining months to matu |
    | PMI                       | pmi        | whether MI is in effect             |
    | DEAL_CLOSE_DATE           | cdate      | closing date for deal               |
    | MTDATE                    | mdate      | matuity date for note               |
    | PREPAY_EXP_DATE           | pexp       | expiration date of prepayment penal |
    | PREPAY_RATE_CODE          | pptype     | prepayment penalty type             |
    | INVESTOR_CODE             | icode      | investor code                       |
    | DUEDATE                   | duedate    | date on which first payment due to  |
    | SUBRATE_CAP               | scap       | subrate cap                         |
    | DEBTRATIO                 | dti        | front-end debt-to-income ratio      |
    | OTERM                     | oterm      | original term                       |
    | ORIG_SERVICING_LOAN_NUMBE | oservno    | original servicing number           |
    | PROSUPP_CLTV              | cltv       | combined LTV                        |
    | ORIG_NRADATE              | rdate      | roll date                           |
    | ATERM                     | aterm      | amortization term                   |
    | ZIP                       | zip        | postal code                         |
    | CITY                      | city       | address city                        |
    | STATUS                    | status     | reserved                            |
    +---------------------------+------------+-------------------------------------+
    43 rows in set (0.00 sec)

After creating these for each of the transactions, I combined them into a single file, used a supplemental zipcode lookup table to identify the metropolitan area (for possible use with the Case Shiller Index of Home Prices) and added fields for loan payment history.

The resulting table with the $25 billion in mortgage loans:

    +----------+-------------+
    | count(*) | sum(obal)   |
    +----------+-------------+
    |   124645 | 25542466576 |
    +----------+-------------+


## Preliminary analysis, based on FICO scores

### FICO's importance

In 1995, Freddie Mac advised lenders that it had found that consumer credit scores developed by Fair, Issac and Company, Inc. (**FICO** scores) strongly predicted the likelihood of default on mortgage loans. Freddie Mac's communication is reproduced as Attachment 2 in testimony before the U.S. House Committee on Oversight and Government Reform, the [Pinto testimony], beginning at page 28 of the pdf.

A traditional description of the limitations of credit score is similar to the following:

> Third-party credit reporting organizations provide credit scores as an aid to lenders in evaluating the creditworthiness of borrowers. Although different credit reporting organizations use different methodologies, higher credit scores indicate greater creditworthiness. Credit scores do not necessarily correspond to the probability of default over the life of the related mortgage loan because they reflect past credit history, rather than an assessment of future payment performance. In addition, the credit scores shown were collected from a variety of sources over a period of weeks, months or longer, and the credit scores do not necessarily reflect the credit scores that would be reported as of the date of this prospectus supplement. Credit scores also only indicate general consumer creditworthiness, and credit scores are not intended to specifically apply to mortgage debt. Therefore, credit scores should not be considered as an accurate predictor of the likelihood of repayment of the related mortgage loans.

*See* the [credit disclosure] beginning on page S-18 by a Washington Mutual affiliate in 2002.

The rating agencies and buyers involved in residential mortgage backed securities, however, attached considerable importance to credit scores, generically referred to as FICOs. Therefore, the FICO composition was an obvious starting point.

```{r setup4, echo = FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(DBI)
library(knitr)
library(kableExtra)
library(RMySQL)
drv <- dbDriver("MySQL")
con <- dbConnect(drv, username="root", password="", dbname ="dlf", host="localhost")
res <- dbGetQuery(con, "SELECT deal, fico FROM y6")
#cs is shorthand for credit score
cs <- as.tibble(res)
p <- ggplot(cs, aes(x=fico)) + geom_histogram(binwidth = 5) + coord_cartesian(xlim = c(300,850))
p + ggtitle("FICO scores for 2006 transactions in increments of 5") + labs(x = "FICO", y = "Number of Loans")
``` 

### FICO scores in the 2006 loan pool

FICO scores have a minimum value of 300, and a maximum value of 850. The summary statistics are:

```{r, echo = FALSE, warning=FALSE, message=FALSE}
summary(cs$fico)
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
nofico <-  filter(cs, fico < 300)
lowfico <- filter(cs, fico < 500 & fico > 0)
```

and the mode is `r getmode(cs$fico)`. The minimum score represents loans without FICO scores:

```{r, results="asis", echo = FALSE}
kable(nofico, format.args = list(big.mark = ","), caption = "Number of no-FICO loans by deal")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

Sixteen other loans had scores below 500:

```{r, results="asis", echo = FALSE}
kable(lowfico, format.args = list(big.mark = ","), caption = "Number of low-FICO loans by deal")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

The *cliffs* around 500 (below which only the small number of loans in the tables above are included), 600, 630 and 635 represent the cumulative segmentation of the pools to obtain favorable ratings. Not all deals followed this approach, however.

```{r, results="asis", echo = FALSE}
p + ggtitle("FICO scores for 2006 transactions in increments") + labs(x = "FICO", y = "Number of Loans") + facet_wrap(~deal)
```

The variability among transactions suggest that *if* FICO scores have an influence on default rates, it may be necessary to stratify of otherwise transform the data to obtain useful results.

### The FICO scores are not normally distributed

```{r, results="asis", echo = FALSE, warning=FALSE}

# Find the slope and intercept of the line that passes through the 1st and 3rd
# quartile of the normal q-q plot
# y: Find the 1st and 3rd quartiles
# x: Find the matching normal values on the x-axis
# slope: Compute the line slope
# intc Compute the line intercept
find_intercepts <- function(obj){
  y = quantile(obj, c(0.25, 0.75), type=5)  
  x = qnorm(c(0.25, 0.75))
  slope = diff(y)/diff(x)
  intc = y[1] - slope * x[1]
  return(c(slope,intc))
}
           
# Code informed by https://goo.gl/nz9ikh

#generate q-q plot of the 2006 pool

si <- find_intercepts(cs$fico)
slope <- si[[1]]
intercept <- si[[2]]

ggplot(cs) + aes(sample=fico) + stat_qq(distribution=qnorm) + 
    geom_abline(intercept=intercept[1], slope=slope[1]) + ylab("FICO") + labs(title = "QQ Plot of FICO Scores")
``` 

As a group, FICO scores have *fat tails,* a trait that is present in each of the deals to some degree. There are more loans with very low FICO scores than you would expect if score were randomly distributed plus more loans with high score. We also see a dip in the 500-600 range (loans that may only have been originated due to compensating factors), which are fewer than the sub-500 FICO loans.

With variations, all the deals have similiar distributions. The variability of FICO scores makes their use as an independent variable in, say, regression analysis potentially problematic.

```{r, results="asis", echo = FALSE}
# Slope/intercept of line that passing through 1st/3rd quartile of the normal q-q plot
find_intercepts <- function(obj){
  y = quantile(obj, c(0.25, 0.75), type=5)  # y: Find the 1st and 3rd quartiles
  x = qnorm(c(0.25, 0.75))                  #x: Find the matching normal values on the x-axis
  slope = diff(y)/diff(x)                   # Compute the line slope
  intc = y[1] - slope * x[1]                # Compute the line intercept
  return(c(slope, intc))
# Assisted by https://goo.gl/nz9ikh
# Example
# si <- find_intercepts(obj)
# slope = si[[1]]
# intercept = si[[2]]
}

# Generate normal q-q plot for total of deals

q <- ggplot(cs, aes(sample=fico)) + stat_qq() + 
           geom_abline(intercept=intercept, slope=slope) + ylab("FICO") + labs(title = "QQ Plot of FICO Scores")

# get slopes and intercepts for each deal
si <- cs %>% group_by(deal) %>% summarize(slope = find_intercepts(fico)[[1]], intercept  = find_intercepts(fico)[[2]])

# generate plots for each deal
deals <- list(si$deal)

dealqq <- function(ta) {
    v_ <- cs %>% filter(deal == ta, fico)
    si_ <- filter(si, deal == ta)
    q_<- v_ %>% ggplot(aes(sample=fico)) + stat_qq() +             
    geom_abline(intercept=si_$intercept, slope=si_$slope) + ylab("FICO")     + ggtitle(paste("QQ Plot of FICO Scores for ", si_$deal))
    q_
}

# With the help of Python scripting while I get a better grasp of R "for"
# https://git.io/f4Sun
dealqq("LBMLT 2006-1")
dealqq("LBMLT 2006-10")
dealqq("LBMLT 2006-11")
dealqq("LBMLT 2006-2")
dealqq("LBMLT 2006-3")
dealqq("LBMLT 2006-4")
dealqq("LBMLT 2006-5")
dealqq("LBMLT 2006-6")
dealqq("LBMLT 2006-7")
dealqq("LBMLT 2006-8")
dealqq("LBMLT 2006-9")
dealqq("LBMLT 2006-WL1")
dealqq("LBMLT 2006-WL2")
dealqq("LBMLT 2006-WL3")
```


### Possible strategies to deal with the issues

Sampling the pool with a reasonably large number of observations will provide a well-behaved normal distribution, courtesy of the Central Limit Theorem. Alternatively, we can cluster based on the distance of each point from the trendline.

#### Possible complication

Between the time each deal was issued and the time that delinquency history was collated, almost a quarter of the loans dropped out:

    MariaDB [dlf]> select count(*) from y6 where ctapeno not in (select ytemp.ctapeno from ytemp )

    +----------+
    | count(*) |
    +----------+
    |    28908 |
    +----------+

whether due to prepayment in full, repurchase by the seller for breach or default, foreclosure and sale.

The dropped loans appear similar to the original pool.

```{r, results="asis", echo = FALSE, warning=FALSE}
res2 <- dbGetQuery(con, "SELECT deal, icode, fico FROM drops")
cds <- as.tibble(res2)
p <- ggplot(cds, aes(x=fico)) + geom_histogram(binwidth = 5) + coord_cartesian(xlim = c(300,850))
p + ggtitle("FICO scores for 2006 dropped transactions in increments of 5") + labs(x = "FICO", y = "Number of Loans")
``` 

Summary statistics are similar.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
summary(cds$fico)
```

and the mode, `r getmode(cds$fico)`, is identical. 

Like the total pool, the dropouts were not normally distributed. Since none of the drops were no-FICO loans, the slopes differ, but the shape of the distribution appears to be remarkably similar to the total pool.

```{r, results="asis", echo = FALSE, warning=FALSE}
slope <- find_intercepts(cds$fico)[[1]]
intercept <- find_intercepts(cds$fico)[[2]]
d <- ggplot(cds, aes(sample=fico)) + stat_qq() + geom_abline(intercept=intercept, slope=slope) + ylab("FICO") + ggtitle("QQ Plot of FICO Scores for Dropped Loans")
d
```

The distribution reports on each deal provide the cumulative number of defaulted loans that were liquidated. These were collected manually because there were only 15 files and I anticipated no future need to examine the reports.

```{r, results="asis", echo = FALSE, warning=FALSE}

liq <- read.csv("data/totliq.csv", header = TRUE, stringsAsFactors = FALSE, sep = ",")
kable(liq, format.args = list(big.mark = ","), caption = "Number of Liquidated Defaulted Loans Reported by the end of 2006")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

(The field "CIK" represents the EDGAR lookup key on the SEC data base; the field "reported" is whether a report was required to filed by the end of 2006.)

#### Decision to focus on loan population still outstanding in early 2007

Only `r sum(liq$liquidated)` loans of the `r nrow(cds)`, or `r round(sum(liq$liquidated)/nrow(cds)*100,2)`% of the dropped loans had defaulted and been liquidated by the end of 2006 reporting. On this basis, I decided to limit analysis to the remaining loans because FICO information about the dropped loans did not appear promising as a predictive factor in loan delinquency for the remaining loans.

### Reduced data set

#### Description

When the problem landed on my desk in early 2007, I had access to the delinquency history of the pool of loans remaining, 

As a result, I turn to the survivors in a similar data base:

    MariaDB [dlf]> describe y7;
    +----------+---------------+------+-----+---------+-------+
    | Field    | Type          | Null | Key | Default | Extra |
    +----------+---------------+------+-----+---------+-------+
    | city     | varchar(25)   | YES  |     | NULL    |       |
    | cltv     | decimal(10,0) | YES  |     | NULL    |       |
    | ctapeno  | decimal(10,0) | YES  |     | NULL    |       |
    | deal     | varchar(25)   | YES  |     | NULL    |       |
    | down     | int(11)       | YES  |     | NULL    |       |
    | dti      | decimal(10,0) | YES  |     | NULL    |       |
    | dtype    | varchar(25)   | YES  |     | NULL    |       |
    | ebal     | decimal(10,0) | YES  |     | NULL    |       |
    | fico     | decimal(10,0) | YES  |     | NULL    |       |
    | fpd      | date          | YES  |     | NULL    |       |
    | gainloss | decimal(10,0) | YES  |     | NULL    |       |
    | grade    | varchar(5)    | YES  |     | NULL    |       |
    | irate    | decimal(10,0) | YES  |     | NULL    |       |
    | issuer   | varchar(10)   | YES  |     | NULL    |       |
    | lat      | decimal(10,0) | YES  |     | NULL    |       |
    | lien     | int(11)       | YES  |     | NULL    |       |
    | liq      | decimal(10,0) | YES  |     | NULL    |       |
    | lng      | decimal(10,0) | YES  |     | NULL    |       |
    | lstatus  | int(11)       | YES  |     | NULL    |       |
    | ltype    | varchar(25)   | YES  |     | NULL    |       |
    | margin   | decimal(10,0) | YES  |     | NULL    |       |
    | metro    | varchar(25)   | YES  |     | NULL    |       |
    | nrate    | decimal(10,0) | YES  |     | NULL    |       |
    | obal     | decimal(10,0) | YES  |     | NULL    |       |
    | odate    | date          | YES  |     | NULL    |       |
    | oltv     | decimal(10,0) | YES  |     | NULL    |       |
    | orate    | decimal(10,0) | YES  |     | NULL    |       |
    | oterm    | int(11)       | YES  |     | NULL    |       |
    | payments | int(11)       | YES  |     | NULL    |       |
    | pmiflag  | int(11)       | YES  |     | NULL    |       |
    | pocode   | int(11)       | YES  |     | NULL    |       |
    | pod      | date          | YES  |     | NULL    |       |
    | ppp      | int(11)       | YES  |     | NULL    |       |
    | ptd      | date          | YES  |     | NULL    |       |
    | purpose  | varchar(25)   | YES  |     | NULL    |       |
    | remit    | decimal(10,0) | YES  |     | NULL    |       |
    | sbal     | decimal(10,0) | YES  |     | NULL    |       |
    | servno   | decimal(10,0) | YES  |     | NULL    |       |
    | sint     | decimal(10,0) | YES  |     | NULL    |       |
    | sprin    | decimal(10,0) | YES  |     | NULL    |       |
    | spymt    | decimal(10,0) | YES  |     | NULL    |       |
    | st       | varchar(4)    | YES  |     | NULL    |       |
    | zip      | int(11)       | YES  |     | NULL    |       |
    | otype    | varchar(25)   | YES  |     | NULL    |       |
    | rdate    | date          | YES  |     | NULL    |       |
    | ptype    | varchar(15)   | YES  |     | NULL    |       |
    | dptd     | int(7)        | YES  |     | NULL    |       |
    | dfpd     | int(7)        | YES  |     | NULL    |       |
    +----------+---------------+------+-----+---------+-------+
    48 rows in set (0.00 sec)
    
Some new fields will be important. As of the end of the first quarter 2007 *down* is the number of months that a loan has been delinquent. A value of down greater than 3 generally indicates a loan in foreclosure or a loan that has been foreclosed and is in the process of being sold.

The field *remit* is the past-month payment history encoded as a base10 number. The remittance reports contain a character field in the form "D110011011110" that shows whether a payment has been made ('0') or missed ('1') during the life of the loan. A little utility program converts the field to the equivalent binary number that captures that information.

     # convert a decimal (decimal, base 10) integer to a binary string (base 2)

    def dec2bin(n):
        '''convert decimal integer n to binary string bStr'''
        bStr = ''
        if n < 0:  raise ValueError("must be a positive integer")
        if n == 0: return '0'
        while n > 0:
            bStr = str(n % 2) + bStr
            n = n >> 1
        return bStr

    def int2bin(n, count=24):
        """returns the binary of integer n, using count number of digits"""
        return "".join([str((n >> y) & 1) for y in range(count-1, -1, -1)])

So, for example, if the remit field contains a value of 4198, the binary string representation, '1000001100110', can be interpreted as follows: missed the payment 12 months before, made five payments, missed two, made two, missed two, made one."

#### Creating a performance outcome measure

This provides a large menu of choices as to how to define *performance.* For example, if *remit = 0*, a loan has missed no payments in the preceding 12 months.

```{r, results="asis", echo = FALSE, warning=FALSE}
#overwriting names from earlier code chunks in the Rmd file frees memory
d <- NULL
p <- NULL
q <- NULL
nofico <- NULL
lowfico <- NULL
si <- NULL
slope <- NULL
intercept <- NULL
liq <- NULL
cds <- NULL
res2 <- NULL

con <- dbConnect(drv, username="root", password="", dbname ="dlf", host="localhost")
res <- dbGetQuery(con, "SELECT deal, remit, fico FROM y7")
#cs is shorthand for credit score
cs <- as.tibble(res)
```

Unfortunately, there were few of those in the 2007 pool (the loans in securitizations issued in 2006 that remained in the pool at the end of the first quarter 2007, we will now call **the pool** and any subsets will be drawn from it), only `r cs %>% filter(remit == 0) %>% nrow` loans with perfect payment records.

```{r, results="asis", echo = FALSE, warning=FALSE}
permw <- function(n,r) n^r
``` 
A bigger problem is the large number of possible payment patterns over a 12-month period. It's the permutation of two objects (pay/no pay) taken 12 times with replacement. This means that there are potentially `r as.character(prettyNum(permw(2,12)),big.mark = ",")` different possibilities.

We *could* treat delinquencies as a continuous variable, but that creates useless distinctions. Some domain specific knowledge will help sort things out.

1. *First payment delinquencies* in which the borrower fails to make the first payment due on a loan generally require the seller to repurchase the loan.
2. *Single payment delinquencies* in which the borrower has missed a payment (after making the first payment due) but made it in the following month (called the *one time thirty test*) are not considered serious, if not repeated.
3. *Two payment delinquencies* in which the as missed two payments (after making the first payment due) but made whether or not the payments were made in the third month (called the *two times thirty test*) are considered serious that it is customary for the seller to warrant that at the time of sale no such event has occurred.
4. *Three payment delinquencies* are referred to as *defaults,* whether or not the borrower subsequently cures. As a rule, loans with a history of default that has been cured can be sold only at a discount.
5. *Uncured defaults* are considered the most serious. Even if the payments missed are the three most recent and the borrower *may* repay in the following month, such a loan is classified as *nonperforming.*
6. *Loans in foreclosure* or *REO* (real estate owned, loans that have been foreclosed and awaiting sale) are also *nonperforming.*

This suggests a 3-factor performance metric:

* A: Performing: No more than two delinquencies in any reporting stratum
* B: Troubled: Three or more delinquencies a reporting stratum but current on most recent reporting date
* C: Non-performing: Uncured default on most recent reporting date

The problem is thus a *classification* problem. Do FICO scores allow us to classify loans? The first task is to assign each loan its appropriate metric.

#### Implementing the metric

From the 2007 pool database, we can pull the loan number, deal encoded remittance history and fico. With that we can decode the remittance history. Here's a sample with only loans having FICO scores below 500.

```{r, results="asis", echo = FALSE, warning=FALSE}
library(tidyverse)
library(DBI)
library(knitr)
library(kableExtra)
library(RMySQL)
drv <- dbDriver("MySQL")
con <- dbConnect(drv, username="root", password="", dbname ="dlf", host="localhost")
res <- dbGetQuery(con, "SELECT ctapeno, deal, remit, fico FROM y7")
#cs is shorthand for credit score
cs <- as.tibble(res)
require(binaryLogic)
require(stringi)
reports <- cs %>% rowwise %>% mutate(repbin = toString(as.binary(remit))) %>% rowwise %>% mutate(repstr = str_replace_all(repbin,"[, ]",'')) %>% mutate(reports = str_count(repstr)) %>% select(ctapeno, deal, fico, repstr, reports)
ex_low <- reports %>% filter(fico < 500)
kable(head(ex_low), format.args = list(big.mark = ","), caption = "2007 Q1 loans by loan number, remittance code and fico < 500 (example")  %>% kable_styling(bootstrap_options = "striped", full_width = F) 
```

In preparing a routine to parse the reports fields to classify the performance categories, I found a potential problem. The number of months reported is variable due to several factors: Some loans in the pool were recently originated, some were seasoned, and others intermediate. The distribution looks like the following histogram.

```{r, results="asis", echo = FALSE, warning=FALSE}
p <- ggplot(reports) + geom_histogram(aes(x=reports), binwidth = 1)
p
```
Observational data never organizes itself conveniently. Parsing the differing numbers of monthly reports is feasible, but I decided to limit the analysis of FICO and performance to the cohort of loans with 11-month reporting histories.

We can now add a column for performance category, and take a quick look at how those categories relate to FICO with the approximately `r as.character(prettyNum(nrow(filter(reports, reports == '11'))),big.mark = ",")` loans with an 11-month payment history, about a third of the pool.

```{r, results="asis", echo = FALSE, warning=FALSE}
elevens <- reports %>% filter(reports == 11)

perf11 <- elevens %>%
    mutate(
    category = case_when(
      str_detect(repstr, '111$') ~ "C",
      str_count(repstr, '0') > 9 ~ "A",
      str_count(repstr, '0') < 9 ~ "B"
    )
  ) %>% select(ctapeno, deal, fico, category)

p <- ggplot(perf11,aes(x=category, y = fico)) + geom_boxplot() + ggtitle("FICO scores for loans remaining in 2007\nwith 11-month payment history by performance") + labs(y = "FICO", x = "A = Performing, B = Non-performing, C = Defaulted")
```

The boxplots show that the three categories of loans are broadly similar. The blow to orthodoxy is that the performing loans are composed of somewhat lower FICO scores than the non-performing and defaulted loans.

The distributions of the three categories show the same non-normal character as the original pools, but this time we'll calculate them using the Shapiro-Wilk normality test based on a sample of 5,000, due to limitations of the test.

```{r, results="asis", echo = TRUE, warning=FALSE}
perf11A <- perf11 %>% filter(category == 'A')
shapiro.test(perf11A$fico) # No sampling needed,  n < 5000, function's limit

```


```{r, results="asis", echo = TRUE, warning=FALSE}
perf11B <- perf11 %>% filter(category == 'B')
smpB <- sample_n(perf11B, 5000)
shapiro.test(smpB$fico) # more than 5000 in dataset 
```

```{r, results="asis", echo = TRUE, warning=FALSE}
perf11C <- perf11 %>% filter(category == 'C')
shapiro.test(perf11C$fico)
``` 

Category A *might* be normally distributed. With only 46 loans, it hardly matters. Categories B and C are definitely not, according to the results of the test.

[credit disclosure]: https://goo.gl/uhX1Pc
[LBMLT 2006-1]: https://www.sec.gov/Archives/edgar/data/1119605/000114420406002461/v033798_fwp.htm

[Pinto testimony]: https://democrats-oversight.house.gov/sites/democrats.oversight.house.gov/files/documents/Fannie%20Freddie%20Testimony%20of%20Edward%20Pinto%2012.9.08%20written%20submission%20Full.pdf

<!--chapter:end:03-failure-part1.Rmd-->

# The subprime mortgage crisis unfolds in early 2007, part 2

Keywords: refactoring, principal component analysis

## The fall of FICO

The problem with the conventional wisdom of long standing is that it loses sight of history. The prominence of FICO in home loan credit underwriting described in the [Pinto Testimony] had its origins in a different time (the early 1990s) and a different lending environment. Freddie Mac was in a good position to ensure that all other things *were* equal. It made only what came to be called "prime" loans, generally for no more than 80% of the value of the property, under more stringent limitations on the debt-to-income ratio of the borrower and many other criteria that it kept within a narrow range, and offered only a few varieties of loans.

In the subprime market that emerged in the late 90s, all of those factors changed. Criteria that were narrow became broad, documentation was relaxed and a widespread assumption was that continually rising home values would preclude any problems. It's not surprising that FICO lost its predictive power.

## Restructuring the data

Some of the testing of FICO as a useful metric involved subsetting the data. There were many more variables than the ones used, some of them categorical and some categorical coded as numeric. One potentially useful variable is location, because we know that real estate value are location sensitive. We have four location fields in the database, all derived from the postal zip code:

* The zip code itself, which is generally either much smaller or much larger than the real estate market, and also changes at the convenience of the postal service. *See* the discussion at [On the use of ZIP codes and ZIP code tabulation areas (ZCTAs) for the spatial analysis of epidemiological data].

* The metropolitan area derived from the U.S. Census ZIP code tabulation area, but covers a larger area than most real estate markets

* Longitude and latitude dervived from the ZCTA's, used for mapping

As a compromise, I converted the 5-digit zip codes into 3-digit zip codes. In metropolitan areas, the 3-digit codes are the sizes comparable to how the multiple listing services divide the market. We'll see if there is any value in this proxy measure of real estate market.

```{r, results='hide', echo = FALSE, warning=FALSE}
library(tidyverse)
library(DBI)
library(RMySQL)
drv <- dbDriver("MySQL")
con <- dbConnect(drv, username="root", password="", dbname ="dlf", host="localhost")
res <- dbGetQuery(con, "SELECT ctapeno, deal, remit, fico, dti, cltv, orate, obal grade, round(zip/100,0), dtype, fpd,ltype, pmiflag, ppp, otype, purpose, ptype FROM y6c")
cs <- as.tibble(res)
require(binaryLogic)
require(stringi)
reports <- cs %>% rowwise %>% mutate(repbin = toString(as.binary(remit))) %>% rowwise %>% mutate(repstr = str_replace_all(repbin,"[, ]",'')) %>% mutate(reports = str_count(repstr)) %>% select(ctapeno, repstr, reports)
perf <- reports %>% mutate(category = case_when(str_detect(repstr, '111$') ~ "C", str_count(repstr, '0') <= 9 ~ "B", str_count(repstr, '0') > 9 ~ "A")) %>% select(ctapeno, category)
y6rf <- cs %>% inner_join(perf, by = "ctapeno")
#y6rf = y6 refactored
y6rf <- y6rf %>% mutate(zip = as.character(`round(zip/100,0)`), pmiflag = as.character(pmiflag), ppp = as.character(ppp), perf = category, grade = as.character((grade)))
y6rf <- y6rf %>% select(ctapeno, deal,fico,dti,cltv,orate, grade, dtype,fpd,ltype, pmiflag, ppp,  otype, purpose, ptype, zip, perf)
# dbWriteTable(con, "loans", y6rf) 
# test
# res <- dbGetQuery(con, "SELECT * from loans limit 25") passed
``` 

It was time to reorganize the database into a more streamlined version, that captured the information on performance (relieving the 11-month constraint) and transformed the fields that needed to be treated as categorical, rather than continuous. It's much more efficient to put this in a new SQL table than to keep in memory, especially since sampling will be involved. Here's the revised data layout:


    MariaDB [dlf]> describe loans;
    +-----------+--------+------+-----+---------+-------+
    | Field     | Type   | Null | Key | Default | Extra |
    +-----------+--------+------+-----+---------+-------+
    | row_names | text   | YES  |     | NULL    |       |
    | ctapeno   | double | YES  |     | NULL    |       |
    | deal      | text   | YES  |     | NULL    |       |
    | fico      | double | YES  |     | NULL    |       |
    | dti       | double | YES  |     | NULL    |       |
    | cltv      | double | YES  |     | NULL    |       |
    | orate     | double | YES  |     | NULL    |       |
    | grade     | text   | YES  |     | NULL    |       |
    | dtype     | text   | YES  |     | NULL    |       |
    | fpd       | text   | YES  |     | NULL    |       |
    | ltype     | text   | YES  |     | NULL    |       |
    | pmiflag   | text   | YES  |     | NULL    |       |
    | ppp       | text   | YES  |     | NULL    |       |
    | otype     | text   | YES  |     | NULL    |       |
    | purpose   | text   | YES  |     | NULL    |       |
    | ptype     | text   | YES  |     | NULL    |       |
    | zip       | text   | YES  |     | NULL    |       |
    | perf      | text   | YES  |     | NULL    |       |
    +-----------+--------+------+-----+---------+-------+
    18 rows in set (0.00 sec)

    
Between the first row (a record identifier) and the last row (the performance category) are the sixteen variables we have to predict the performance outcome. For the almost 100,000 records, that is 1.5 million pieces of information. Technically we are in 16-dimensional space, and we need a way of flattening the dimensionality to be able to question the data.


[On the use of ZIP codes and ZIP code tabulation areas (ZCTAs) for the spatial analysis of epidemiological data]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1762013/




[Pinto testimony]: https://democrats-oversight.house.gov/sites/democrats.oversight.house.gov/files/documents/Fannie%20Freddie%20Testimony%20of%20Edward%20Pinto%2012.9.08%20written%20submission%20Full.pdf


<!--chapter:end:04-failure-part2.Rmd-->

# The Enron Email Corpus

## Case Description

[Enron Corporation] was a $100 billion annual revenue company:

1. They were in the gas and electricity business, mainly as traders, rather than as a utility
2. California had an auction process for electricity that Enron was manipulating
3. Enron used special purpose entities in a way that hid its financial condition
4. There was a special purpose entity used for a deal involving barges in Nigeria
5. Several individuals, including the CEO and his deputy were prosecuted
6. Many employees lost their retirement savings when Enron stock became worthless

The Federal Energy Regulatory Commission (**FERC**) investigated Enron's activities in the western U.S. wholesale electricity market for evidence of price manipulation and other violations. It obtained approximately 500,000 copies of emails from 149 email users. Copies of these were acquired by Leslie Kaelbling of MIT and published by William W. Cohen of Carnegie Mellon University. It is one of the largest publicly available datasets of corporate email and is referred to as the [Enron Corpus]. The term *corpus* is used in natural language processing to denote a collection of related text.

Civil and criminal litigation of other cases is conducted either by commercial or proprietary software. Much of the focus is directed to keyword searches and depends on visual scanning of emails by attorneys. Email examination can be a substantial expense.

Although "smoking gun" emails may be found, brute force examination misses opportunities to understand the social networks that reflect how the organization operates, what their concerns are and which part of the corpus should receive priority. To do that the corpus must be distilled.

## Data preparation

The data was provided in the form of a directory tree of text copies of emails in the file folders of the **custodians** (users), rather than in "native" format. The version of the [Enron Corpus] that I used is dated August 21, 2009.

The directory tree of one of the users is representative:

 ![An email folder](https://s3-us-west-2.amazonaws.com/tuva/DirTree.jpg)

Each of these files is plain text and contains the following types of data:

![Parts of an email](https://s3-us-west-2.amazonaws.com/tuva/parse_email.png)
### Preparation
 
Because the same message body resides in multiple folders of multiple custodians, some way was needed to de-duplicate.

The method differed dependant on whether the email was originated on the IBM Notes system or Microsoft Outlook. In either case, however, it consisted traversing the directoryl tree and extracting the same fields 

* Sender
* Date
* Receiver(s)
* cc(s)
* Message body
* file-name

and, in addition, adding a digital signature (MD5 digest) to the message body to nearly guarantee its uniqueness. (The body is the content of the originating email strip of meta-data, legends and disclaimers, and replies and replies to replies. It is the payload.)

This was done by traditional Unix command line tools, perl and Python scripts and other tools to create a database with the following structure:

    +----------+--------------+------+-----+---------+-------+
    | Field    | Type         | Null | Key | Default | Extra |
    +----------+--------------+------+-----+---------+-------+
    | body     | mediumtext   | YES  |     | NULL    |       |
    | lastword | mediumtext   | YES  |     | NULL    |       |
    | hash     | varchar(250) | YES  | UNI | NULL    |       |
    | sender   | varchar(250) | YES  |     | NULL    |       |
    | tos      | text         | YES  |     | NULL    |       |
    | mid      | varchar(250) | YES  |     | NULL    |       |
    | ccs      | text         | YES  |     | NULL    |       |
    | date     | datetime     | YES  |     | NULL    |       |
    | subj     | varchar(500) | YES  |     | NULL    |       |
    | tosctn   | mediumint(9) | YES  |     | NULL    |       |
    | ccsctn   | mediumint(9) | YES  |     | NULL    |       |
    | source   | varchar(250) | YES  |     | NULL    |       |
    +----------+--------------+------+-----+---------+-------+

Of the approxiately 500,000 emails in the [Enron corpus], approximately half are duplicates. A large part of the remainder consists of newsletters, bulletins, fantasy football matters and other emails addressed to a large audience with only one or a few Enron recipients. Say, 125,000. The remaining 125,000 have roughly 75,000 addressed to large groups ("be advised that the Houston gym hours will be changing") or to individuals on routine matters ("your approval for expense report #1234 is overdue"). Another 25,000 deal with scheduling of meetings, transmission of periodic reports, and circulation of form documents covering derivative trading with counterparties. A cull list of senders and topics was developed to extract these.

Of the remaining emails, approximately 15,000 involve correspondence from a sender to a custodian who never sends a reply or an original email to the sender. This leaves about 35,000 emails among senders and receivers who engage in some degree of reciprocal correspondence. This is where to begin. Insights from this group can be used to recycle over the discards.

## Strategy for exploration

### Don't look for much from the big shots

CEO Ken Lays's administrative assistant handled much of his email, CFO Andy Fastow's email is not included. COO Jeff Skilling is included, but his volume is small. On the other hand, some of the largest senders are relatively low ranking, a legal assistant distributing documents and a lobbyist in California.

### Volume is not evenly distributed among users

![Number of emails by sender](http://media.richard-careaga.com/img/BigSenders.png)
 

### Keywords may not help much

Natural language processing (**NLP**) approaches based on written composition are of little help in the misspelled, ungrammatical, freeflowing, implicit meaning-rich world of email. Reviewing email is much more like eavesdropping than reading.

First, it is essential to have a well-thought out file of stop words to eliminate the most common words, which tend to be "glue" words. Second, the business of Enron was trading, and the tool of trading is and was the Bloomberg terminal with its instant messaging feature. The conduct of a trading floor, more than most other large corporate enterprises, is effectuated face-to-face and by telephone. Don't look for meeting agendas and minutes.

Every organization has a vocabulary profile that is unique to its business. To develop candidate lists for Enron, I used the following NLP program:

	#!/usr/bin/env python
	# encoding: utf-8
	"""
	oddfreq.py: word frequency list, disregarding capitalization, excluding stopwords
	and words _not_ in standard dictionary, sorted by frequency

	Created on 2010-04-15
	Richard Careaga
	"""
	from itertools import izip, chain, repeat
	from Prep import *
	from util import *
	import nltk
	from nltk import FreqDist
	from nltk.corpus import stopwords

	# Natural Language Toolkit: unusual_words
	# from nltk and used by permission
	def unusual_words(text):
		text_vocab = set(w.lower() for w in text if w.lower().isalpha())
		english_vocab = set(w.lower() for w in nltk.corpus.words.words())
		unusual = text_vocab.difference(english_vocab)
		return sorted(unusual)

	# Natural Language Toolkit: code_plural
	# from nltk and used by permission
	def plural(word):
		if word.endswith('y'):
			return word[:-1] + 'ies'
		elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:
			return word + 'es'
		elif word.endswith('an'):
			return word[:-2] + 'en'
		else:
			return word + 's'

	def main():
		# query to select new topic email subject lines sent to single recipients
		sql =   sql = """SELECT date, subject FROM scrub where subject not \
				regexp 'RE:|Re:|re:|FW:|Fw:|fw:|FWD:|Fwd:|fwd:' and twoway is true \
				and tosctn = 1 and ccsctn = 0 and chaff is false"""
		# create an object
		C = Prepare()
		# do the necessary processing to return an nltk Text object
		X = C.prep(sql)
		# collect list of common words, such as prepositions, articles, etc.
		stops = stopwords.words('english')
		# extract word list after converting to all lowercase
		words = [w.lower() for w in X if w.isalpha() and w.lower() not in stops]
		# extract unique words
		vocab = set(words)
		# fetch a standard English vocabular
		english_vocab = set(w.lower() for w in nltk.corpus.words.words())
		# convert to a list for pluralization
		english = list(english_vocab)
		# make a list of "plural-like" words
		plurals = [plural(w) for w in english]
		# cobmine lists
		englishes = english + plurals
		# make a set of the list
		englishes_vocab = set(englishes)
		# compare to the vocabulary extracted from the emails
		oddball = vocab.difference(englishes_vocab)
		# convert to a list
		enronic = list(oddball)
		# sort the list
		enronic.sort()
		# create a new list of the text _without__  those words
		words[:] = [w for w in words if w not in enronic]
		# recycle the original vocabulary definition
		# this allows the remainder of the code to be reused as is
		vocab = set(words)
		# prepare dictionary of word frequencies
		fdist = FreqDist(words)
		# collect the unique words for sorting
		alphalist = list(vocab)
		# sort the unique words
		alphalist.sort()
		# create a list to hold results
		LA = []
		# collect key:item pairs for word:frequency in alphabetical order
		for item in alphalist:
		   bag = []
		   bag.append(item)
		   bag.append(fdist[item])
		   LA.append(bag)
		# calculate width of word field and add one
		width = find_max_width(LA,0)+1
		# calculate width of frequency field and add one
		numwidth = find_max_width(LA,1)+1
		# assign a desired pagewidth
		pagewidth = 85
		# assign desired margins
		lmargin = 5
		rmargin = 5
		margins = lmargin + rmargin
		# calculate column width
		columnwidth = width + numwidth
		# calculate whole number of columns that will fit (floor division)
		columns = (pagewidth-margins)//columnwidth
		# assign a gutter width
		gutter = 4*' ' # 4 spaces
		# calculate available gutter
		copywidth = columnwidth*columns + len(gutter)
		# assign desired pagelength
		pagelength = 54
		# open a file for output
		f = open("enronic.txt", 'w')
		# convenience definition to append a newline
		nl = '\n'
		# generate an iterator object to fetch 54 lines of the word frequency
		# list at a time
		g = grouper(pagelength,LA)
		# iterate through the object until exhausted
		while g:
			# write a ruler and newline
			f.write("="*copywidth)
			f.write(nl)
			# chunk first two batches of pagelength lines side by side
			z = zip(g.next(),g.next())
			# print each pair
			for row in z:
				stringline = ("%s%s%s%s%s") % (repr(row[0][0]).ljust(width),\
				repr(row[0][1]).rjust(numwidth), gutter, \
				repr(row[1][0]).ljust(width),\
				repr(row[1][1]).rjust(numwidth))
				f.write(stringline)
				f.write(nl)

		f.close()

	if __name__ == '__main__':
		main()

which produced pages like the following (here limited to words with 10-99 occurrences):

    aec                        17    aloha                       1
    aep                        19    alport                      1
    ag                         18    amerada                     6
    agl                         3    amerex                     22
    anahiem                     1    aron                       11
    anglo                       1    asap                       14
    apb                        90    atoka                       1
    api                        10    attaching                   1
    approved                   29    attending                   1
    australia                  15    bayer                       3
    autoreply                 220    bball                       1
    bandwidth                  14    bennett                     4
    barnett                    20    berney                      1
    bge                         3    bp                         16
    bgml                        1    bpa                        16
    bingaman                    1    brazos                     15
    bio                        10    breakeven                   1
    blackline                   6    bridgeline                 17
    bloomberg                  14    brl                         1
    bmo                         1    broadband                  11
    bnp                        18    bros                        1
    boise                       1    bruce                      10
    byler                       1    cargill                    40
    caiso                      18    carolyn                     1
    calgary                    14    cartersville                1
    calif                      11    cashion                     1
    calley                      1    catalytica                 12
    calpine                    47    ccf                         1
    caminus                     4    cdwr                       13
    canceled                   15    cementos                    1
    cancun                      2    centana                    14
    ceo                        11    cibc                        3
    cera                       24    cif                         1
    cfd                         1    cinergy                    21
    cftc                       17    cipico                      1
    cgas                       16    cirino                      1
    changed                    16    clair                       6
    checking                    3    clickathome                23
    checklist                  13    clicking                    7
    checkosut                   1    clickpaper                 40
    checkout                  160    clifford                    3
    chemconnect                 1    closing                    23
    chicago                    24    cmr                         1
    chilkina                    1    cms                        10
    cng                        14    confer                      1
    cogen                      10    confirmlogic                4
    coi                         1    congrats                   18
    coleman                    10    congratulotions             1
    colstrip                    1    conoco                     10
    completed                  10    corhshucker                 1
    conf                       14    countdown                   1
    confederated                3    counterparties             45
    counterparty               64    curtis                      2
    cp                         12    cuves                       1
    cps                        14    cysive                      1
    cpuc                       25    dabacle                     2
    cpy                         2    dabhol                     18



### Don't neglect time series

In looking at a subset, pay attention to how volume varies with time. As others have noted, a sudden drop in email volume within the senior group, such as occurred in May 2001, can indicate a situation in which decisionmakers are meeting personally. When trouble looms, people clam up.

The traffic in 2001 suggests lines of inquiry focused on specific periods.

![May gap](http://media.richard-careaga.com/img/emails2001.png)

### Avoid the echo chamber

Email streams in which an initial email goes out to a group with responses coming back quoting the original email, forwards and reforwards and responses, amplify whatever NLP content can be gleaned. Parse each email to determine its original content load and delete that from replies and forwards, as well as standard disclaimers.

### The REAL value proposition

The purpose of examining a huge body of email is not to find a smoking gun. It is to understand the business, its language and its people. While you may sometimes run across braggadocio crowing over putting one over, those are rare and seldom determinative.

## Some preliminary results

### Unique senders

     mysql> select count(*) from usenders;
     +----------+
     | count(*) |
     +----------+
     |    17568 |
     +----------+

### Unique receivers
 
     mysql> select count(*) from ureceivers;
     +----------+
     | count(*) |
     +----------+
     |    68199 |
     +----------+

### Senders who are also receivers

    mysql> select count(*) from twoway;
    +----------+
    | count(*) |
    +----------+
    |    10235 |
    +----------+

### Sender/receivers with Enron addresses

    mysql> select count(*) from insiders;
    +----------+
    | count(*) |
    +----------+
    |     6099 |
    +----------+

### Subject line words

    mysql> select count(*) from wordlist;
    +----------+
    | count(*) |
    +----------+
    |   141180 |
    +----------+

Two of the most common words are *power* and *energy*. There seems to be a difference, however, in how different senders tend toward one or another

![Power vs. Energy](http://media.richard-careaga.com/img/joule.png)

and the time distribution differs


![Power vs. Energy over time](http://media.richard-careaga.com/img/energypower.png)
 
### Places mentioned

> 	Abu Accra Addis Agra Ak Akron Al Almaty Amman Andorra Angola Ankara Ar Aruba Ashmore Astana Atoll Az Baghdad Bahamas Bahrain Baker Bakersfield Baku Balkans Baltimore Bandar Bangalore Bangkok Bangladesh Barbados Barbuda Barcelona Barranquilla Barthelemy Beaumont Beijing Belarus Belgium Belgrade Belize Bellevue Belo Benin Berkeley Berlin Bermuda Bernardino Bhopal Birmingham Bogota Boise Bolivia Bonn Bosnia Boston Botswana Brasilia Brazil Bremen Bridgeport Brisbane Bristol Britain British Brownsville Brunei Brussels Bucharest Budapest Buenos Buffalo Bulgaria Burbank Burma Bursa Burundi Ca Caicos Cairo Caledonia Calgary Cali California Cambodia Cambridge Cameroon Campinas Campo Canada Cancun Cape Caracas Carolina Carrollton Cartagena Cartier Casablanca Cayman Cebu Cedar Chad Chandler Charlotte Chattanooga Chengdu Chennai Chesapeake Chiba Chicago Chihuahua Chile China Chon Christi Cincinnati Ciudad Clarita Clarksville Clearwater Cleveland Cochabamba Collins Colombia Colombo Colorado Columbia Columbus Comoros Concord Congo Connecticut Cook Coral Cordoba Corona Costa Cote Cotonou Covina Croatia Ct Cuba Cucamonga Cuiaba Culiacan Curitiba Cyprus Czech Dakota Dali Dallas Daly Damascus Dar Davao Davidson Daye Dayton Dc De Delaware Delhi Denmark Denver Detroit Dhaka Diego District Dominica Dominican Dongguan Dortmund Downey Dubai Dublin Duesseldorf Duque Durban Durham Dushanbe Ecuador Edmonton Egypt Emirates England Erie Escondido Essen Estonia Ethiopia Eugene Europa Europe Evansville Faridabad Faroe Fayette Fayetteville Fiji Finland Fl Flint Florida Fm Fontana Fort Fortaleza Foshan France Francisco Frankfurt Fremont Fresno Fukuoka Fullerton Ga Gabon Garland Gary Gaza Genova Georgia Germany Ghana Ghaziabad Gibraltar Gilbert Giza Glasgow Glendale Gold Greece Green Greenland Greensboro Grenada Gu Guadalajara Guadeloupe Guam Guatemala Guernsey Guinea Guyana Ha Haiti Hama Hamburg Hamilton Hampton Hangzhou Harare Harbin Hartford Havana Haven Hawaii Hayward Helsinki Henderson Hialeah Hiroshima Ho Hollywood Homs Honduras Hong Honolulu Houston Howland Hungary Huntsville Hyderabad Ia Iceland Id Idaho Il Illinois Independence India Indiana Indianapolis Indonesia Inglewood Iowa Iran Iraq Ireland Irkutsk Irvine Irving Islamabad Israel Istanbul Italy Jackson Jacksonville Jakarta Jamaica Jammu Jamnagar Japan Jarvis Jeddah Jersey Jerusalem Jilin Jinjiang Jintan Joao Johannesburg Joliet Jordan Juan Juarez Jurong Kabul Kalyan Kano Kanpur Kansas Karachi Kathmandu Kawasaki Kazakhstan Kazan Keeling Kentucky Kenya Kerman Khartoum Kingdom Kingman Kingston Kiribati Knoxville Kobe Kolkata Kong Korea Kosovo Kota Kozhikode Krakow Krasnoyarsk Ks Kuala Kuwait Ky Kyiv Kyoto Kyrgyzstan La Lafayette Lagos Lahore Lakewood Lancaster Lanka Lansing Lanzhou Laos Laredo Las Latvia Lauderdale Lebanon Leon Leone Lesotho Lexington Liberia Libya Liechtenstein Lima Lincoln Lisbon Lithuania Liverpool Lomas London Los Louisiana Louisville Lowell Lubbock Lucia Lusaka Luxembourg Ma Macau Macedonia Madagascar Madison Madrid Maine Malaga Malawi Malaysia Maldives Mali Malta Manaus Manchester Mangalore Manila Maputo Mariana Marshall Martin Maryland Massachusetts Mauritania Mauritius Mayen Mcallen Md Medellin Melbourne Memphis Mendoza Merida Mesa Mesquite Mexicali Mexico Mh Mi Miami Michigan Midway Milan Milwaukee Minneapolis Minnesota Mississippi Missouri Mn Mo Mobile Modesto Mogadishu Moines Moldova Monaco Mongolia Montana Monte Monterrey Montgomery Montreal Moreno Morocco Moron Moscow Mosul Mozambique Mp Ms Mt Muenchen Mumbai Myanmar Nagoya Nagpur Nairobi Namibia Nanchong Nanhai Nanjing Nanning Nanyang Naperville Naples Nashik Nashville Natal Nauru Navi Nc Ne Nebraska Nepal Netherlands Nevada Newark Newport Nh Nicaragua Niger Nigeria Niigata Ningbo Nj Nm Norfolk Norwalk Norway Nottingham Nova Novokuznetsk Novosibirsk Nv Ny Oakland Oaks Oceanside Odessa Ohio Oklahoma Omaha Oman Ontario Oran Orange Oregon Orlando Orleans Osaka Oslo Ottawa Overland Oxnard Pa Pacific Pakistan Palermo Palestine Palmdale Panama Papua Paris Pasadena Paso Paterson Patna Pembroke Pennsylvania Peoria Perm Peru Peshawar Petersburg Philadelphia Philippines Phoenix Pittsburgh Plano Poland Polynesia Pomona Ponce Port Portland Porto Portsmouth Portugal Pr Prague Prairie Preston Principe Providence Provo Puebla Pueblo Puente Puerto Pune Pw Pyongyang Qatar Qingdao Quebec Quezon Quito Rabat Raleigh Recife Reno Ri Richmond Riga Rio Riverside Riyadh Rizhao Rochester Rockford Romania Rome Rosario Rotterdam Russia Rwanda Sacramento Safi Sahara Saint Sale Salem Salinas Salt Saltillo Salvador Samoa San Santa Santiago Santo Sao Sapporo Sarajevo Saudi Savannah Sc Scotland Scottsdale Sd Seattle Senegal Seoul Serbia Seychelles Shanghai Sheffield Shenzhen Shiraz Shreveport Simi Singapore Sioux Slovakia Slovenia Sofia Solomon Somalia Soviet Spain Spokane Springfield Stamford Sterling Stockholm Stockton Stuttgart Sudan Sunnyvale Surabaya Surat Suriname Suzhou Swaziland Sweden Switzerland Sydney Syracuse Syria Tacoma Taipei Taiwan Tajikistan Tallahassee Tampa Tanzania Tashkent Tehran Tel Tempe Tennessee Texas Thailand Thane Tianjin Tijuana Timor Tn Tobago Togo Tokyo Toledo Tome  Tonga Topeka Torino Toronto Torrance Torreon Trinidad Tripoli Trujillo Tucson Tulsa Tunisia Turkey Turkmenistan Turks Tx Tyumen Ufa Uganda Ukraine Uruguay Ussr Ut Utah Uzbekistan Va Valencia Vallejo Vancouver Vatican Vegas Venezuela Ventura Veracruz Verde Vermont Vi Vienna Vietnam Vijayawada Virgin Virginia Vt Wa Waco Wake Wales Wallis Warren Warsaw Washington Waterbury Wenzhou West Westminster Wi Wichita Winnipeg Winston Wisconsin Worcester Wuhan Wuwei Wv Wy Wyoming Xinyi Yemen Yicheng Yokohama Yonkers York Yueyang Yugoslavia Zagreb Zambia Zamboanga Zealand Zimbabwe Zurich

### Periodicity

![periodicity](http://media.richard-careaga.com/img/1999-2002.png)

**Things to note:**

A. The difference between 1999 and 2000 may reflect differences in levels of activity or availability of data or both. Each day in the three-year period is represented. The data points near the zero level generally represent weekends and holidays.

B. Emails in 2000 build to a peak in December.

C. Emails in 2001 build to a peak in June and another pair in October and November, but there is no December peak.

D. Instead there is a January 2002 peak.

E. The blue line is the smoothed trend of the data. Point above or below the dark gray band show higher or lower activity than the trend.
 
### Social Networks

Starting with *penpals*, people who regularly exchange one-on-one emails, it is possible to infer the functional organization of an organization like Enron. Think of it like whispering. Going beyond that, avoid the spoke and wheel representations that you so often see. Most of those connections end up being broadcast email. What helps is to look at email patterns over relatively small intervals.

![Week 39, 2001](https://s3-us-west-2.amazonaws.com/tuva/wk39net.png)
The user pairs were anonomized to reduce analyst bias

## Future work

* Sampling of penpals to measure centrality and connectiveness
* Clustering
* NLP processing of email cliques

[Enron Corporation]: (https://en.wikipedia.org/wiki/Enron)
[Enron Corpus]: (https://www.cs.cmu.edu/~enron/)

<!--chapter:end:05-enron.Rmd-->

# Daycare Costs and Unexamined Assumptions {#daycare}

Keywords: Skepticism, Census data API

## Case Description

Data journalism is a welcome trend, but it has not always been accompanied by a rise in thoughtful analysis. I saw in a 2015 article from [Vox] and I was annoyed that the article relied on a map just to show percentages (why not a table?) and then began noticing other problems, such as the lack of a date for the data.

![Vox article on day care](https://cdn.vox-cdn.com/thumbor/xf2ee3OS9VrbZRYnGdImMWJImEc=/0x0:1200x630/920x0/filters:focal(0x0:1200x630)/cdn.vox-cdn.com/uploads/chorus_asset/file/3751728/childcaremap.png)

```{r setup3,  echo=FALSE, results= 'asis', warning=FALSE}
library(knitr)
library(kableExtra)
library(readr)
library(scales)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
options(knitr.table.format = "html")
cost <- read.csv("assets/DayCare.csv")
names(cost[1]) <- 'Dollars'
kable(cost, format.args = list(big.mark = ","), caption = "2014 Annual Costs for Infant Daycare") %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

[Source] Child Care Aware of America

The costs come from a [source] that gives separate estimates for an infant, a four-year old and a school child at day-care centers and the [Vox] article doesnt say which age group it used to calculate the percentages of median income that costs represent. Worse, it uses median *household* income, when the source used *median state income* (*see* [source] at *fn* 49, *p.* 2.). (Unfortunately the [source] cites to a Census data table that no longer exists.)

## With the right metrics

Do the [Vox] estimates agree with both the costs reported by [Cost of Daycare] and the Census 2014 median household income, which is what [Vox] used?

```{r   mhi, echo = FALSE, warning=FALSE, message=FALSE, results= 'asis'}
library(acs)
#mhi <- acs.fetch(2014, span = 1, geo.make(state = "*"), table.number = ("B19013"))
mhi_acs <- acs.fetch(2014, span = 1, geo.make(state = "*"), table.number = ("B19013")) 
mhi_df <- as.data.frame(mhi_acs@estimate) %>% tibble::rownames_to_column()
colnames(mhi_df) <- c('State', 'Median_household_income') 
mhi_tbl <- mhi_df %>% filter(State != 'Puerto Rico')
cost_mhi <- cost %>% inner_join(mhi_tbl, by = 'State') 
pctdc <- cost_mhi %>% mutate(Percentage = Cost/Median_household_income) %>% mutate(Percentage = round(Percentage*100))
kable(pctdc, format.args = list(big.mark = ","), caption = "2014 Cost of Day Care for infants and median household income") %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

Sources: [source] and 2014 U.S. Census ACS Table B19013

Wherever [Vox] derived its data on median household income, the percentages conflict with the official Census data.

Repeating the table using median *family* income of families with children under 18 changes the picture even more.

```{r  mhi2, echo = FALSE, warning=FALSE, message=FALSE, results= 'asis'}
mfi <- acs.fetch(2014, span = 1, geo.make(state = "*"), table.number = ("B19126"))
mfi_acs <- acs.fetch(2014, span = 1, geo.make(state = "*"), table.number = ("B19126")) 
mfi_df <- as.data.frame(mfi_acs@estimate[,3]) %>% tibble::rownames_to_column()
colnames(mfi_df) <- c('State', 'Median_family_income') 
mfi_tbl <- mfi_df %>% filter(State != 'Puerto Rico')
cost_mfi <- cost %>% inner_join(mfi_tbl, by = 'State') 
pctdcf <- cost_mfi %>% mutate(Percentage = Cost/Median_family_income) %>% mutate(Percentage = round(Percentage*100))
kable(pctdcf, format.args = list(big.mark = ","), caption = "2014 Cost of Day Care for infants and median family income for families with children under 18") %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

Sources: [Cost of Daycare] and 2014 U.S. Census ACS Table B19126

The table below compares the [Vox] table with this analysis.

```{r  diff, echo = FALSE, warning=FALSE, message=FALSE, results= 'asis'}
pctdif <- pctdc %>% inner_join(pctdcf, by = 'State') %>% dplyr::select('State', 'Percentage.x', 'Percentage.y')
colnames(pctdif) <- c('State', 'Vox', 'This_paper') 
pctdif <- pctdif %>% mutate(Difference = Vox - This_paper)
kable(pctdif, format.args = list(big.mark = ","), caption = "Differences in estates of 2014 Cost of Day Care data for infants as a percentage of median income between Vox article and this paper") %>% kable_styling(bootstrap_options = "striped", full_width = F)
```

## Takeaway
> ,  

Trust, but verify. This is true of any data, but especially data that you can trace trace to an authoritative source. And it is critical to think about the choice of a benchmark to evaluate one set of data against another. Here, the costs of day care came from a credible source and seem reasonable, based on anecdotal evidence. If this were more than an exercise, it would be important to seek out alternative estimates.

The data used to provide the cost of day care as a percentage of median household income proved a poor choice. Not *all* households have children. There is, however, data not only for *family households,* but even for *families with children under 18.* Relatively few households residing in retirement communities have day care expenses; it also turns out that families with children under 18 have a *higher* median income than the *all household* group.

There may be a way to determine median income of families with one or more children under 6, since the number of those families is collected or estimated (between the full Census years), which would provide an even better metric.

The data you have is not always the data you need.

Data sources: 

* [Cost of Daycare], Appendix I: 2014 Average Annual Cost of Full-Time Child Care by State, *pp.* 53-54 in pdf

* U.S. Census American Community Survey for 2014, using the **R** package by Ezra Haber Glenn (2018). acs: Download, Manipulate, and Present American Community Survey and Decennial Data from the US Census. R package version 2.1.3. https://CRAN.R-project.org/package=acs

[Vox]: https://www.vox.com/2015/6/3/8721331/child-care-costs
[Cost of Daycare]: http://usa.childcareaware.org/wp-content/uploads/2016/05/Parents-and-the-High-Cost-of-Child-Care-2015-FINAL.pdf
[source]: http://usa.childcareaware.org/wp-content/uploads/2016/05/Parents-and-the-High-Cost-of-Child-Care-2015-FINAL.pdf

<!--chapter:end:06-daycare.Rmd-->

# Assorted Examples of Toolmaking

## The Short Genome File: Day-to-Day Python

Keywords: Python, one-off, utility

### Case Description
One of the recent courses that I took at MIT had a simple short datafile [genome], representing a portion of the DNA sequences of *Caulobacter crescentus*, a species of bacteria very popular in biological studies. The class problem was in K-means clustering based on principal component analysis of the [genome] data. 

### Conversion Problem

That data consisted 1,528 lines of 200-character fragments. The building blocks of DNA genomes are very simple, consisting of sequences of only four possible character values, 'A','C','G','T'.

The immediate problem was that the assignment called for 300-character fragments. This is a recurring fact of life for data science; transforming data from one structure to another.

### Everyday Solution

The obvious tool was Python, and it took no more than 20 minutes to do, even for someone who used Python as a handtool rather than industrial machinery.

In short, we start with

> \>fragment of c.crescentus genome
>gccgatagcctatgatccc... [to 200 characters]

and then apply a short Python program

    # required modules
    import csv
    import itertools
    import textwrap

    data  = "/Users/rc/Desktop/MITx8/ccrescentus.fa" # your path here
    # Desired length of genome fragments
    fraglen = 300
    OBJECT = [] # empty bucket
    f = csv.reader(open(data, 'r'))

    # read the source file into a list
    for row in f:
        OBJECT.append(row) 
    # remove header '>fragment of c.crescentus genome'
    del(OBJECT[0])
    # combine all of the 200-character lines from the source
    laundry = list(itertools.chain(*OBJECT))
    # join them back into a single string
    washed = ''.join(laundry)
    # check to make sure each line of data can be 300 characters long
    overage = len(washed)%fraglen 
    # should return True to the console
    overage == 0
    # split the string into 300-character list elements
    dried = textwrap.wrap(washed,300)
    # write the processed file back to a processed file, adding
    # opening and closing double quotes and new lines
    with open('fragments.csv', 'w') as f:
        f.write('\"')
        f.write('\"\n'.join(dried))
    # "fragments.csv" is now ready for analysis with Python or R

and end up with

> "gccgatagcctatgatcccc ... [to 300 characters]
    
Athough I checked that I had enough data to make each line 300-characters long, I did lose a few bytes in the conversions, which isn't material for purposes of the exercise.

This is not intended as an example of best-practices Python programming, nor I am not a programmer. I use programming to solve problems. If I have a solution like this, I go to a programmer to scale it, when necessary.

[genome]: https://goo.gl/qYvDHA
## Hard but simple -- parsing YAML files in Haskell

It is a truth universally acknowledged that the best way to learn a language is from the lips of a lover. Mine, alas, has no Haskell, so I have made the choice I usually do. Rather than attempting first to master the rules of syntax and the rationale underlying a new computer language, I launch into trying to solve a real problem, by which I mean automating some repetitive task that I would otherwise have to do by hand.

In this case, it was tweaking a LaTeX table. I chose Haskell because I was already using the estimable **pandoc** to convert the rest of my document and it has a facility to pipe its AST (abstract syntax tree) through to a filter for intermediate processing before rendering the final document.

Alas, the functionality I sought was not available without doing some heavy lifting of internals that is beyond my pay grade as a rank Haskell beginner. I did manage to get a version working, but it had two major defects: First, it relied on parsing regular expressions. I imagine a term like unPythonic applies to this approach  unHaskellian? Second, it lacked a clean separation between logic and data, meaning it would have to be rewritten for each new table that differed from this first use.

The functionality could also be achieved through *sed* and *awk* in combination with the other standard tools of *bash*. But, as I had come this far with Haskell, I determined to continue.

I learned a lot along the way, but Im only going to report the results. There are any number of great resources on theory and concepts, but recipes seem hard to come by.

To begin a yaml file, table.yaml

    stripComments: true
    stripLable: true
    zeroEntry: "."
    justifyLeft: true
    stubHeader: "Cause of Death"
    subHeader: "& (\\%) & (\\%) & (\\%) & (\\%)"

In the same directory, mwe.hs

    {-# LANGUAGE OverloadedStrings #-}  -- yaml

    module Mwe where

    import Data.Yaml 
    import Data.Maybe (fromJust)

    data ReadData = ReadData { stripComments     :: Bool
                             , stripLable      :: Bool
                             , zeroEntry       :: Bool
                             , justifyLeft     :: Bool
                             , stubHeader      :: String
                             , subHeader       :: String
                             } deriving (Eq, Ord, Show)

    instance FromJSON ReadData where
      parseJSON (Object v) = ReadData <$>
                             v .: "stripComments" <*>
                             v .: "stripLable"    <*>
                             v .: "zeroEntry"     <*>
                             v .: "justifyLeft"   <*>
                             v .: "stubHeader"    <*>
                             v .: "subHeader" 
      parseJSON _ = error "Can't parse ReadData from YAML/JSON"

I wont embarrass myself by revealing how long it took to get this working. I did pick up a fairly solid understanding of types, some insight into typeclasses and instances and got on the right track for IO and Maybe, with some notion of what it means to be pure in Haskell. What took the longest was how to do anything with a successful read of a yaml file other than to print it, which is where the examples I found stopped. I acknowledge my debt to the many sources I consulted to figure this out.

## Contextual Awareness

Whenever I see a time series that purports to show dramatic results, I like to look back to trace the prior history.

![Misleading unemployment data](http://media.richard-careaga.com/img/unemp.png)
 
## Minimalism Throws You into the Pool

I have been dabbling in the hipster lanaguage Lua, which you have to be cool even to have heard about. Its claim to fame is minimalism. It has one data type, called a table and I found myself wanting to combine two or more of them  la cat (concatenate). WTF, there is no built-in way to do this? Incredulity began to lift when I discovered that neither was there a built-in way to even print tables. As a public service for the benefit of other pilgrims, here is

    --cat.lua concatenate tables
     function cat( ... ) -- append 1D tables
         local List = require 'pl.List' -- penlight library
         local args = {...}
         local l = List()
         for i=1, #args do
             l:extend(args[i])
         end
         return l
     end
     --[[Example
     a = {
        "gala",
        "cameo",
        "jazz"
     }
     o = {
        "seville",
        "valencia",
        "navel"
     }
     g = {
        "concord",
        "thompson",
        "muscat"
     }
     f = cat(a,o,g)
     {gala,cameo,jazz,seville,valencia,navel,concord,thompson,muscat}
     --]]

## Parsing system date strings into Python datetime objects

     from datetime import datetime
     from dateutil.parser import parse
     from dateutil import tz

     s = "Mon Aug 15 21:17:14 GMT 2011"     
     d = parse(s)                           

     GMT = tz.gettz('UTC')                  
     Beijing = tz.gettz('Asia/Shanghai')    

     there = d.astimezone(GMT)              
                                            
     here = d.astimezone(Beijing)           

     print(here)
     print(there)


## Combination of k items, taken n at a time

Very functional

    -- combo.hs
    -- problem: C(k,n), where k = the integers from 1 to 9, inclusive
    -- and n = 3, without regard to order, then sum the subsets
    import Data.List
    combinations 0 lst = [[]]
    combinations n lst = do
        (x:xs) <- tails lst
        rest <- combinations (n-1) xs
        return $ x : rest
    result = ( map sum (combinations 3 [1..9]))

Python alternative

    import itertools
    result = map(sum, itertools.combinations([1,2,3,4,5,6,7,8,9], 3))
    for i in result: print(i)

## Flex/Bison to compile data parser for June 20, 2018 form

    datify.l Bison script

    /* NB: OSX, cannot link to -lfl, use -ll */
    /* can't have internal comments */

    %{
    %}

    %%

    ^[0-9]+         {printf("")             ; }
    "January "      {printf("2015-01-")     ; }
    "February "     {printf("2015-02-")     ; }
    "March "        {printf("2015-03-")     ; }
    "April "        {printf("2015-04-")     ; }
    "May "          {printf("2015-05-")     ; }
    "June "         {printf("2015-06-")     ; }
    "July "         {printf("2015-07-")     ; }
    "August "       {printf("2015-08-")     ; }
    "September "    {printf("2015-09-")     ; }
    "October "      {printf("2015-10-")     ; }
    "November "     {printf("2015-11-")     ; }
    "December "     {printf("2015-12-")     ; }

    %%

    int main()

    {
     yylex();
    }

    datify2.l Bison script

    /* NB: OSX, cannot link to -lfl, use -ll */
    /* can':t have internal comments */

    %{
    %}

    %%

    "-1"{1}[ \t\n]  {printf("-01\n")        ; }
    "-2"{1}[ \t\n]  {printf("-02\n")        ; }
    "-3"{1}[ \t\n]  {printf("-03\n")        ; }
    "-4"{1}[ \t\n]  {printf("-04\n")        ; }
    "-5"{1}[ \t\n]  {printf("-05\n")        ; }
    "-6"{1}[ \t\n]  {printf("-06\n")        ; }
    "-7"{1}[ \t\n]  {printf("-07\n")        ; }
    "-8"{1}[ \t\n]  {printf("-08\n")        ; }
    "-9"{1}[ \t\n]  {printf("-09\n")        ; }

    %%

    int main()
    {
     yylex();
    }


<!--chapter:end:07-tools.Rmd-->

