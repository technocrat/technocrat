# The subprime mortgage crisis unfolds in early 2007, part 2

Keywords: refactoring, principal component analysis

## The fall of FICO

The problem with the conventional wisdom of long standing is that it loses sight of history. The prominence of FICO in home loan credit underwriting described in the [Pinto Testimony] had its origins in a different time (the early 1990s) and a different lending environment. Freddie Mac was in a good position to ensure that all other things *were* equal. It made only what came to be called "prime" loans, generally for no more than 80% of the value of the property, under more stringent limitations on the debt-to-income ratio of the borrower and many other criteria that it kept within a narrow range, and offered only a few varieties of loans.

In the subprime market that emerged in the late 90s, all of those factors changed. Criteria that were narrow became broad, documentation was relaxed and a widespread assumption was that continually rising home values would preclude any problems. It's not surprising that FICO lost its predictive power.

## Restructuring the data

Some of the testing of FICO as a useful metric involved subsetting the data. There were many more variables than the ones used, some of them categorical and some categorical coded as numeric. One potentially useful variable is location, because we know that real estate value are location sensitive. We have four location fields in the database, all derived from the postal zip code:

* The zip code itself, which is generally either much smaller or much larger than the real estate market, and also changes at the convenience of the postal service. *See* the discussion at [On the use of ZIP codes and ZIP code tabulation areas (ZCTAs) for the spatial analysis of epidemiological data].

* The metropolitan area derived from the U.S. Census ZIP code tabulation area, but covers a larger area than most real estate markets

* Longitude and latitude dervived from the ZCTA's, used for mapping

As a compromise, I converted the 5-digit zip codes into 3-digit zip codes. In metropolitan areas, the 3-digit codes are the sizes comparable to how the multiple listing services divide the market. We'll see if there is any value in this proxy measure of real estate market.

```{r, results='hide', echo = FALSE, warning=FALSE}
library(tidyverse)
library(DBI)
library(RMySQL)
drv <- dbDriver("MySQL")
con <- dbConnect(drv, username="root", password="", dbname ="dlf", host="localhost")
res <- dbGetQuery(con, "SELECT ctapeno, deal, remit, fico, dti, cltv, orate, obal grade, round(zip/100,0), dtype, fpd,ltype, pmiflag, ppp, otype, purpose, ptype FROM y6c")
cs <- as.tibble(res)
require(binaryLogic)
require(stringi)
reports <- cs %>% rowwise %>% mutate(repbin = toString(as.binary(remit))) %>% rowwise %>% mutate(repstr = str_replace_all(repbin,"[, ]",'')) %>% mutate(reports = str_count(repstr)) %>% select(ctapeno, repstr, reports)
perf <- reports %>% mutate(category = case_when(str_detect(repstr, '111$') ~ "C", str_count(repstr, '0') <= 9 ~ "B", str_count(repstr, '0') > 9 ~ "A")) %>% select(ctapeno, category)
y6rf <- cs %>% inner_join(perf, by = "ctapeno")
#y6rf = y6 refactored
y6rf <- y6rf %>% mutate(zip = as.character(`round(zip/100,0)`), pmiflag = as.character(pmiflag), ppp = as.character(ppp), perf = category, grade = as.character((grade)))
y6rf <- y6rf %>% select(ctapeno, deal,fico,dti,cltv,orate, grade, dtype,fpd,ltype, pmiflag, ppp,  otype, purpose, ptype, zip, perf)
# dbWriteTable(con, "loans", y6rf) 
# test
# res <- dbGetQuery(con, "SELECT * from loans limit 25") passed
``` 

It was time to reorganize the database into a more streamlined version, that captured the information on performance (relieving the 11-month constraint) and transformed the fields that needed to be treated as categorical, rather than continuous. It's much more efficient to put this in a new SQL table than to keep in memory, especially since sampling will be involved. Here's the revised data layout:


    MariaDB [dlf]> describe loans;
    +-----------+--------+------+-----+---------+-------+
    | Field     | Type   | Null | Key | Default | Extra |
    +-----------+--------+------+-----+---------+-------+
    | row_names | text   | YES  |     | NULL    |       |
    | ctapeno   | double | YES  |     | NULL    |       |
    | deal      | text   | YES  |     | NULL    |       |
    | fico      | double | YES  |     | NULL    |       |
    | dti       | double | YES  |     | NULL    |       |
    | cltv      | double | YES  |     | NULL    |       |
    | orate     | double | YES  |     | NULL    |       |
    | grade     | text   | YES  |     | NULL    |       |
    | dtype     | text   | YES  |     | NULL    |       |
    | fpd       | text   | YES  |     | NULL    |       |
    | ltype     | text   | YES  |     | NULL    |       |
    | pmiflag   | text   | YES  |     | NULL    |       |
    | ppp       | text   | YES  |     | NULL    |       |
    | otype     | text   | YES  |     | NULL    |       |
    | purpose   | text   | YES  |     | NULL    |       |
    | ptype     | text   | YES  |     | NULL    |       |
    | zip       | text   | YES  |     | NULL    |       |
    | perf      | text   | YES  |     | NULL    |       |
    +-----------+--------+------+-----+---------+-------+
    18 rows in set (0.00 sec)

    
Between the first row (a record identifier) and the last row (the performance category) are the sixteen variables we have to predict the performance outcome. For the almost 100,000 records, that is 1.5 million pieces of information. Technically we are in 16-dimensional space, and we need a way of flattening the dimensionality to be able to question the data.


[On the use of ZIP codes and ZIP code tabulation areas (ZCTAs) for the spatial analysis of epidemiological data]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1762013/


[Pinto testimony]: https://democrats-oversight.house.gov/sites/democrats.oversight.house.gov/files/documents/Fannie%20Freddie%20Testimony%20of%20Edward%20Pinto%2012.9.08%20written%20submission%20Full.pdf

