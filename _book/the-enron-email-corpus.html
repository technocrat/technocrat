<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Data Science Portfolio</title>
  <meta name="description" content="Landing site">
  <meta name="generator" content="bookdown 0.7.10 and GitBook 2.6.7">

  <meta property="og:title" content="Data Science Portfolio" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Landing site" />
  <meta name="github-repo" content="github/technocrat/technocrat.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Data Science Portfolio" />
  
  <meta name="twitter:description" content="Landing site" />
  

<meta name="author" content="Richard Careaga">


<meta name="date" content="2018-07-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="daycare.html">
<link rel="next" href="counted.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/kePrint-0.0.1/kePrint.js"></script>



</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Purpose</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#updates"><i class="fa fa-check"></i><b>1.1</b> Updates</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#literate-programming-the-tight-integration-of-code-and-text"><i class="fa fa-check"></i><b>1.2</b> Literate programming: the tight integration of code and text</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#background"><i class="fa fa-check"></i><b>1.3</b> Background</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#the-cases"><i class="fa fa-check"></i><b>1.4</b> The Cases</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#current-credentials"><i class="fa fa-check"></i><b>1.5</b> Current Credentials</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#graduate-school-level"><i class="fa fa-check"></i><b>1.5.1</b> Graduate school level</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#undergraduate-level"><i class="fa fa-check"></i><b>1.5.2</b> Undergraduate level</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#prior-analytic-and-programming-experience"><i class="fa fa-check"></i><b>1.5.3</b> Prior analytic and programming experience</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="dectab.html"><a href="dectab.html"><i class="fa fa-check"></i><b>2</b> The Financial Cash Flow Model: Python on Wall Street</a><ul>
<li class="chapter" data-level="2.1" data-path="dectab.html"><a href="dectab.html#case-description"><i class="fa fa-check"></i><b>2.1</b> Case Description</a></li>
<li class="chapter" data-level="2.2" data-path="dectab.html"><a href="dectab.html#xml-conversion"><i class="fa fa-check"></i><b>2.2</b> XML Conversion</a></li>
<li class="chapter" data-level="2.3" data-path="dectab.html"><a href="dectab.html#from-xml-to-plain-text"><i class="fa fa-check"></i><b>2.3</b> From XML to Plain Text</a></li>
<li class="chapter" data-level="2.4" data-path="dectab.html"><a href="dectab.html#proof-of-concept-summary-fico-statistics"><i class="fa fa-check"></i><b>2.4</b> Proof of Concept, Summary FICO Statistics</a></li>
<li class="chapter" data-level="2.5" data-path="dectab.html"><a href="dectab.html#the-cash-flow-model"><i class="fa fa-check"></i><b>2.5</b> The Cash Flow Model</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><i class="fa fa-check"></i><b>3</b> The subprime mortgage crisis unfolds in early 2007, part 1</a><ul>
<li class="chapter" data-level="3.1" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html#case-description-1"><i class="fa fa-check"></i><b>3.1</b> Case Description</a></li>
<li class="chapter" data-level="3.2" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html#data-wrangling"><i class="fa fa-check"></i><b>3.2</b> Data Wrangling</a></li>
<li class="chapter" data-level="3.3" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html#database-conversion"><i class="fa fa-check"></i><b>3.3</b> Database conversion</a></li>
<li class="chapter" data-level="3.4" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html#preliminary-analysis-based-on-fico-scores"><i class="fa fa-check"></i><b>3.4</b> Preliminary analysis, based on FICO scores</a><ul>
<li class="chapter" data-level="3.4.1" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html#ficos-importance"><i class="fa fa-check"></i><b>3.4.1</b> FICO’s importance</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html#fico-scores-in-the-2006-loan-pool"><i class="fa fa-check"></i><b>3.4.2</b> FICO scores in the 2006 loan pool</a></li>
<li class="chapter" data-level="3.4.3" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html#the-fico-scores-are-not-normally-distributed"><i class="fa fa-check"></i><b>3.4.3</b> The FICO scores are not normally distributed</a></li>
<li class="chapter" data-level="3.4.4" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html#possible-strategies-to-deal-with-the-issues"><i class="fa fa-check"></i><b>3.4.4</b> Possible strategies to deal with the issues</a></li>
<li class="chapter" data-level="3.4.5" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html#reduced-data-set"><i class="fa fa-check"></i><b>3.4.5</b> Reduced data set</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-1.html#results-at-this-point"><i class="fa fa-check"></i><b>3.5</b> Results at this point</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html"><i class="fa fa-check"></i><b>4</b> The subprime mortgage crisis unfolds in early 2007, part 2</a><ul>
<li class="chapter" data-level="4.1" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html#the-fall-of-fico"><i class="fa fa-check"></i><b>4.1</b> The fall of FICO</a></li>
<li class="chapter" data-level="4.2" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html#restructuring-the-data"><i class="fa fa-check"></i><b>4.2</b> Restructuring the data</a></li>
<li class="chapter" data-level="4.3" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html#principal-component-analysis"><i class="fa fa-check"></i><b>4.3</b> Principal component analysis</a></li>
<li class="chapter" data-level="4.4" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html#k-means-clustering"><i class="fa fa-check"></i><b>4.4</b> K-means clustering</a><ul>
<li class="chapter" data-level="4.4.1" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html#housekeeping"><i class="fa fa-check"></i><b>4.4.1</b> Housekeeping</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html#do-the-clusters-predict-loan-performance"><i class="fa fa-check"></i><b>4.5</b> Do the clusters predict loan performance?</a><ul>
<li class="chapter" data-level="4.5.1" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html#correspondence-analysis"><i class="fa fa-check"></i><b>4.5.1</b> Correspondence analysis</a></li>
<li class="chapter" data-level="4.5.2" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html#what-do-other-categorical-contingency-tables-tell-us"><i class="fa fa-check"></i><b>4.5.2</b> What do other categorical contingency tables tell us?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-3.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-3.html"><i class="fa fa-check"></i><b>5</b> The subprime mortgage crisis unfolds in early 2007, part 3</a><ul>
<li class="chapter" data-level="5.1" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-3.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-3.html#selection-bias"><i class="fa fa-check"></i><b>5.1</b> Selection bias</a><ul>
<li class="chapter" data-level="5.1.1" data-path="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-3.html"><a href="the-subprime-mortgage-crisis-unfolds-in-early-2007-part-3.html#fico-the-light-that-failed"><i class="fa fa-check"></i><b>5.1.1</b> FICO, the light that failed</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="a-tibble-96145-x-2.html"><a href="a-tibble-96145-x-2.html"><i class="fa fa-check"></i><b>6</b> A tibble: 96,145 x 2</a></li>
<li class="chapter" data-level="7" data-path="daycare.html"><a href="daycare.html"><i class="fa fa-check"></i><b>7</b> Daycare Costs and Unexamined Assumptions</a><ul>
<li class="chapter" data-level="7.1" data-path="daycare.html"><a href="daycare.html#case-description-2"><i class="fa fa-check"></i><b>7.1</b> Case Description</a></li>
<li class="chapter" data-level="7.2" data-path="daycare.html"><a href="daycare.html#with-the-right-metrics"><i class="fa fa-check"></i><b>7.2</b> With the right metrics</a></li>
<li class="chapter" data-level="7.3" data-path="daycare.html"><a href="daycare.html#takeaway"><i class="fa fa-check"></i><b>7.3</b> Takeaway</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html"><i class="fa fa-check"></i><b>8</b> The Enron Email Corpus</a><ul>
<li class="chapter" data-level="8.1" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#case-description-3"><i class="fa fa-check"></i><b>8.1</b> Case Description</a></li>
<li class="chapter" data-level="8.2" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#data-preparation"><i class="fa fa-check"></i><b>8.2</b> Data preparation</a></li>
<li class="chapter" data-level="8.3" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#strategy-for-exploration"><i class="fa fa-check"></i><b>8.3</b> Strategy for exploration</a><ul>
<li class="chapter" data-level="8.3.1" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#dont-look-for-much-from-the-big-shots"><i class="fa fa-check"></i><b>8.3.1</b> Don’t look for much from the big shots</a></li>
<li class="chapter" data-level="8.3.2" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#volume-is-not-evenly-distributed-among-users"><i class="fa fa-check"></i><b>8.3.2</b> Volume is not evenly distributed among users</a></li>
<li class="chapter" data-level="8.3.3" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#keywords-may-not-help-much"><i class="fa fa-check"></i><b>8.3.3</b> Keywords may not help much</a></li>
<li class="chapter" data-level="8.3.4" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#dont-neglect-time-series"><i class="fa fa-check"></i><b>8.3.4</b> Don’t neglect time series</a></li>
<li class="chapter" data-level="8.3.5" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#avoid-the-echo-chamber"><i class="fa fa-check"></i><b>8.3.5</b> Avoid the echo chamber</a></li>
<li class="chapter" data-level="8.3.6" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#the-real-value-proposition"><i class="fa fa-check"></i><b>8.3.6</b> The REAL value proposition</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#some-preliminary-results"><i class="fa fa-check"></i><b>8.4</b> Some preliminary results</a><ul>
<li class="chapter" data-level="8.4.1" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#unique-senders"><i class="fa fa-check"></i><b>8.4.1</b> Unique senders</a></li>
<li class="chapter" data-level="8.4.2" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#unique-receivers"><i class="fa fa-check"></i><b>8.4.2</b> Unique receivers</a></li>
<li class="chapter" data-level="8.4.3" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#senders-who-are-also-receivers"><i class="fa fa-check"></i><b>8.4.3</b> Senders who are also receivers</a></li>
<li class="chapter" data-level="8.4.4" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#senderreceivers-with-enron-addresses"><i class="fa fa-check"></i><b>8.4.4</b> Sender/receivers with Enron addresses</a></li>
<li class="chapter" data-level="8.4.5" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#subject-line-words"><i class="fa fa-check"></i><b>8.4.5</b> Subject line words</a></li>
<li class="chapter" data-level="8.4.6" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#places-mentioned"><i class="fa fa-check"></i><b>8.4.6</b> Places mentioned</a></li>
<li class="chapter" data-level="8.4.7" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#periodicity"><i class="fa fa-check"></i><b>8.4.7</b> Periodicity</a></li>
<li class="chapter" data-level="8.4.8" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#social-networks"><i class="fa fa-check"></i><b>8.4.8</b> Social Networks</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="the-enron-email-corpus.html"><a href="the-enron-email-corpus.html#future-work"><i class="fa fa-check"></i><b>8.5</b> Future work</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="counted.html"><a href="counted.html"><i class="fa fa-check"></i><b>9</b> 2015 Police Involved Homicides</a><ul>
<li class="chapter" data-level="9.1" data-path="counted.html"><a href="counted.html#introduction"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="counted.html"><a href="counted.html#the-data"><i class="fa fa-check"></i><b>9.2</b> The Data</a></li>
<li class="chapter" data-level="9.3" data-path="counted.html"><a href="counted.html#tabular-summary-of-the-data"><i class="fa fa-check"></i><b>9.3</b> Tabular summary of the data</a><ul>
<li class="chapter" data-level="9.3.1" data-path="counted.html"><a href="counted.html#gender"><i class="fa fa-check"></i><b>9.3.1</b> Gender</a></li>
<li class="chapter" data-level="9.3.2" data-path="counted.html"><a href="counted.html#raceethnicity"><i class="fa fa-check"></i><b>9.3.2</b> Race/Ethnicity</a></li>
<li class="chapter" data-level="9.3.3" data-path="counted.html"><a href="counted.html#age"><i class="fa fa-check"></i><b>9.3.3</b> Age</a></li>
<li class="chapter" data-level="9.3.4" data-path="counted.html"><a href="counted.html#cause-of-death"><i class="fa fa-check"></i><b>9.3.4</b> Cause of Death</a></li>
<li class="chapter" data-level="9.3.5" data-path="counted.html"><a href="counted.html#whether-and-how-civilians-were-armed"><i class="fa fa-check"></i><b>9.3.5</b> Whether and how civilians were armed</a></li>
<li class="chapter" data-level="9.3.6" data-path="counted.html"><a href="counted.html#location"><i class="fa fa-check"></i><b>9.3.6</b> Location</a></li>
<li class="chapter" data-level="9.3.7" data-path="counted.html"><a href="counted.html#months-and-days-of-death"><i class="fa fa-check"></i><b>9.3.7</b> Months and Days of Death</a></li>
<li class="chapter" data-level="9.3.8" data-path="counted.html"><a href="counted.html#cause-of-death-and-civilian-armed-status"><i class="fa fa-check"></i><b>9.3.8</b> Cause of Death and Civilian Armed Status</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="counted.html"><a href="counted.html#geographic-analysis-of-the-data"><i class="fa fa-check"></i><b>9.4</b> Geographic Analysis of the Data</a><ul>
<li class="chapter" data-level="9.4.1" data-path="counted.html"><a href="counted.html#many-datasets-are-distributed-similarly-to-population"><i class="fa fa-check"></i><b>9.4.1</b> Many Datasets are Distributed Similarly to Population</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="counted.html"><a href="counted.html#what-conclusions-can-we-draw-from-the-short-national-summary"><i class="fa fa-check"></i><b>9.5</b> What conclusions can we draw from the short, national summary?</a><ul>
<li class="chapter" data-level="9.5.1" data-path="counted.html"><a href="counted.html#detail-underlying-deaths-of-women"><i class="fa fa-check"></i><b>9.5.1</b> Detail underlying deaths of women</a></li>
<li class="chapter" data-level="9.5.2" data-path="counted.html"><a href="counted.html#cautionary-example"><i class="fa fa-check"></i><b>9.5.2</b> Cautionary Example</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="counted.html"><a href="counted.html#next-steps"><i class="fa fa-check"></i><b>9.6</b> Next steps</a><ul>
<li class="chapter" data-level="9.6.1" data-path="counted.html"><a href="counted.html#finer-geographic-detail"><i class="fa fa-check"></i><b>9.6.1</b> Finer geographic detail</a></li>
<li class="chapter" data-level="9.6.2" data-path="counted.html"><a href="counted.html#testing-hypotheses"><i class="fa fa-check"></i><b>9.6.2</b> Testing hypotheses</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html"><i class="fa fa-check"></i><b>10</b> Assorted Examples of Toolmaking</a><ul>
<li class="chapter" data-level="10.1" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html#diy-r-packages"><i class="fa fa-check"></i><b>10.1</b> DIY R Packages</a></li>
<li class="chapter" data-level="10.2" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html#the-short-genome-file-day-to-day-python"><i class="fa fa-check"></i><b>10.2</b> The Short Genome File: Day-to-Day Python</a><ul>
<li class="chapter" data-level="10.2.1" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html#case-description-4"><i class="fa fa-check"></i><b>10.2.1</b> Case Description</a></li>
<li class="chapter" data-level="10.2.2" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html#conversion-problem"><i class="fa fa-check"></i><b>10.2.2</b> Conversion Problem</a></li>
<li class="chapter" data-level="10.2.3" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html#everyday-solution"><i class="fa fa-check"></i><b>10.2.3</b> Everyday Solution</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html#hard-but-simple-parsing-yaml-files-in-haskell"><i class="fa fa-check"></i><b>10.3</b> Hard but simple – parsing YAML files in Haskell</a></li>
<li class="chapter" data-level="10.4" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html#contextual-awareness"><i class="fa fa-check"></i><b>10.4</b> Contextual Awareness</a></li>
<li class="chapter" data-level="10.5" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html#minimalism-throws-you-into-the-pool"><i class="fa fa-check"></i><b>10.5</b> Minimalism Throws You into the Pool</a></li>
<li class="chapter" data-level="10.6" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html#parsing-system-date-strings-into-python-datetime-objects"><i class="fa fa-check"></i><b>10.6</b> Parsing system date strings into Python datetime objects</a></li>
<li class="chapter" data-level="10.7" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html#combination-of-k-items-taken-n-at-a-time"><i class="fa fa-check"></i><b>10.7</b> Combination of k items, taken n at a time</a></li>
<li class="chapter" data-level="10.8" data-path="assorted-examples-of-toolmaking.html"><a href="assorted-examples-of-toolmaking.html#flexbison-to-compile-data-parser-for-june-20-2018-form"><i class="fa fa-check"></i><b>10.8</b> Flex/Bison to compile data parser for June 20, 2018 form</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science Portfolio</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-enron-email-corpus" class="section level1">
<h1><span class="header-section-number">8</span> The Enron Email Corpus</h1>
<div id="case-description-3" class="section level2">
<h2><span class="header-section-number">8.1</span> Case Description</h2>
<p><a href="" title="https://en.wikipedia.org/wiki/Enron">Enron Corporation</a> was a $100 billion annual revenue company:</p>
<ol style="list-style-type: decimal">
<li>They were in the gas and electricity business, mainly as traders, rather than as a utility</li>
<li>California had an auction process for electricity that Enron was manipulating</li>
<li>Enron used special purpose entities in a way that hid its financial condition</li>
<li>There was a special purpose entity used for a deal involving barges in Nigeria</li>
<li>Several individuals, including the CEO and his deputy were prosecuted</li>
<li>Many employees lost their retirement savings when Enron stock became worthless</li>
</ol>
<p>The Federal Energy Regulatory Commission (<strong>FERC</strong>) investigated Enron’s activities in the western U.S. wholesale electricity market for evidence of price manipulation and other violations. It obtained approximately 500,000 copies of emails from 149 email users. Copies of these were acquired by Leslie Kaelbling of MIT and published by William W. Cohen of Carnegie Mellon University. It is one of the largest publicly available datasets of corporate email and is referred to as the <a href="" title="https://www.cs.cmu.edu/~enron/">Enron Corpus</a>. The term <em>corpus</em> is used in natural language processing to denote a collection of related text.</p>
<p>Civil and criminal litigation of other cases is conducted either by commercial or proprietary software. Much of the focus is directed to keyword searches and depends on visual scanning of emails by attorneys. Email examination can be a substantial expense.</p>
<p>Although “smoking gun” emails may be found, brute force examination misses opportunities to understand the social networks that reflect how the organization operates, what their concerns are and which part of the corpus should receive priority. To do that the corpus must be distilled.</p>
</div>
<div id="data-preparation" class="section level2">
<h2><span class="header-section-number">8.2</span> Data preparation</h2>
<p>The data was provided in the form of a directory tree of text copies of emails in the file folders of the <strong>custodians</strong> (users), rather than in “native” format. The version of the <a href="" title="https://www.cs.cmu.edu/~enron/">Enron Corpus</a> that I used is dated August 21, 2009.</p>
<p>The directory tree of one of the users is representative:</p>
<div class="figure">
<img src="https://s3-us-west-2.amazonaws.com/tuva/DirTree.jpg" alt="An email folder" />
<p class="caption">An email folder</p>
</div>
<p>Each of these files is plain text and contains the following types of data:</p>
<p><img src="https://s3-us-west-2.amazonaws.com/tuva/parse_email.png" alt="Parts of an email" /> ### Preparation</p>
<p>Because the same message body resides in multiple folders of multiple custodians, some way was needed to de-duplicate.</p>
<p>The method differed dependant on whether the email was originated on the IBM Notes system or Microsoft Outlook. In either case, however, it consisted traversing the directoryl tree and extracting the same fields</p>
<ul>
<li>Sender</li>
<li>Date</li>
<li>Receiver(s)</li>
<li>cc(s)</li>
<li>Message body</li>
<li>file-name</li>
</ul>
<p>and, in addition, adding a digital signature (MD5 digest) to the message body to nearly guarantee its uniqueness. (The body is the content of the originating email strip of meta-data, legends and disclaimers, and replies and replies to replies. It is the payload.)</p>
<p>This was done by traditional Unix command line tools, perl and Python scripts and other tools to create a database with the following structure:</p>
<pre><code>+----------+--------------+------+-----+---------+-------+
| Field    | Type         | Null | Key | Default | Extra |
+----------+--------------+------+-----+---------+-------+
| body     | mediumtext   | YES  |     | NULL    |       |
| lastword | mediumtext   | YES  |     | NULL    |       |
| hash     | varchar(250) | YES  | UNI | NULL    |       |
| sender   | varchar(250) | YES  |     | NULL    |       |
| tos      | text         | YES  |     | NULL    |       |
| mid      | varchar(250) | YES  |     | NULL    |       |
| ccs      | text         | YES  |     | NULL    |       |
| date     | datetime     | YES  |     | NULL    |       |
| subj     | varchar(500) | YES  |     | NULL    |       |
| tosctn   | mediumint(9) | YES  |     | NULL    |       |
| ccsctn   | mediumint(9) | YES  |     | NULL    |       |
| source   | varchar(250) | YES  |     | NULL    |       |
+----------+--------------+------+-----+---------+-------+</code></pre>
<p>Of the approxiately 500,000 emails in the <a href="" title="https://www.cs.cmu.edu/~enron/">Enron corpus</a>, approximately half are duplicates. A large part of the remainder consists of newsletters, bulletins, fantasy football matters and other emails addressed to a large audience with only one or a few Enron recipients. Say, 125,000. The remaining 125,000 have roughly 75,000 addressed to large groups (“be advised that the Houston gym hours will be changing”) or to individuals on routine matters (“your approval for expense report #1234 is overdue”). Another 25,000 deal with scheduling of meetings, transmission of periodic reports, and circulation of form documents covering derivative trading with counterparties. A cull list of senders and topics was developed to extract these.</p>
<p>Of the remaining emails, approximately 15,000 involve correspondence from a sender to a custodian who never sends a reply or an original email to the sender. This leaves about 35,000 emails among senders and receivers who engage in some degree of reciprocal correspondence. This is where to begin. Insights from this group can be used to recycle over the discards.</p>
</div>
<div id="strategy-for-exploration" class="section level2">
<h2><span class="header-section-number">8.3</span> Strategy for exploration</h2>
<div id="dont-look-for-much-from-the-big-shots" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Don’t look for much from the big shots</h3>
<p>CEO Ken Lays’s administrative assistant handled much of his email, CFO Andy Fastow’s email is not included. COO Jeff Skilling is included, but his volume is small. On the other hand, some of the largest senders are relatively low ranking, a legal assistant distributing documents and a lobbyist in California.</p>
</div>
<div id="volume-is-not-evenly-distributed-among-users" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Volume is not evenly distributed among users</h3>
<div class="figure">
<img src="http://media.richard-careaga.com/img/BigSenders.png" alt="Number of emails by sender" />
<p class="caption">Number of emails by sender</p>
</div>
</div>
<div id="keywords-may-not-help-much" class="section level3">
<h3><span class="header-section-number">8.3.3</span> Keywords may not help much</h3>
<p>Natural language processing (<strong>NLP</strong>) approaches based on written composition are of little help in the misspelled, ungrammatical, freeflowing, implicit meaning-rich world of email. Reviewing email is much more like eavesdropping than reading.</p>
<p>First, it is essential to have a well-thought out file of stop words to eliminate the most common words, which tend to be “glue” words. Second, the business of Enron was trading, and the tool of trading is and was the Bloomberg terminal with its instant messaging feature. The conduct of a trading floor, more than most other large corporate enterprises, is effectuated face-to-face and by telephone. Don’t look for meeting agendas and minutes.</p>
<p>Every organization has a vocabulary profile that is unique to its business. To develop candidate lists for Enron, I used the following NLP program:</p>
<pre><code>#!/usr/bin/env python
# encoding: utf-8
&quot;&quot;&quot;
oddfreq.py: word frequency list, disregarding capitalization, excluding stopwords
and words _not_ in standard dictionary, sorted by frequency

Created on 2010-04-15
Richard Careaga
&quot;&quot;&quot;
from itertools import izip, chain, repeat
from Prep import *
from util import *
import nltk
from nltk import FreqDist
from nltk.corpus import stopwords

# Natural Language Toolkit: unusual_words
# from nltk and used by permission
def unusual_words(text):
    text_vocab = set(w.lower() for w in text if w.lower().isalpha())
    english_vocab = set(w.lower() for w in nltk.corpus.words.words())
    unusual = text_vocab.difference(english_vocab)
    return sorted(unusual)

# Natural Language Toolkit: code_plural
# from nltk and used by permission
def plural(word):
    if word.endswith(&#39;y&#39;):
        return word[:-1] + &#39;ies&#39;
    elif word[-1] in &#39;sx&#39; or word[-2:] in [&#39;sh&#39;, &#39;ch&#39;]:
        return word + &#39;es&#39;
    elif word.endswith(&#39;an&#39;):
        return word[:-2] + &#39;en&#39;
    else:
        return word + &#39;s&#39;

def main():
    # query to select new topic email subject lines sent to single recipients
    sql =   sql = &quot;&quot;&quot;SELECT date, subject FROM scrub where subject not \
            regexp &#39;RE:|Re:|re:|FW:|Fw:|fw:|FWD:|Fwd:|fwd:&#39; and twoway is true \
            and tosctn = 1 and ccsctn = 0 and chaff is false&quot;&quot;&quot;
    # create an object
    C = Prepare()
    # do the necessary processing to return an nltk Text object
    X = C.prep(sql)
    # collect list of common words, such as prepositions, articles, etc.
    stops = stopwords.words(&#39;english&#39;)
    # extract word list after converting to all lowercase
    words = [w.lower() for w in X if w.isalpha() and w.lower() not in stops]
    # extract unique words
    vocab = set(words)
    # fetch a standard English vocabular
    english_vocab = set(w.lower() for w in nltk.corpus.words.words())
    # convert to a list for pluralization
    english = list(english_vocab)
    # make a list of &quot;plural-like&quot; words
    plurals = [plural(w) for w in english]
    # cobmine lists
    englishes = english + plurals
    # make a set of the list
    englishes_vocab = set(englishes)
    # compare to the vocabulary extracted from the emails
    oddball = vocab.difference(englishes_vocab)
    # convert to a list
    enronic = list(oddball)
    # sort the list
    enronic.sort()
    # create a new list of the text _without__  those words
    words[:] = [w for w in words if w not in enronic]
    # recycle the original vocabulary definition
    # this allows the remainder of the code to be reused as is
    vocab = set(words)
    # prepare dictionary of word frequencies
    fdist = FreqDist(words)
    # collect the unique words for sorting
    alphalist = list(vocab)
    # sort the unique words
    alphalist.sort()
    # create a list to hold results
    LA = []
    # collect key:item pairs for word:frequency in alphabetical order
    for item in alphalist:
       bag = []
       bag.append(item)
       bag.append(fdist[item])
       LA.append(bag)
    # calculate width of word field and add one
    width = find_max_width(LA,0)+1
    # calculate width of frequency field and add one
    numwidth = find_max_width(LA,1)+1
    # assign a desired pagewidth
    pagewidth = 85
    # assign desired margins
    lmargin = 5
    rmargin = 5
    margins = lmargin + rmargin
    # calculate column width
    columnwidth = width + numwidth
    # calculate whole number of columns that will fit (floor division)
    columns = (pagewidth-margins)//columnwidth
    # assign a gutter width
    gutter = 4*&#39; &#39; # 4 spaces
    # calculate available gutter
    copywidth = columnwidth*columns + len(gutter)
    # assign desired pagelength
    pagelength = 54
    # open a file for output
    f = open(&quot;enronic.txt&quot;, &#39;w&#39;)
    # convenience definition to append a newline
    nl = &#39;\n&#39;
    # generate an iterator object to fetch 54 lines of the word frequency
    # list at a time
    g = grouper(pagelength,LA)
    # iterate through the object until exhausted
    while g:
        # write a ruler and newline
        f.write(&quot;=&quot;*copywidth)
        f.write(nl)
        # chunk first two batches of pagelength lines side by side
        z = zip(g.next(),g.next())
        # print each pair
        for row in z:
            stringline = (&quot;%s%s%s%s%s&quot;) % (repr(row[0][0]).ljust(width),\
            repr(row[0][1]).rjust(numwidth), gutter, \
            repr(row[1][0]).ljust(width),\
            repr(row[1][1]).rjust(numwidth))
            f.write(stringline)
            f.write(nl)

    f.close()

if __name__ == &#39;__main__&#39;:
    main()</code></pre>
<p>which produced pages like the following (here limited to words with 10-99 occurrences):</p>
<pre><code>aec                        17    aloha                       1
aep                        19    alport                      1
ag                         18    amerada                     6
agl                         3    amerex                     22
anahiem                     1    aron                       11
anglo                       1    asap                       14
apb                        90    atoka                       1
api                        10    attaching                   1
approved                   29    attending                   1
australia                  15    bayer                       3
autoreply                 220    bball                       1
bandwidth                  14    bennett                     4
barnett                    20    berney                      1
bge                         3    bp                         16
bgml                        1    bpa                        16
bingaman                    1    brazos                     15
bio                        10    breakeven                   1
blackline                   6    bridgeline                 17
bloomberg                  14    brl                         1
bmo                         1    broadband                  11
bnp                        18    bros                        1
boise                       1    bruce                      10
byler                       1    cargill                    40
caiso                      18    carolyn                     1
calgary                    14    cartersville                1
calif                      11    cashion                     1
calley                      1    catalytica                 12
calpine                    47    ccf                         1
caminus                     4    cdwr                       13
canceled                   15    cementos                    1
cancun                      2    centana                    14
ceo                        11    cibc                        3
cera                       24    cif                         1
cfd                         1    cinergy                    21
cftc                       17    cipico                      1
cgas                       16    cirino                      1
changed                    16    clair                       6
checking                    3    clickathome                23
checklist                  13    clicking                    7
checkosut                   1    clickpaper                 40
checkout                  160    clifford                    3
chemconnect                 1    closing                    23
chicago                    24    cmr                         1
chilkina                    1    cms                        10
cng                        14    confer                      1
cogen                      10    confirmlogic                4
coi                         1    congrats                   18
coleman                    10    congratulotions             1
colstrip                    1    conoco                     10
completed                  10    corhshucker                 1
conf                       14    countdown                   1
confederated                3    counterparties             45
counterparty               64    curtis                      2
cp                         12    cuves                       1
cps                        14    cysive                      1
cpuc                       25    dabacle                     2
cpy                         2    dabhol                     18</code></pre>
</div>
<div id="dont-neglect-time-series" class="section level3">
<h3><span class="header-section-number">8.3.4</span> Don’t neglect time series</h3>
<p>In looking at a subset, pay attention to how volume varies with time. As others have noted, a sudden drop in email volume within the senior group, such as occurred in May 2001, can indicate a situation in which decisionmakers are meeting personally. When trouble looms, people clam up.</p>
<p>The traffic in 2001 suggests lines of inquiry focused on specific periods.</p>
<div class="figure">
<img src="http://media.richard-careaga.com/img/emails2001.png" alt="May gap" />
<p class="caption">May gap</p>
</div>
</div>
<div id="avoid-the-echo-chamber" class="section level3">
<h3><span class="header-section-number">8.3.5</span> Avoid the echo chamber</h3>
<p>Email streams in which an initial email goes out to a group with responses coming back quoting the original email, forwards and reforwards and responses, amplify whatever NLP content can be gleaned. Parse each email to determine its original content load and delete that from replies and forwards, as well as standard disclaimers.</p>
</div>
<div id="the-real-value-proposition" class="section level3">
<h3><span class="header-section-number">8.3.6</span> The REAL value proposition</h3>
<p>The purpose of examining a huge body of email is not to find a smoking gun. It is to understand the business, its language and its people. While you may sometimes run across braggadocio crowing over putting one over, those are rare and seldom determinative.</p>
</div>
</div>
<div id="some-preliminary-results" class="section level2">
<h2><span class="header-section-number">8.4</span> Some preliminary results</h2>
<div id="unique-senders" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Unique senders</h3>
<pre><code> mysql&gt; select count(*) from usenders;
 +----------+
 | count(*) |
 +----------+
 |    17568 |
 +----------+</code></pre>
</div>
<div id="unique-receivers" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Unique receivers</h3>
<pre><code> mysql&gt; select count(*) from ureceivers;
 +----------+
 | count(*) |
 +----------+
 |    68199 |
 +----------+</code></pre>
</div>
<div id="senders-who-are-also-receivers" class="section level3">
<h3><span class="header-section-number">8.4.3</span> Senders who are also receivers</h3>
<pre><code>mysql&gt; select count(*) from twoway;
+----------+
| count(*) |
+----------+
|    10235 |
+----------+</code></pre>
</div>
<div id="senderreceivers-with-enron-addresses" class="section level3">
<h3><span class="header-section-number">8.4.4</span> Sender/receivers with Enron addresses</h3>
<pre><code>mysql&gt; select count(*) from insiders;
+----------+
| count(*) |
+----------+
|     6099 |
+----------+</code></pre>
</div>
<div id="subject-line-words" class="section level3">
<h3><span class="header-section-number">8.4.5</span> Subject line words</h3>
<pre><code>mysql&gt; select count(*) from wordlist;
+----------+
| count(*) |
+----------+
|   141180 |
+----------+</code></pre>
<p>Two of the most common words are <em>power</em> and <em>energy</em>. There seems to be a difference, however, in how different senders tend toward one or another</p>
<div class="figure">
<img src="http://media.richard-careaga.com/img/joule.png" alt="Power vs. Energy" />
<p class="caption">Power vs. Energy</p>
</div>
<p>and the time distribution differs</p>
<div class="figure">
<img src="http://media.richard-careaga.com/img/energypower.png" alt="Power vs. Energy over time" />
<p class="caption">Power vs. Energy over time</p>
</div>
</div>
<div id="places-mentioned" class="section level3">
<h3><span class="header-section-number">8.4.6</span> Places mentioned</h3>
<blockquote>
<p>Abu Accra Addis Agra Ak Akron Al Almaty Amman Andorra Angola Ankara Ar Aruba Ashmore Astana Atoll Az Baghdad Bahamas Bahrain Baker Bakersfield Baku Balkans Baltimore Bandar Bangalore Bangkok Bangladesh Barbados Barbuda Barcelona Barranquilla Barthelemy Beaumont Beijing Belarus Belgium Belgrade Belize Bellevue Belo Benin Berkeley Berlin Bermuda Bernardino Bhopal Birmingham Bogota Boise Bolivia Bonn Bosnia Boston Botswana Brasilia Brazil Bremen Bridgeport Brisbane Bristol Britain British Brownsville Brunei Brussels Bucharest Budapest Buenos Buffalo Bulgaria Burbank Burma Bursa Burundi Ca Caicos Cairo Caledonia Calgary Cali California Cambodia Cambridge Cameroon Campinas Campo Canada Cancun Cape Caracas Carolina Carrollton Cartagena Cartier Casablanca Cayman Cebu Cedar Chad Chandler Charlotte Chattanooga Chengdu Chennai Chesapeake Chiba Chicago Chihuahua Chile China Chon Christi Cincinnati Ciudad Clarita Clarksville Clearwater Cleveland Cochabamba Collins Colombia Colombo Colorado Columbia Columbus Comoros Concord Congo Connecticut Cook Coral Cordoba Corona Costa Cote Cotonou Covina Croatia Ct Cuba Cucamonga Cuiaba Culiacan Curitiba Cyprus Czech Dakota Dali Dallas Daly Damascus Dar Davao Davidson Daye Dayton Dc De Delaware Delhi Denmark Denver Detroit Dhaka Diego District Dominica Dominican Dongguan Dortmund Downey Dubai Dublin Duesseldorf Duque Durban Durham Dushanbe Ecuador Edmonton Egypt Emirates England Erie Escondido Essen Estonia Ethiopia Eugene Europa Europe Evansville Faridabad Faroe Fayette Fayetteville Fiji Finland Fl Flint Florida Fm Fontana Fort Fortaleza Foshan France Francisco Frankfurt Fremont Fresno Fukuoka Fullerton Ga Gabon Garland Gary Gaza Genova Georgia Germany Ghana Ghaziabad Gibraltar Gilbert Giza Glasgow Glendale Gold Greece Green Greenland Greensboro Grenada Gu Guadalajara Guadeloupe Guam Guatemala Guernsey Guinea Guyana Ha Haiti Hama Hamburg Hamilton Hampton Hangzhou Harare Harbin Hartford Havana Haven Hawaii Hayward Helsinki Henderson Hialeah Hiroshima Ho Hollywood Homs Honduras Hong Honolulu Houston Howland Hungary Huntsville Hyderabad Ia Iceland Id Idaho Il Illinois Independence India Indiana Indianapolis Indonesia Inglewood Iowa Iran Iraq Ireland Irkutsk Irvine Irving Islamabad Israel Istanbul Italy Jackson Jacksonville Jakarta Jamaica Jammu Jamnagar Japan Jarvis Jeddah Jersey Jerusalem Jilin Jinjiang Jintan Joao Johannesburg Joliet Jordan Juan Juarez Jurong Kabul Kalyan Kano Kanpur Kansas Karachi Kathmandu Kawasaki Kazakhstan Kazan Keeling Kentucky Kenya Kerman Khartoum Kingdom Kingman Kingston Kiribati Knoxville Kobe Kolkata Kong Korea Kosovo Kota Kozhikode Krakow Krasnoyarsk Ks Kuala Kuwait Ky Kyiv Kyoto Kyrgyzstan La Lafayette Lagos Lahore Lakewood Lancaster Lanka Lansing Lanzhou Laos Laredo Las Latvia Lauderdale Lebanon Leon Leone Lesotho Lexington Liberia Libya Liechtenstein Lima Lincoln Lisbon Lithuania Liverpool Lomas London Los Louisiana Louisville Lowell Lubbock Lucia Lusaka Luxembourg Ma Macau Macedonia Madagascar Madison Madrid Maine Malaga Malawi Malaysia Maldives Mali Malta Manaus Manchester Mangalore Manila Maputo Mariana Marshall Martin Maryland Massachusetts Mauritania Mauritius Mayen Mcallen Md Medellin Melbourne Memphis Mendoza Merida Mesa Mesquite Mexicali Mexico Mh Mi Miami Michigan Midway Milan Milwaukee Minneapolis Minnesota Mississippi Missouri Mn Mo Mobile Modesto Mogadishu Moines Moldova Monaco Mongolia Montana Monte Monterrey Montgomery Montreal Moreno Morocco Moron Moscow Mosul Mozambique Mp Ms Mt Muenchen Mumbai Myanmar Nagoya Nagpur Nairobi Namibia Nanchong Nanhai Nanjing Nanning Nanyang Naperville Naples Nashik Nashville Natal Nauru Navi Nc Ne Nebraska Nepal Netherlands Nevada Newark Newport Nh Nicaragua Niger Nigeria Niigata Ningbo Nj Nm Norfolk Norwalk Norway Nottingham Nova Novokuznetsk Novosibirsk Nv Ny Oakland Oaks Oceanside Odessa Ohio Oklahoma Omaha Oman Ontario Oran Orange Oregon Orlando Orleans Osaka Oslo Ottawa Overland Oxnard Pa Pacific Pakistan Palermo Palestine Palmdale Panama Papua Paris Pasadena Paso Paterson Patna Pembroke Pennsylvania Peoria Perm Peru Peshawar Petersburg Philadelphia Philippines Phoenix Pittsburgh Plano Poland Polynesia Pomona Ponce Port Portland Porto Portsmouth Portugal Pr Prague Prairie Preston Principe Providence Provo Puebla Pueblo Puente Puerto Pune Pw Pyongyang Qatar Qingdao Quebec Quezon Quito Rabat Raleigh Recife Reno Ri Richmond Riga Rio Riverside Riyadh Rizhao Rochester Rockford Romania Rome Rosario Rotterdam Russia Rwanda Sacramento Safi Sahara Saint Sale Salem Salinas Salt Saltillo Salvador Samoa San Santa Santiago Santo Sao Sapporo Sarajevo Saudi Savannah Sc Scotland Scottsdale Sd Seattle Senegal Seoul Serbia Seychelles Shanghai Sheffield Shenzhen Shiraz Shreveport Simi Singapore Sioux Slovakia Slovenia Sofia Solomon Somalia Soviet Spain Spokane Springfield Stamford Sterling Stockholm Stockton Stuttgart Sudan Sunnyvale Surabaya Surat Suriname Suzhou Swaziland Sweden Switzerland Sydney Syracuse Syria Tacoma Taipei Taiwan Tajikistan Tallahassee Tampa Tanzania Tashkent Tehran Tel Tempe Tennessee Texas Thailand Thane Tianjin Tijuana Timor Tn Tobago Togo Tokyo Toledo Tome Tonga Topeka Torino Toronto Torrance Torreon Trinidad Tripoli Trujillo Tucson Tulsa Tunisia Turkey Turkmenistan Turks Tx Tyumen Ufa Uganda Ukraine Uruguay Ussr Ut Utah Uzbekistan Va Valencia Vallejo Vancouver Vatican Vegas Venezuela Ventura Veracruz Verde Vermont Vi Vienna Vietnam Vijayawada Virgin Virginia Vt Wa Waco Wake Wales Wallis Warren Warsaw Washington Waterbury Wenzhou West Westminster Wi Wichita Winnipeg Winston Wisconsin Worcester Wuhan Wuwei Wv Wy Wyoming Xinyi Yemen Yicheng Yokohama Yonkers York Yueyang Yugoslavia Zagreb Zambia Zamboanga Zealand Zimbabwe Zurich</p>
</blockquote>
</div>
<div id="periodicity" class="section level3">
<h3><span class="header-section-number">8.4.7</span> Periodicity</h3>
<div class="figure">
<img src="http://media.richard-careaga.com/img/1999-2002.png" alt="periodicity" />
<p class="caption">periodicity</p>
</div>
<p><strong>Things to note:</strong></p>
<p>A. The difference between 1999 and 2000 may reflect differences in levels of activity or availability of data or both. Each day in the three-year period is represented. The data points near the zero level generally represent weekends and holidays.</p>
<p>B. Emails in 2000 build to a peak in December.</p>
<p>C. Emails in 2001 build to a peak in June and another pair in October and November, but there is no December peak.</p>
<p>D. Instead there is a January 2002 peak.</p>
<p>E. The blue line is the smoothed trend of the data. Point above or below the dark gray band show higher or lower activity than the trend.</p>
</div>
<div id="social-networks" class="section level3">
<h3><span class="header-section-number">8.4.8</span> Social Networks</h3>
<p>Starting with <em>penpals</em>, people who regularly exchange one-on-one emails, it is possible to infer the functional organization of an organization like Enron. Think of it like whispering. Going beyond that, avoid the spoke and wheel representations that you so often see. Most of those connections end up being broadcast email. What helps is to look at email patterns over relatively small intervals.</p>
<p><img src="https://s3-us-west-2.amazonaws.com/tuva/wk39net.png" alt="Week 39, 2001" /> The user pairs were anonomized to reduce analyst bias</p>
</div>
</div>
<div id="future-work" class="section level2">
<h2><span class="header-section-number">8.5</span> Future work</h2>
<ul>
<li>Sampling of penpals to measure centrality and connectiveness</li>
<li>Clustering</li>
<li>NLP processing of email cliques</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="daycare.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="counted.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
