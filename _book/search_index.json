[
["the-subprime-mortgage-crisis-unfolds-in-early-2007-part-2.html", "4 The subprime mortgage crisis unfolds in early 2007, part 2 4.1 The fall of FICO 4.2 Restructuring the data 4.3 Logistic regression using locational fields 4.4 Principal component analysis 4.5 K-means clustering 4.6 Do the clusters predict loan performance?", " 4 The subprime mortgage crisis unfolds in early 2007, part 2 Keywords: logistic regression, unsupervised learning, principal component analysis, k-means clustering, correspondence analysis, chi-squared test, contingency table, pivot, correlation table 4.1 The fall of FICO The problem with the conventional wisdom of long standing is that it loses sight of history. The prominence of FICO in home loan credit underwriting described in the Pinto Testimony had its origins in a different time (the early 1990s) and a different lending environment. Freddie Mac was in a good position to ensure that all other things were equal. It made only what came to be called “prime” loans, generally for no more than 80% of the value of the property, under more stringent limitations on the debt-to-income ratio of the borrower and many other criteria that it kept within a narrow range, and offered only a few varieties of loans. In the subprime market that emerged in the late 90s, all of those factors changed. Criteria that were narrow became broad, documentation was relaxed and a widespread assumption was that continually rising home values would preclude any problems. It’s not surprising that FICO lost its predictive power. 4.2 Restructuring the data Some of the testing of FICO as a useful metric involved subsetting the data. There were many more variables than the ones used, some of them categorical and some categorical coded as numeric. One potentially useful variable is location, because we know that real estate value are location sensitive. We have four location fields in the database, all derived from the postal zip code: The zip code itself, which is generally either much smaller or much larger than the real estate market, and also changes at the convenience of the postal service. The metropolitan area derived from the U.S. Census ZIP code tabulation area, but covers a larger area than most real estate markets Longitude and latitude dervived from the ZCTA’s, used for mapping 4.3 Logistic regression using locational fields As a compromise, I converted the 5-digit zip codes into 3-digit zip codes. In metropolitan areas, the 3-digit codes are the sizes comparable to how the multiple listing services divide the market. We’ll see if there is any value in this proxy measure of real estate market, using logistic regression (linear regression is inappropriate because zip codes are not continuous). We continue to bump up against the same problem in the Normal Q-Q plot. Do metropolitan areas do a better job? No. [In preparation, remaining categorical variables for logistic regression] 4.4 Principal component analysis The cumulative variance is 0.258, 0.498, 0.706, 0.863, 1 for this sample of 300. From the loadings chart on the right, we see that obal (the original amount of the loan) and orate (the original rate of interest) are negatively correlated, -0.451. The remaining continuous variables, fico, dti (debt-to-income ratio), cltv (the ratio of first and second loans to the value of the mortgaged property), have small correlations: The variables fico and cltv have some correlation (0.295), while fico and dti (-0.036) and dti and cltv (0.121) have low correlations. The variable orate, however, is clearly discrete (it takes only a handful of integer values), so we re-run without it and increase the size of the sample to 9,000. The variances are [1] 0.2902293 0.2621526 0.2522584 0.1953596 4.5 K-means clustering The following represents the unsupervised classification of the fico, cltv, obal, and dti of an approximately 10% sample from the loans dataset. css.k &lt;- kmeans(css, 3, iter.max = 20, nstart = 50) plot(css, col=(css.k$cluster+1), main=&quot;K-Means Clustering Result with K=3, n = 9,000, nstart = 50, max.iter = 20&quot;, pch=20, cex=2) Using the entire population, rather than a sample creates somewhat better defined clusters K-means clustering of all loans in loans database Consideration might be given to separately clustering high-balance loans. Other outliers, such as the zero FICO scores, can be safely ignored, as can be the anomalous loan with a 97% dti ratio. 4.5.1 Housekeeping Since PCA is a relatively expensive calculation for 100K records, I saved the result to and Rda file for reuse. Now, I’ll add it to the create a new loans database as another categorical variable. Because I don’t run the MySQL server with rollback and to avoid a proliferation of names MariaDB [dlf]&gt; CREATE TABLE y7k AS SELECT * from y7; Query OK, 96147 rows affected (1.06 sec) Records: 96147 Duplicates: 0 Warnings: 0 The strategy is to create df loans with all fields and cbind cs.k$clusters, renamed “value” to kcluster and write back to loans. dbWriteTable(con, &quot;y7k&quot;, cs) Here’s the code (not run, to avoid churn) library(tidyverse) library(DBI) library(RMySQL) library(FactoMineR) drv &lt;- dbDriver(&quot;MySQL&quot;) con &lt;- dbConnect(drv, username=&quot;root&quot;, password=&quot;&quot;, dbname =&quot;dlf&quot;, host=&quot;localhost&quot;) res &lt;- dbGetQuery(con, &quot;SELECT * from y7&quot;) cs &lt;- as.tibble(res) load(&quot;data/kmeans_loans.Rda&quot;) # brings in cs.k #check for missing values nrow(cs) == length(cs.k$cluster) # convert to data frame csk &lt;- as.tibble(cs.k$cluster) # rename the column csk &lt;- csk %&gt;% transmute(kcluster = value) # combine loans &lt;- bind_cols(cs,csk) # remove duplicate rownames column loans &lt;- loans[-1] # write back to SQL #dbWriteTable(con, &quot;loans&quot;, y7k) # test #dbGetQuery(con, &quot;SELECT * from loans limit 25&quot;) 4.6 Do the clusters predict loan performance? Do the clusters derived from the principal components of the numeric variables, obal, orate, dti and cltv have an association with the down variable of consequtive payments missed? If they do, we have the makings of a model; otherwise, we need to look at the other data available. "]
]
