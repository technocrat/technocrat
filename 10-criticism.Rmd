---
output: html_document
---
# Criticism

## B Schools Need Data Science Faculty

### Catchy titles in academic papers attract readers. It's hard to resist.

> When Harry Fired Sally: The Double Standard in Punishing Misconduct Mark L. Egan, Gregor Matvos, and Amit Seru NBER Working Paper No. 23242 March 2017, Revised August 2018 JEL No. D18,G24,G28,J71

### The paper examines gender inequlity in the securities business

The authors identify a [gender punishment gap](http://www.nber.org/papers/w23242) in their analysis of over one million cases in which an employee required to be licensed by the Securities and Exchange Commission (*SEC*) through the Financial Industry Regulatory Authority (*FINRA*) and was found to have engaged in misconduct in violation of FINRA's rules. They find that women are terminated at a higher rate than their male counterparts and that they require a longer time to obtain another position at a different firm.

That this gender based disparity exists is highly plausible based on anecdoctal evidence in many areas of economic and social life and many academic studies.

My interest lies in their methodological approach. They give their model at page 9 of the report.

### The authors construct a multiple linear regression model

Separation model:

\$Misconduct_iqjlt=αFemale_i+βX_it+µ_qjlt+ε*_iqjlt\$

(© 2017 by Mark L. Egan, Gregor Matvos, and Amit Seru. All rights reserved. Short sections of text, not to exceed two paragraphs, may be quoted without explicit permission provided that full credit,including © notice, is given to the source.)

The authors characterize their model as linear regression. Mutiple regression equations are an extension of ordinary least square regression where

\$y = mx + b\$

with the addition of other terms.

The subscripts indicate individual, licensure, firm, time, and county.

### The authors collect an impressively large database

The authors collected monthly records on 1.2 million registered advisers during the 2005-2015 period. They carefully analyzed first names to determine gender and censored cases where the given name was ambiguous. I was impressed by the effort put into controlling for variables with the potential to bias their results.

### The authors make an unforced error in their choice of methodology

But now be dragons. The dependent variable is binary: how likely, given misconduct in year 0, is a subject likely to be fired in year 2. In the aggregate are there gender differences in that likelihood?

The technical term for this class of problem is *logistic regression,* commonly performed through a statistical technique similar to linear regression, termed *generalized linear model* with a specification of a *binary outcome.*

### How could this happen?

The authors had good data, careful thinking about controls, yet applied a mistaken analysis. How could they have put so much time and effort into analysis, yet fall into such an obvious methodological trap?

My gut reaction is that they had no data science colleague down the hall to drop in and consult with.

### Who am I to criticize?

Two of the authors are affiliated with business schools that [2017 rankings by *The Economist*](https://www.economist.com/whichmba/full-time-mba-ranking) include in the top 5 world wide. The third is affiliated with another university that is highly regarded in a number of other fields.

I am a mere student of applied statistics. If the question of methodological appropriateness should be decided solely on credentials, stop here.

### Why am I doing this?

It's an important issue and the cause of promoting gender equality in this business can be set back by a faulty analysis.

> Harumph, says top management. Must not be true, then. -- imaginary executive

It's also an important reminder not to take impressive looking quantitative analyses at face value.

### How do I know?

I've run my own multiple regression analyses and logistic models and I will show what the output looks like. I'll only take their principal model, separation given misconduct as the basis for illustrating the issues.

### Their results

Table 3b of the paper shows their results on separation given misconduct. If accurate, the results are not pretty. *Whether or not accurate*, the results are incomplete and misleading, even if their methodology were applicable.

![Table 3b](https://s3-us-west-2.amazonaws.com/tuva/Table3b.png)

Starting at the top, the first task to determine what the numbered columns refer to. The first column does not control for differences among the adviser's experience and licensing, year, firm or county. The second column adds adviser exerpience and licensing. The third column adds year, firm and county. The fourth column adds the possible combinations of potential licenses that the individual holds.

Essentially these are four different models.

1.  \$Misconduct_i = Person_i\$
2.  \$Misconduct_iq = αFemale_i + ε_iqjlt\$
3.  \$Misconduct_iqj = αFemale_i + βX_it + ε_iqjlt\$
4.  \$Misconduct_iqjlt = αFemale_i + βX_it + µ_qjlt + ε_iqjlt\$

as described on page 11.

On page 11, the authors state: "The coefficient on misconduct measures the probability that a *male* adviser experiences a job separation following misconduct." (emphasis added). Thus, predicting male separation from some measure of female misconduct and female status, with or without other control, seems non-sensical. Perhaps the authors expect the reader to infer the corresponding coefficient for the probability that a *female* adviser experiences a job separation. That dog won't hunt.

Each column reports a number of "observations," which correspond to the 10-year period of the data, meaning about 400K-600K observations per year. According to Table 1 on page 62, however, the 10-year incidence of misconduct by male advisers in any given year was 0.72%. Therefore, we expect to observe, on average *obvervations*/10\*0.072 misconduct incidents by males per year or, for the first model approximately 43,350 incidents, of which some proportion resulted in separation. That is the *n*, or population size, that goes into the calculations, including the degrees of freedom needed to calculate the F-statistic. From the information presented, we cannot be sure what value of *n* was relied on.

Finally, the means of the dependent variable have unexplained similarity. Case three should not be identical to cases one and two if the added independent variables are increasing explanatory power, and the mean in case four seems too little difference from case three, given the increase in R-Squared. Without access to the author's data, however, there is no way for me to decide that.

The first line, "Misconduct" contains coefficients for each of the columns, given in percentages. (More often coefficients are expressed in proportions.) Coefficients are a prediction of the effect of the variable on the outcome, holding the other variables constant. The asterisks following each measure are explained in a note to Table 3b as standard errors less than 0.01. The terminology of expressing standard errors in the form *p &lt; 0.01* is also unusual. A *p-value* measure of "statistical significance" (the probability that the result was a product of chance) usually refers to the probability of the variable is greater than a measure called the *t value.* Standard errors are also not generally expressed as percentages.

R-squared, sometimes called the coefficient of determination and indicates the proportion of the variance in the data that an independent variable accounts for. It is usually tweeked to an *adjusted R square* to better eliminate random variation.

A value of R-squared that is equal to 1 completely explains the variance. A value of 0 indicates a complete lack of explanatory power. For the first column, R-squared is 0.004. Four basis points on a billion dollars is not chump change. The same proportion in a analytic study is. The result of the second column is R-squared equals 0.014. These two columns tell us essentially nothing except the failure of the model to reflect reality when not controlling for other factors.

Columns three and four show R-squared of 0.332 and 0.403, respectively, which show appreciable effects in a multiple linear regression model appropriately applied.

### A counterexample: what a data scientist would expect to see in a multiple linear regression model output

Data on [Seattle area housing prices](https://www.kaggle.com/harlfoxem/housesalesprediction?login=true) provide a convenient way to illustrate the usual output of a multiple linear regression model output. This is a 21K dataset with 20 variables on housing characteristics and sales price.

The first thing I am going to do is to split the dataset 2:1 into a training set and a test set. I'll model the training set, then see if I get comparable results on the test set. That's something the authors of the paper omitted. I'll be using the R statistical programming language.

```{r bonehead, results="asis", echo = FALSE, warning=FALSE} 
library(tidyverse) 
library(kableExtra)
library(stargazer)
house <- read_csv('/Users/rc/projects/statistics-for-data-scientists/kc_house_data.csv')
kable(glimpse(house), format.args = list(big.mark = ","), caption = "Seattle Area Housing Data example")  %>% kable_styling(bootstrap_options = "striped", full_width = F)
glimpse(house)
train <- house %>% sample_frac(0.66, replace = TRUE) 
test <- setdiff(house,train) 
mod <- lm(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + yr_built, data = train) 
stargazer(mod)
```

Well, now! I pick a handful of variables off the top of my head and presto! My model explains over 50% of the variation in price. Am I good or what?

Not really, and I'll explain all the bonehead errors in another post (didn't normalize, used variables that are not truly independent, didn't check the distribution of residual errors and a host of other sins). But let's talk about the information you should expect from a multiple linear regression.
